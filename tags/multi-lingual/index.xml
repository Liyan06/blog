<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>MULTI-LINGUAL on Liyan Tang</title>
    <link>https://tangliyan.com/blog/tags/multi-lingual/</link>
    <description>Recent content in MULTI-LINGUAL on Liyan Tang</description>
    <image>
      <url>https://tangliyan.com/blog/papermod-cover.png</url>
      <link>https://tangliyan.com/blog/papermod-cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Mon, 28 Dec 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://tangliyan.com/blog/tags/multi-lingual/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Cross-Lingual Learning</title>
      <link>https://tangliyan.com/blog/posts/corss_lingual/</link>
      <pubDate>Mon, 28 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/corss_lingual/</guid>
      <description>Cross-lingual learning Most languages do not have training data available to create state-of-the-art models and thus our ability to create intelligent systems for these languages is limited as well.
Cross-lingual learning (CLL) is one possible remedy to solve the lack of data for low-resource languages. In essence, it is an effort to utilize annotated data from other languages when building new NLP models. When CLL is considered, target languages usually lack resources, while source languages are resource-rich and they can be used to improve the results for the former.</description>
    </item>
    
    <item>
      <title>Multi-lingual: M-Bert, LASER, MultiFiT, XLM</title>
      <link>https://tangliyan.com/blog/posts/multilingual/</link>
      <pubDate>Sat, 08 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/multilingual/</guid>
      <description>Multilingual Models are a type of Machine Learning model that can understand different languages. In this post, I&amp;rsquo;m going to discuss four common multi-lingual language models Multilingual-Bert (M-Bert), Language-Agnostic SEntence Representations (LASER Embeddings), Efficient multi-lingual language model fine-tuning (MultiFiT) and Cross-lingual Language Model (XLM).
Ways of tokenization Word-based tokenization Word-based tokenization works well for the morphologically poor English, but results in very large and sparse vocabularies for morphologically rich languages, such as Polish and Turkish.</description>
    </item>
    
  </channel>
</rss>
