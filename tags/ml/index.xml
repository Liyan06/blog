<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>ML on Liyan Tang</title>
    <link>https://tangliyan.com/blog/tags/ml/</link>
    <description>Recent content in ML on Liyan Tang</description>
    <image>
      <url>https://tangliyan.com/blog/papermod-cover.png</url>
      <link>https://tangliyan.com/blog/papermod-cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Wed, 05 Aug 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://tangliyan.com/blog/tags/ml/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Knowledge Distillation</title>
      <link>https://tangliyan.com/blog/posts/distillation/</link>
      <pubDate>Wed, 05 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/distillation/</guid>
      <description>Currently, especially in NLP, very large scale models are being trained. A large portion of those can’t even fit on an average person’s hardware. We can train a small network that can run on the limited computational resource of our mobile device. But small models can’t extract many complex features that can be handy in generating predictions unless you devise some elegant algorithm to do so. Plus, due to the Law of diminishing returns, a great increase in the size of model barely maps to a small increase in the accuracy.</description>
    </item>
    
    <item>
      <title>Intro to Deep Learning and Backpropagation</title>
      <link>https://tangliyan.com/blog/posts/dl/</link>
      <pubDate>Tue, 26 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/dl/</guid>
      <description>Deep Learning v.s. Machine Learning The major difference between Deep Learning and Machine Learning technique is the problem solving approach. Deep Learning techniques tend to solve the problem end to end, where as Machine learning techniques need the problem statements to break down to different parts to be solved first and then their results to be combine at final stage.
Forward Propagation The general procedure is the following:
$$ \begin{aligned} a^{(1)}(x) &amp;amp;= w^{(1)^T} \cdot x + b^{(1)} \\ h^{(1)}(x) &amp;amp;= g_1(a^{(1)}(x)) \\ a^{(2)}(x) &amp;amp;= w^{(2)^T} \cdot h^{(1)}(x) + b^{(2)} \\ h^{(2)}(x) &amp;amp;= g_2(a^{(2)}(x)) \\ &amp;amp;&amp;hellip;&amp;hellip; \\ a^{(L+1)}(x) &amp;amp;= w^{(L+1)^T} \cdot h^{(L)}(x) + b^{(L+1)} \\ h^{(L+1)}(x) &amp;amp;= g_{L+1}(a^{(L+1)}(x)) \end{aligned} $$</description>
    </item>
    
    <item>
      <title>Gaussian mixture model (GMM), k-means</title>
      <link>https://tangliyan.com/blog/posts/gmm/</link>
      <pubDate>Sat, 16 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/gmm/</guid>
      <description>Gaussian mixture model (GMM) A Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters.
Interpretation from geometry $p(x)$ is a weighted sum of multiple Gaussian distribution.
$$p(x)=\sum_{k=1}^{K} \alpha_{k} \cdot \mathcal{N}\left(x | \mu_{k}, \Sigma_{k}\right) $$
Interpretation from mixture model setup:
  The total number of Gaussian distribution $K$.
  $x$, a sample (observed variable).</description>
    </item>
    
    <item>
      <title>Probabilistic Graphical Model (PGM)</title>
      <link>https://tangliyan.com/blog/posts/pgm/</link>
      <pubDate>Mon, 11 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/pgm/</guid>
      <description>Probabilistic Graphical Model (PGM) Definition: A probabilistic graphical model is a probabilistic model for which a graph expresses the conditional dependence structure between random variables.
In general, PGM obeys following rules: $$ \begin{aligned} &amp;amp;\text {Sum Rule : } p\left(x_{1}\right)=\int p\left(x_{1}, x_{2}\right) d x_{2}\\ &amp;amp;\text {Product Rule : } p\left(x_{1}, x_{2}\right)=p\left(x_{1} | x_{2}\right) p\left(x_{2}\right)\\ &amp;amp;\text {Chain Rule: } p\left(x_{1}, x_{2}, \cdots, x_{p}\right)=\prod_{i=1}^{p} p\left(x_{i} | x_{i+1, x_{i+2}} \ldots x_{p}\right)\\ &amp;amp;\text {Bayesian Rule: } p\left(x_{1} | x_{2}\right)=\frac{p\left(x_{2} | x_{1}\right) p\left(x_{1}\right)}{p\left(x_{2}\right)} \end{aligned} $$</description>
    </item>
    
    <item>
      <title>EM (Expectation–Maximization) Algorithm</title>
      <link>https://tangliyan.com/blog/posts/em/</link>
      <pubDate>Fri, 24 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/em/</guid>
      <description>Jensen’s inequality  Theorem: Let $f$ be a convex function, and let $X$ be a random variable. Then:  $$E[f(X)] \geq f(E[X])$$
$\quad$ Moreover, if $f$ is strictly convex, then $E[f(X)] = f(E[X])$ holds true if and only if $X$ is a constant.
 Later in the post we are going to use the following fact from the Jensen&amp;rsquo;s inequality: Suppose $\lambda_j \geq 0$ for all $j$ and $\sum_j \lambda_j = 1$, then  $$ \log \sum_j \lambda_j y_j \geq \sum_j \lambda_j , log , y_j$$</description>
    </item>
    
    <item>
      <title>Distributed representation, Hyperbolic Space, Gaussian/Graph Embedding</title>
      <link>https://tangliyan.com/blog/posts/representation/</link>
      <pubDate>Fri, 17 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/representation/</guid>
      <description>Overview of various word representation and Embedding methods Local Representation v.s. Distributed Representation  One-hot encoding is local representation and is good for local generalization; distributed representation is good for global generalization.  Comparison between local generalization and global generalization:
Here is an example for better understanding this pair of concepts. Suppose now you have a bunch of ingredients and you&amp;rsquo;re able to cook 100 different meals with these ingredients.</description>
    </item>
    
    <item>
      <title>SVM, Dual SVM, Non-linear SVM</title>
      <link>https://tangliyan.com/blog/posts/svm/</link>
      <pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/svm/</guid>
      <description>Linear SVM Idea We want to find a hyper-plane $w^\top x + b = 0$ that maximizes the margin.
Set up We first show that the vector $w$ is orthogonal to this hyper-plane. Let $x_1$, $x_2$ be any element on the hyper-plane. So we have $w^\top x_1 + b = 0$ and $w^\top x_2 + b = 0$. Then $w^\top (x_1 - x_2) = 0$, which implies $w$ is orthogonal to the hyper-plane.</description>
    </item>
    
    <item>
      <title>Introduction to Convex Optimization - Primal problem to Dual problem</title>
      <link>https://tangliyan.com/blog/posts/convex2/</link>
      <pubDate>Sun, 29 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/convex2/</guid>
      <description>Consider an optimization problem in the standard form (we call this a primal problem):
We denote the optimal value of this as $p^\star$. We don&amp;rsquo;t assume the problem is convex.
The Lagrange dual function We define the Lagrangian $L$ associated with the problem as $$ L(x,\lambda, v) = f_0(x) + \sum^m_{i=1}\lambda_if_i(x) + \sum^p_{i=1}v_ih_i(x)$$ We call vectors $\lambda$ and $v$ the dual variables or Lagrange multiplier vectors associated with the problem (1).</description>
    </item>
    
    <item>
      <title>Introduction to Convex Optimization - Basic Concepts</title>
      <link>https://tangliyan.com/blog/posts/convex1/</link>
      <pubDate>Sat, 28 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/convex1/</guid>
      <description>Optimization problem All optimization problems can be written as:
Optimization Categories   convex v.s. non-convex Deep Neural Network is non-convex
  continuous v.s.discrete Most are continuous variable; tree structure is discrete
  constrained v.s. non-constrained We add prior to make it a constrained problem
  smooth v.s.non-smooth Most are smooth optimization
  Different initialization brings different optimum (if not convex) Idea: Give up global optimal and find a good local optimal.</description>
    </item>
    
    <item>
      <title>XGBoost</title>
      <link>https://tangliyan.com/blog/posts/xgboost/</link>
      <pubDate>Fri, 27 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/xgboost/</guid>
      <description>Bagging v.s. Boosting: Bagging: Leverages unstable base learners that are weak because of overfitting.
Boosting: Leverages stable base learners that are weak because of underfitting.
XGBoost Learning Process through XGBoost:
 How to set a objective function. Hard to solve directly, how to approximate. How to add tree structures into the objective function. Still hard to optimize, then have to use greedy algorithms.  Step 1. How to set an objective function Suppose we trained $K$ trees, then the prediction for the ith sample is $$\hat{y}_i = \sum_{k=1}^K f_k(x_i), f_k \in \mathcal{F}$$ where $K$ is the number of trees, $f$ is a function in the functional space $\mathcal{F}$, and $\mathcal{F}$ is the set of all possible CARTs.</description>
    </item>
    
    <item>
      <title>Maximum Likelihood Estimation (MLE) and Maximum a Posteriori (MAP)</title>
      <link>https://tangliyan.com/blog/posts/mle/</link>
      <pubDate>Thu, 26 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/mle/</guid>
      <description>MLE v.s. MAP  MLE: learn parameters from data. MAP: add a prior (experience) into the model; more reliable if data is limited. As we have more and more data, the prior becomes less useful. As data increase, MAP $\rightarrow$ MLE.  Notation: $D = {(x_1, y_1), (x_2, y_2), &amp;hellip;, (x_n, y_n)}$
Framework:
 MLE: $\mathop{\rm arg\ max} P(D \ |\ \theta)$ MAP: $\mathop{\rm arg\ max} P(\theta \ |\ D)$  Note that taking a product of some numbers less than 1 would approaching 0 as the number of those numbers goes to infinity, it would be not practical to compute, because of computation underflow.</description>
    </item>
    
    <item>
      <title>Logistic Regression, L1, L2 regularization, Gradient/Coordinate descent</title>
      <link>https://tangliyan.com/blog/posts/logistic_regression/</link>
      <pubDate>Wed, 25 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/logistic_regression/</guid>
      <description>Generative model v.s. Discriminative model： Examples:
 Generative model: Naive Bayes, HMM, VAE, GAN. Discriminative model：Logistic Regression, CRF.  Objective function:
 Generative model: $max \ p \ (x,y)$ Discriminative model：$max \ p \ (y \ |\ x)$  Difference:
 Generative model: We first assume a distribution of the data in the consideration of computation efficiency and features of the data. Next, the model will learn the parameters of distribution of the data.</description>
    </item>
    
  </channel>
</rss>
