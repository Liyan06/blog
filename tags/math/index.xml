<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>MATH on Liyan Tang</title>
    <link>https://tangliyan.com/blog/tags/math/</link>
    <description>Recent content in MATH on Liyan Tang</description>
    <image>
      <url>https://tangliyan.com/blog/papermod-cover.png</url>
      <link>https://tangliyan.com/blog/papermod-cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Mon, 24 Aug 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://tangliyan.com/blog/tags/math/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Graph Convolutional Neural Network -  Spectral Convolution</title>
      <link>https://tangliyan.com/blog/posts/spectral_conv/</link>
      <pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/spectral_conv/</guid>
      <description>Fourier Transform Virtually everything in the world can be described via a waveform - a function of time, space or some other variable. For instance, sound waves, the price of a stock, etc. The Fourier Transform gives us a unique and powerful way of viewing these waveforms: All waveforms, no matter what you scribble or observe in the universe, are actually just the sum of simple sinusoids of different frequencies.</description>
    </item>
    
    <item>
      <title>Graph Convolutional Neural Network - Spatial Convolution</title>
      <link>https://tangliyan.com/blog/posts/spatial_conv/</link>
      <pubDate>Thu, 20 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/spatial_conv/</guid>
      <description>Note This is the second post of the Graph Neural Networks (GNNs) series.
Convolutional graph neural networks (ConvGNNs) Convolutional graph neural networks (ConvGNNs) generalize the operation of convolution from grid data to graph data. The main idea is to generate a node $v$’s representation by aggregating its own features $\mathbf{x}_{v}$ and neighbors’ features $\mathbf{x}_{u}$, where $u \in N(v)$. Different from RecGNNs, ConvGNNs stack fixed number of multiple graph convolutional layers with different weights to extract high-level node representations.</description>
    </item>
    
    <item>
      <title>Introduction to Graph Neural Network (GNN)</title>
      <link>https://tangliyan.com/blog/posts/gnn/</link>
      <pubDate>Mon, 17 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/gnn/</guid>
      <description>Note This is the first post of the Graph Neural Networks (GNNs) series.
Background and Intuition There is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. For examples, in e-commence, a graph-based learning system can exploit the interactions between users and products to make highly accurate recommendations. In chemistry, molecules are modeled as graphs.</description>
    </item>
    
    <item>
      <title>Recurrent Neural Network (RNN) and Long Short Term Memory (LSTM)</title>
      <link>https://tangliyan.com/blog/posts/lstm/</link>
      <pubDate>Thu, 04 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/lstm/</guid>
      <description>Sequence Data There are many sequence data in applications. Here are some examples
  Machine translation
 from text sequence to text sequence.    Text Summarization
 from text sequence to text sequence.    Sentiment classification
 from text sequence to categories.    Music Generation
 from nothing or some simple stuff (character, integer, etc) to wave sequence.    Name entity recognition (NER)</description>
    </item>
    
    <item>
      <title>Intro to Deep Learning and Backpropagation</title>
      <link>https://tangliyan.com/blog/posts/dl/</link>
      <pubDate>Tue, 26 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/dl/</guid>
      <description>Deep Learning v.s. Machine Learning The major difference between Deep Learning and Machine Learning technique is the problem solving approach. Deep Learning techniques tend to solve the problem end to end, where as Machine learning techniques need the problem statements to break down to different parts to be solved first and then their results to be combine at final stage.
Forward Propagation The general procedure is the following:
$$ \begin{aligned} a^{(1)}(x) &amp;amp;= w^{(1)^T} \cdot x + b^{(1)} \\ h^{(1)}(x) &amp;amp;= g_1(a^{(1)}(x)) \\ a^{(2)}(x) &amp;amp;= w^{(2)^T} \cdot h^{(1)}(x) + b^{(2)} \\ h^{(2)}(x) &amp;amp;= g_2(a^{(2)}(x)) \\ &amp;amp;&amp;hellip;&amp;hellip; \\ a^{(L+1)}(x) &amp;amp;= w^{(L+1)^T} \cdot h^{(L)}(x) + b^{(L+1)} \\ h^{(L+1)}(x) &amp;amp;= g_{L+1}(a^{(L+1)}(x)) \end{aligned} $$</description>
    </item>
    
    <item>
      <title>Log-Linear Model, Conditional Random Field(CRF)</title>
      <link>https://tangliyan.com/blog/posts/crf/</link>
      <pubDate>Tue, 19 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/crf/</guid>
      <description>Log-Linear model Let $x$ be an example, and let $y$ be a possible label for it. A log-linear model assumes that
$$ p(y | x ; w)=\frac{\exp [\sum_{j=1}^J w_{j} F_{j}(x, y)]}{Z(x, w)} $$
where the partition function
$$ Z(x, w)=\sum_{y^{\prime}} \exp [\sum_{j=1}^J w_{j} F_{j}\left(x, y^{\prime}\right)] $$
Note that in $\sum_{y^{\prime}}$, we make a summation over all possible $y$. Therefore, given $x$, the label predicted by the model is
$$ \hat{y}=\underset{y}{\operatorname{argmax}} p(y | x ; w)=\underset{y}{\operatorname{argmax}} \sum_{j=1}^J w_{j} F_{j}(x, y) $$</description>
    </item>
    
    <item>
      <title>Gaussian mixture model (GMM), k-means</title>
      <link>https://tangliyan.com/blog/posts/gmm/</link>
      <pubDate>Sat, 16 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/gmm/</guid>
      <description>Gaussian mixture model (GMM) A Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters.
Interpretation from geometry $p(x)$ is a weighted sum of multiple Gaussian distribution.
$$p(x)=\sum_{k=1}^{K} \alpha_{k} \cdot \mathcal{N}\left(x | \mu_{k}, \Sigma_{k}\right) $$
Interpretation from mixture model setup:
  The total number of Gaussian distribution $K$.
  $x$, a sample (observed variable).</description>
    </item>
    
    <item>
      <title>Probabilistic Graphical Model (PGM)</title>
      <link>https://tangliyan.com/blog/posts/pgm/</link>
      <pubDate>Mon, 11 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/pgm/</guid>
      <description>Probabilistic Graphical Model (PGM) Definition: A probabilistic graphical model is a probabilistic model for which a graph expresses the conditional dependence structure between random variables.
In general, PGM obeys following rules: $$ \begin{aligned} &amp;amp;\text {Sum Rule : } p\left(x_{1}\right)=\int p\left(x_{1}, x_{2}\right) d x_{2}\\ &amp;amp;\text {Product Rule : } p\left(x_{1}, x_{2}\right)=p\left(x_{1} | x_{2}\right) p\left(x_{2}\right)\\ &amp;amp;\text {Chain Rule: } p\left(x_{1}, x_{2}, \cdots, x_{p}\right)=\prod_{i=1}^{p} p\left(x_{i} | x_{i+1, x_{i+2}} \ldots x_{p}\right)\\ &amp;amp;\text {Bayesian Rule: } p\left(x_{1} | x_{2}\right)=\frac{p\left(x_{2} | x_{1}\right) p\left(x_{1}\right)}{p\left(x_{2}\right)} \end{aligned} $$</description>
    </item>
    
    <item>
      <title>Hidden Markov Model (HMM)</title>
      <link>https://tangliyan.com/blog/posts/hmm/</link>
      <pubDate>Sun, 03 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/hmm/</guid>
      <description>Before reading this post, make sure you are familiar with the EM Algorithm and decent among of knowledge of convex optimization. If not, please check out my previous post
  EM Algorithm
  convex optimization primal and dual problem
  Let&amp;rsquo;s get started!
Conditional independence $A$ and $B$ are conditionally independent given $C$ if and only if, given knowledge that $C$ occurs, knowledge of whether $A$ occurs provides no information on the likelihood of $B$ occurring, and knowledge of whether $B$ occurs provides no information on the likelihood of $A$ occurring.</description>
    </item>
    
    <item>
      <title>EM (Expectation–Maximization) Algorithm</title>
      <link>https://tangliyan.com/blog/posts/em/</link>
      <pubDate>Fri, 24 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/em/</guid>
      <description>Jensen’s inequality  Theorem: Let $f$ be a convex function, and let $X$ be a random variable. Then:  $$E[f(X)] \geq f(E[X])$$
$\quad$ Moreover, if $f$ is strictly convex, then $E[f(X)] = f(E[X])$ holds true if and only if $X$ is a constant.
 Later in the post we are going to use the following fact from the Jensen&amp;rsquo;s inequality: Suppose $\lambda_j \geq 0$ for all $j$ and $\sum_j \lambda_j = 1$, then  $$ \log \sum_j \lambda_j y_j \geq \sum_j \lambda_j , log , y_j$$</description>
    </item>
    
    <item>
      <title>Skip-gram</title>
      <link>https://tangliyan.com/blog/posts/skipgram/</link>
      <pubDate>Sat, 18 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/skipgram/</guid>
      <description>Comparison between CBOW and Skip-gram The major difference is that skip-gram is better for infrequent words than CBOW in word2vec. For simplicity, suppose there is a sentence &amp;ldquo;$w_1w_2w_3w_4$&amp;rdquo;, and the window size is $1$.
For CBOW, it learns to predict the word given a context, or to maximize the following probability
$$ p(w_2|w_1,w_3) \cdot P(w_3|w_2,w_4)$$
This is an issue for infrequent words, since they don’t appear very often in a given context.</description>
    </item>
    
    <item>
      <title>NLP Basics, Spell Correction with Noisy Channel</title>
      <link>https://tangliyan.com/blog/posts/nlp_basic/</link>
      <pubDate>Fri, 10 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/nlp_basic/</guid>
      <description>NLP = NLU + NLG  NLU: Natural Language Understanding NLG: Natural Language Generation  NLG may be viewed as the opposite of NLU: whereas in NLU, the system needs to disambiguate the input sentence to produce the machine representation language, in NLG the system needs to make decisions about how to put a concept into human understandable words.
Classical applications in NLP   Question Answering
  Sentiment Analysis</description>
    </item>
    
    <item>
      <title>SVM, Dual SVM, Non-linear SVM</title>
      <link>https://tangliyan.com/blog/posts/svm/</link>
      <pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/svm/</guid>
      <description>Linear SVM Idea We want to find a hyper-plane $w^\top x + b = 0$ that maximizes the margin.
Set up We first show that the vector $w$ is orthogonal to this hyper-plane. Let $x_1$, $x_2$ be any element on the hyper-plane. So we have $w^\top x_1 + b = 0$ and $w^\top x_2 + b = 0$. Then $w^\top (x_1 - x_2) = 0$, which implies $w$ is orthogonal to the hyper-plane.</description>
    </item>
    
    <item>
      <title>Introduction to Convex Optimization - Primal problem to Dual problem</title>
      <link>https://tangliyan.com/blog/posts/convex2/</link>
      <pubDate>Sun, 29 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/convex2/</guid>
      <description>Consider an optimization problem in the standard form (we call this a primal problem):
We denote the optimal value of this as $p^\star$. We don&amp;rsquo;t assume the problem is convex.
The Lagrange dual function We define the Lagrangian $L$ associated with the problem as $$ L(x,\lambda, v) = f_0(x) + \sum^m_{i=1}\lambda_if_i(x) + \sum^p_{i=1}v_ih_i(x)$$ We call vectors $\lambda$ and $v$ the dual variables or Lagrange multiplier vectors associated with the problem (1).</description>
    </item>
    
    <item>
      <title>Introduction to Convex Optimization - Basic Concepts</title>
      <link>https://tangliyan.com/blog/posts/convex1/</link>
      <pubDate>Sat, 28 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/convex1/</guid>
      <description>Optimization problem All optimization problems can be written as:
Optimization Categories   convex v.s. non-convex Deep Neural Network is non-convex
  continuous v.s.discrete Most are continuous variable; tree structure is discrete
  constrained v.s. non-constrained We add prior to make it a constrained problem
  smooth v.s.non-smooth Most are smooth optimization
  Different initialization brings different optimum (if not convex) Idea: Give up global optimal and find a good local optimal.</description>
    </item>
    
    <item>
      <title>XGBoost</title>
      <link>https://tangliyan.com/blog/posts/xgboost/</link>
      <pubDate>Fri, 27 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/xgboost/</guid>
      <description>Bagging v.s. Boosting: Bagging: Leverages unstable base learners that are weak because of overfitting.
Boosting: Leverages stable base learners that are weak because of underfitting.
XGBoost Learning Process through XGBoost:
 How to set a objective function. Hard to solve directly, how to approximate. How to add tree structures into the objective function. Still hard to optimize, then have to use greedy algorithms.  Step 1. How to set an objective function Suppose we trained $K$ trees, then the prediction for the ith sample is $$\hat{y}_i = \sum_{k=1}^K f_k(x_i), f_k \in \mathcal{F}$$ where $K$ is the number of trees, $f$ is a function in the functional space $\mathcal{F}$, and $\mathcal{F}$ is the set of all possible CARTs.</description>
    </item>
    
    <item>
      <title>Maximum Likelihood Estimation (MLE) and Maximum a Posteriori (MAP)</title>
      <link>https://tangliyan.com/blog/posts/mle/</link>
      <pubDate>Thu, 26 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/mle/</guid>
      <description>MLE v.s. MAP  MLE: learn parameters from data. MAP: add a prior (experience) into the model; more reliable if data is limited. As we have more and more data, the prior becomes less useful. As data increase, MAP $\rightarrow$ MLE.  Notation: $D = {(x_1, y_1), (x_2, y_2), &amp;hellip;, (x_n, y_n)}$
Framework:
 MLE: $\mathop{\rm arg\ max} P(D \ |\ \theta)$ MAP: $\mathop{\rm arg\ max} P(\theta \ |\ D)$  Note that taking a product of some numbers less than 1 would approaching 0 as the number of those numbers goes to infinity, it would be not practical to compute, because of computation underflow.</description>
    </item>
    
    <item>
      <title>Logistic Regression, L1, L2 regularization, Gradient/Coordinate descent</title>
      <link>https://tangliyan.com/blog/posts/logistic_regression/</link>
      <pubDate>Wed, 25 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/logistic_regression/</guid>
      <description>Generative model v.s. Discriminative model： Examples:
 Generative model: Naive Bayes, HMM, VAE, GAN. Discriminative model：Logistic Regression, CRF.  Objective function:
 Generative model: $max \ p \ (x,y)$ Discriminative model：$max \ p \ (y \ |\ x)$  Difference:
 Generative model: We first assume a distribution of the data in the consideration of computation efficiency and features of the data. Next, the model will learn the parameters of distribution of the data.</description>
    </item>
    
  </channel>
</rss>
