<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>NLP on Liyan Tang</title>
    <link>https://tangliyan.com/blog/tags/nlp/</link>
    <description>Recent content in NLP on Liyan Tang</description>
    <image>
      <url>https://tangliyan.com/blog/papermod-cover.png</url>
      <link>https://tangliyan.com/blog/papermod-cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Mon, 25 Oct 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://tangliyan.com/blog/tags/nlp/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Paper Review - Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing</title>
      <link>https://tangliyan.com/blog/posts/pretrain_prompt/</link>
      <pubDate>Mon, 25 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/pretrain_prompt/</guid>
      <description>Authors: Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig</description>
    </item>
    
    <item>
      <title>Paper Review - Knowledge Neurons in Pretrained Transformers</title>
      <link>https://tangliyan.com/blog/posts/knowledge_neurons/</link>
      <pubDate>Sun, 24 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/knowledge_neurons/</guid>
      <description>Authors: Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Furu Wei</description>
    </item>
    
    <item>
      <title>Paper Review - Learning to summarize from human feedback</title>
      <link>https://tangliyan.com/blog/posts/learning_to_summarize/</link>
      <pubDate>Mon, 18 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/learning_to_summarize/</guid>
      <description>Authors: Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul Christiano</description>
    </item>
    
    <item>
      <title>Paper Review - Inspecting the Factuality of Hallucinated Entities in Abstractive Summarization</title>
      <link>https://tangliyan.com/blog/posts/inspecting_the_factuality/</link>
      <pubDate>Mon, 27 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/inspecting_the_factuality/</guid>
      <description>Authors: Meng Cao, Yue Dong, Jackie Chi Kit Cheung</description>
    </item>
    
    <item>
      <title>Paper Review - Decision-Focused Summarization</title>
      <link>https://tangliyan.com/blog/posts/decision_focused_sum/</link>
      <pubDate>Wed, 22 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/decision_focused_sum/</guid>
      <description>Authors: Chao-Chun Hsu, Chenhao Tan</description>
    </item>
    
    <item>
      <title>Paper Review - A Survey of the State of Explainable AI for Natural Language Processing</title>
      <link>https://tangliyan.com/blog/posts/a_survey_of/</link>
      <pubDate>Tue, 21 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/a_survey_of/</guid>
      <description>Authors: Marina Danilevsky, Kun Qian, Ranit Aharonov, Yannis Katsis, Ban Kawas, Prithviraj Sen</description>
    </item>
    
    <item>
      <title>Paper Review - An Exploratory Study on Long Dialogue Summarization: What Works and What’s Next</title>
      <link>https://tangliyan.com/blog/posts/an_exploratory_study/</link>
      <pubDate>Tue, 14 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/an_exploratory_study/</guid>
      <description>Authors: Yusen Zhang Ansong Ni Tao Yu Rui Zhang Chenguang Zhu Budhaditya Deb Asli Celikyilmaz Ahmed H. Awadallah Dragomir Radev</description>
    </item>
    
    <item>
      <title>Paper Review - GNNExplainer: Generating Explanations for Graph Neural Networks</title>
      <link>https://tangliyan.com/blog/posts/gnnexplainer/</link>
      <pubDate>Mon, 13 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/gnnexplainer/</guid>
      <description>Authors: Rex Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, Jure Leskovec</description>
    </item>
    
    <item>
      <title>Paper Review - Factual Consistency Evaluation for Text Summarization via Counterfactual Estimation</title>
      <link>https://tangliyan.com/blog/posts/factual_consistency_evaluation/</link>
      <pubDate>Fri, 10 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/factual_consistency_evaluation/</guid>
      <description>Authors: Yuexiang Xie, Fei Sun, Yang Deng, Yaliang Li, Bolin Ding</description>
    </item>
    
    <item>
      <title>Paper Review - Discretized Integrated Gradients for Explaining Language Models</title>
      <link>https://tangliyan.com/blog/posts/discretized_integrated_gradients/</link>
      <pubDate>Wed, 08 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/discretized_integrated_gradients/</guid>
      <description>Authors: Soumya Sanyal, Xiang Ren</description>
    </item>
    
    <item>
      <title>Paper Review - What’s in a Summary? Laying the Groundwork for Advances in Hospital-Course Summarization</title>
      <link>https://tangliyan.com/blog/posts/what_is_in/</link>
      <pubDate>Tue, 07 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/what_is_in/</guid>
      <description>Authors: Griffin Adams, Emily Alsentzer, Mert Ketenci, Jason Zucker, Noémie Elhadad</description>
    </item>
    
    <item>
      <title>Paper Review - Causal Inference in Natural Language Processing: Estimation, Prediction, Interpretation and Beyond</title>
      <link>https://tangliyan.com/blog/posts/causal_inference_for/</link>
      <pubDate>Mon, 06 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/causal_inference_for/</guid>
      <description>Authors: Amir Feder, Katherine A. Keith, Emaad Manzoor, Reid Pryzant, Dhanya Sridhar, Zach Wood-Doughty, Jacob Eisenstein, Justin Grimmer, Roi Reichart, Margaret E. Roberts, Brandon M. Stewart, Victor Veitch, Diyi Yang</description>
    </item>
    
    <item>
      <title>Paper Review - Flexible Operations for Natural Language Deduction</title>
      <link>https://tangliyan.com/blog/posts/flexible_operations_for/</link>
      <pubDate>Thu, 02 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/flexible_operations_for/</guid>
      <description>Authors: Kaj Bostrom, Xinyu Zhao, Swarat Chaudhuri, Greg Durrett</description>
    </item>
    
    <item>
      <title>Paper Review - Discourse-Aware Neural Extractive Text Summarization</title>
      <link>https://tangliyan.com/blog/posts/discourse_aware_neural/</link>
      <pubDate>Sat, 21 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/discourse_aware_neural/</guid>
      <description>Authors: Jiacheng Xu, Zhe Gan, Yu Cheng, Jingjing Liu</description>
    </item>
    
    <item>
      <title>Paper Review - Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval</title>
      <link>https://tangliyan.com/blog/posts/approximate_nearst_neighbor/</link>
      <pubDate>Thu, 19 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/approximate_nearst_neighbor/</guid>
      <description>Authors: Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, Arnold Overwijk</description>
    </item>
    
    <item>
      <title>Paper Review - REALM: Retrieval-Augmented Language Model Pre-Training</title>
      <link>https://tangliyan.com/blog/posts/realm/</link>
      <pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/realm/</guid>
      <description>Authors: Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang</description>
    </item>
    
    <item>
      <title>Paper Review - Joint Retrieval and Generation Training for Grounded Text Generation</title>
      <link>https://tangliyan.com/blog/posts/joint_retrieval_and/</link>
      <pubDate>Mon, 16 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/joint_retrieval_and/</guid>
      <description>Authors: Yizhe Zhang, Siqi Sun, Xiang Gao, Yuwei Fang, Chris Brockett, Michel Galley, Jianfeng Gao, Bill Dolan</description>
    </item>
    
    <item>
      <title>Paper Review - Self-Supervised Meta-Learning for Few-Shot Natural Language Classification Tasks</title>
      <link>https://tangliyan.com/blog/posts/self_supervised_meta/</link>
      <pubDate>Sat, 14 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/self_supervised_meta/</guid>
      <description>Authors: Trapit Bansal, Rishikesh Jha, Tsendsuren Munkhdalai, Andrew McCallum</description>
    </item>
    
    <item>
      <title>Paper Review - Noisy Channel Language Model Prompting for Few-Shot Text Classification</title>
      <link>https://tangliyan.com/blog/posts/noisy_channel_language/</link>
      <pubDate>Thu, 12 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/noisy_channel_language/</guid>
      <description>Authors: Sewon Min, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer</description>
    </item>
    
    <item>
      <title>Paper Review - TWAG: A Topic-guided Wikipedia Abstract Generator</title>
      <link>https://tangliyan.com/blog/posts/twag/</link>
      <pubDate>Wed, 11 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/twag/</guid>
      <description>Authors: Fangwei Zhu, Shangqing Tu, Jiaxin Shi, Juanzi Li, Lei Hou, Tong Cui</description>
    </item>
    
    <item>
      <title>Paper Review - Improving Factual Consistency of Abstractive Summarization via Question Answering</title>
      <link>https://tangliyan.com/blog/posts/improving_factual_consistency/</link>
      <pubDate>Tue, 10 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/improving_factual_consistency/</guid>
      <description>Authors: Feng Nan, Cicero Nogueira dos Santos, Henghui Zhu, Patrick Ng, Kathleen McKeown, Ramesh Nallapati, Dejiao Zhang, Zhiguo Wang, Andrew O. Arnold, Bing Xiang</description>
    </item>
    
    <item>
      <title>Paper Review - Dissecting Generation Modes for Abstractive Summarization Models via Ablation and Attribution</title>
      <link>https://tangliyan.com/blog/posts/dissecting_generation_modes/</link>
      <pubDate>Mon, 09 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/dissecting_generation_modes/</guid>
      <description>Authors: Jiacheng Xu, Greg Durrett</description>
    </item>
    
    <item>
      <title>Paper Review - Generating Query Focused Summaries from Query-Free Resources</title>
      <link>https://tangliyan.com/blog/posts/generating_query_focused/</link>
      <pubDate>Sun, 08 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/generating_query_focused/</guid>
      <description>Authors: Yumo Xu, Mirella Lapata</description>
    </item>
    
    <item>
      <title>Paper Review - BLEURT: Learning Robust Metrics for Text Generation</title>
      <link>https://tangliyan.com/blog/posts/bleurt/</link>
      <pubDate>Sat, 07 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/bleurt/</guid>
      <description>Authors: Thibault Sellam, Dipanjan Das, Ankur P. Parikh</description>
    </item>
    
    <item>
      <title>Paper Review - Big Bird: Transformers for Longer Sequences</title>
      <link>https://tangliyan.com/blog/posts/bigbird/</link>
      <pubDate>Fri, 06 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/bigbird/</guid>
      <description>Authors: Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed</description>
    </item>
    
    <item>
      <title>Paper Review - LongSumm 2021: Session based automatic summarization model for scientific document</title>
      <link>https://tangliyan.com/blog/posts/long_summ_2021/</link>
      <pubDate>Thu, 05 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/long_summ_2021/</guid>
      <description>Authors: Senci Ying, Yanzhao Zheng, Wuhe Zou</description>
    </item>
    
    <item>
      <title>Paper Review - Annotating and Modeling Fine-grained Factuality in Summarization</title>
      <link>https://tangliyan.com/blog/posts/annotating_and_modeling/</link>
      <pubDate>Wed, 04 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/annotating_and_modeling/</guid>
      <description>Authors: Tanya Goyal, Greg Durrett</description>
    </item>
    
    <item>
      <title>Paper Review - GSum: A General Framework for Guided Neural Abstractive Summarization</title>
      <link>https://tangliyan.com/blog/posts/gsum/</link>
      <pubDate>Tue, 03 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/gsum/</guid>
      <description>Authors: Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao Jiang, Graham Neubig</description>
    </item>
    
    <item>
      <title>Paper Review - Nutri-bullets Hybrid: Consensual Multi-document Summarization</title>
      <link>https://tangliyan.com/blog/posts/nutri_bullets/</link>
      <pubDate>Mon, 02 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/nutri_bullets/</guid>
      <description>Authors: Darsh Shah, Lili Yu, Tao Lei, Regina Barzilay</description>
    </item>
    
    <item>
      <title>Paper Review - CLIP: A Dataset for Extracting Action Items for Physicians from Hospital Discharge Notes</title>
      <link>https://tangliyan.com/blog/posts/clip/</link>
      <pubDate>Sun, 01 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/clip/</guid>
      <description>Authors: James Mullenbach, Yada Pruksachatkun, Sean Adler, Jennifer Seale, Jordan Swartz, T. Greg McKelvey, Hui Dai, Yi Yang, David Sontag</description>
    </item>
    
    <item>
      <title>Paper Review - Noisy Self-Knowledge Distillation for Text Summarization</title>
      <link>https://tangliyan.com/blog/posts/noisy_sele_knowledge/</link>
      <pubDate>Sat, 31 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/noisy_sele_knowledge/</guid>
      <description>Authors: Yang Liu, Sheng Shen, Mirella Lapata</description>
    </item>
    
    <item>
      <title>Paper Review - PAIR: Planning and Iterative Refinement in Pre-trained Transformers for Long Text Generation</title>
      <link>https://tangliyan.com/blog/posts/pair/</link>
      <pubDate>Fri, 30 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/pair/</guid>
      <description>Authors: Xinyu Hua, Lu Wang</description>
    </item>
    
    <item>
      <title>Paper Review - Improving Faithfulness in Abstractive Summarization with Contrast Candidate Generation and Selection</title>
      <link>https://tangliyan.com/blog/posts/improving_faithfulness_in/</link>
      <pubDate>Thu, 29 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/improving_faithfulness_in/</guid>
      <description>Authors: Sihao Chen, Fan Zhang, Kazoo Sone, Dan Roth</description>
    </item>
    
    <item>
      <title>Paper Review - Scoring Sentence Singletons and Pairs for Abstractive Summarization</title>
      <link>https://tangliyan.com/blog/posts/scoring_sentence_singletons/</link>
      <pubDate>Thu, 29 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/scoring_sentence_singletons/</guid>
      <description>Authors: Logan Lebanoff, Kaiqiang Song, Franck Dernoncourt, Doo Soon Kim, Seokhwan Kim, Walter Chang, Fei Liu</description>
    </item>
    
    <item>
      <title>Paper Review - Efficient Attentions for Long Document Summarization</title>
      <link>https://tangliyan.com/blog/posts/efficient_attentions_for/</link>
      <pubDate>Wed, 28 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/efficient_attentions_for/</guid>
      <description>Authors: Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, Lu Wang</description>
    </item>
    
    <item>
      <title>Paper Review - AdaptSum: Towards Low-Resource Domain Adaptation for Abstractive Summarization</title>
      <link>https://tangliyan.com/blog/posts/adaptsum/</link>
      <pubDate>Tue, 27 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/adaptsum/</guid>
      <description>Authors: Tiezheng Yu, Zihan Liu, Pascale Fung</description>
    </item>
    
    <item>
      <title>Paper Review - FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization</title>
      <link>https://tangliyan.com/blog/posts/feqa/</link>
      <pubDate>Tue, 27 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/feqa/</guid>
      <description>Authors: Esin Durmus, He He, Mona Diab</description>
    </item>
    
    <item>
      <title>Paper Review - Neural Text Summarization: A Critical Evaluation</title>
      <link>https://tangliyan.com/blog/posts/neural_text_summarization/</link>
      <pubDate>Mon, 26 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/neural_text_summarization/</guid>
      <description>Authors: Wojciech Kryściński, Nitish Shirish Keskar, Bryan McCann, Caiming Xiong, Richard Socher</description>
    </item>
    
    <item>
      <title>Paper Review - On Faithfulness and Factuality in Abstractive Summarization</title>
      <link>https://tangliyan.com/blog/posts/on_faithfulness_and/</link>
      <pubDate>Mon, 26 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/on_faithfulness_and/</guid>
      <description>Authors: Joshua Maynez, Shashi Narayan, Bernd Bohnet, Ryan McDonald</description>
    </item>
    
    <item>
      <title>Paper Review - Learning Neural Templates for Text Generation</title>
      <link>https://tangliyan.com/blog/posts/learning_neural_templates/</link>
      <pubDate>Sun, 25 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/learning_neural_templates/</guid>
      <description>Authors: Sam Wiseman, Stuart M. Shieber, Alexander M. Rush</description>
    </item>
    
    <item>
      <title>Paper Review - Bottom-Up Abstractive Summarization</title>
      <link>https://tangliyan.com/blog/posts/bottom_up_sum/</link>
      <pubDate>Fri, 23 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/bottom_up_sum/</guid>
      <description>Authors: Sebastian Gehrmann, Yuntian Deng, Alexander M. Rush</description>
    </item>
    
    <item>
      <title>Paper Review - Improving Zero and Few-Shot Abstractive Summarization with Intermediate Fine-tuning and Data Augmentation</title>
      <link>https://tangliyan.com/blog/posts/improving_zero_and/</link>
      <pubDate>Fri, 23 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/improving_zero_and/</guid>
      <description>Authors: Alexander R. Fabbri, Simeng Han, Haoyuan Li, Haoran Li, Marjan Ghazvininejad, Shafiq Joty, Dragomir Radev, Yashar Mehdad</description>
    </item>
    
    <item>
      <title>Overlook of Relation Extraction</title>
      <link>https://tangliyan.com/blog/posts/relation_extraction/</link>
      <pubDate>Sun, 03 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/relation_extraction/</guid>
      <description>Information Extraction v.s. Relation Extraction Information Extraction: Information extraction is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents and other electronically represented sources.
Relation extraction (RE) is an important task in IE. It focuses on extracting relations between entities. A complete relation RE system consists of
 a named entity recognizer to identify named entities from text. an entity linker to link entities to existing knowledge graphs.</description>
    </item>
    
    <item>
      <title>Paper Review - Connecting the Dots: Document-level Neural Relation Extraction with Edge-oriented Graphs</title>
      <link>https://tangliyan.com/blog/posts/connecting_the_dots/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/connecting_the_dots/</guid>
      <description>Authors: Fenia Christopoulou, Makoto Miwa, Sophia Ananiadou</description>
    </item>
    
    <item>
      <title>Cross-Lingual Learning</title>
      <link>https://tangliyan.com/blog/posts/corss_lingual/</link>
      <pubDate>Mon, 28 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/corss_lingual/</guid>
      <description>Cross-lingual learning Most languages do not have training data available to create state-of-the-art models and thus our ability to create intelligent systems for these languages is limited as well.
Cross-lingual learning (CLL) is one possible remedy to solve the lack of data for low-resource languages. In essence, it is an effort to utilize annotated data from other languages when building new NLP models. When CLL is considered, target languages usually lack resources, while source languages are resource-rich and they can be used to improve the results for the former.</description>
    </item>
    
    <item>
      <title>BERT and RoBERTa </title>
      <link>https://tangliyan.com/blog/posts/bert_roberta/</link>
      <pubDate>Fri, 18 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/bert_roberta/</guid>
      <description>BERT Recap Overview  Bert (Bidirectional Encoder Representations from Transformers) uses a &amp;ldquo;masked language model&amp;rdquo; to randomly mask some tokens from the input and predict the original vocabulary id of the masked token. Bert shows that &amp;ldquo;pre-trained representations reduce the need for many heavily-engineered task-specific architectures&amp;rdquo;.  BERT Specifics There are two steps to the BERT framework: pre-training and fine-tuning   During pre training, the model is trained on unlabeled data over different pre-training tasks.</description>
    </item>
    
    <item>
      <title>Paper Review - What Does BERT Look At? An Analysis of BERT’s Attention</title>
      <link>https://tangliyan.com/blog/posts/bert_attn/</link>
      <pubDate>Mon, 14 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/bert_attn/</guid>
      <description>Authors: Kevin Clark, Urvashi Khandelwal, Omer Levy, Christopher D. Manning</description>
    </item>
    
    <item>
      <title>Kaggle: Jigsaw Multilingual Toxic Comment Classification - top solutions</title>
      <link>https://tangliyan.com/blog/posts/kaggle_jigsaw/</link>
      <pubDate>Tue, 11 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/kaggle_jigsaw/</guid>
      <description>Before we start Two of my previous post might be helpful in getting a general understanding of the top solutions of this competition. Please feel free to check them out.
 Knowledge Distillation clearly explained Common Multilingual Language Modeling methods (M-Bert, LASER, MultiFiT, XLM)  Jigsaw Multilingual Toxic Comment Classification  Use TPUs to identify toxicity comments across multiple languages.
 Overview of the competition Jigsaw Multilingual Toxic Comment Classification is the 3rd annual competition organized by the Jigsaw team.</description>
    </item>
    
    <item>
      <title>Multi-lingual: M-Bert, LASER, MultiFiT, XLM</title>
      <link>https://tangliyan.com/blog/posts/multilingual/</link>
      <pubDate>Sat, 08 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/multilingual/</guid>
      <description>Multilingual Models are a type of Machine Learning model that can understand different languages. In this post, I&amp;rsquo;m going to discuss four common multi-lingual language models Multilingual-Bert (M-Bert), Language-Agnostic SEntence Representations (LASER Embeddings), Efficient multi-lingual language model fine-tuning (MultiFiT) and Cross-lingual Language Model (XLM).
Ways of tokenization Word-based tokenization Word-based tokenization works well for the morphologically poor English, but results in very large and sparse vocabularies for morphologically rich languages, such as Polish and Turkish.</description>
    </item>
    
    <item>
      <title>Kaggle: Tweet Sentiment Extraction - top solutions</title>
      <link>https://tangliyan.com/blog/posts/kaggle_tweet_sent2/</link>
      <pubDate>Thu, 02 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/kaggle_tweet_sent2/</guid>
      <description>Note This post is the second part of overall summarization of the competition. The first half is here.
Noteworthy ideas in 1st place solution Idea First step:
Use transformers to extract token level start and end probabilities.
Second step:
Feed these probabilities to a character level model. This step gives the team a huge improve on the final score since it handled the &amp;ldquo;noise&amp;rdquo; in the data properly.
Last step:</description>
    </item>
    
    <item>
      <title>Kaggle: Tweet Sentiment Extraction - common methods</title>
      <link>https://tangliyan.com/blog/posts/kaggle_tweet_sent1/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/kaggle_tweet_sent1/</guid>
      <description>Note This post is the first part of overall summarization of the competition. The second half is here.
Before we start I attended two NLP competition in June, Tweet Sentiment Extraction and Jigsaw Multilingual Toxic Comment Classification, and I&amp;rsquo;m happy to be a Kaggle Expert from now on :)
Tweet Sentiment Extraction Goal:
The objective in this competition is to &amp;ldquo;Extract support phrases for sentiment labels&amp;rdquo;. More precisely, this competition asks kagglers to construct a model that can figure out what word or phrase best supports the given tweet from the labeled sentiment.</description>
    </item>
    
    <item>
      <title>Recurrent Neural Network (RNN) and Long Short Term Memory (LSTM)</title>
      <link>https://tangliyan.com/blog/posts/lstm/</link>
      <pubDate>Thu, 04 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/lstm/</guid>
      <description>Sequence Data There are many sequence data in applications. Here are some examples
  Machine translation
 from text sequence to text sequence.    Text Summarization
 from text sequence to text sequence.    Sentiment classification
 from text sequence to categories.    Music Generation
 from nothing or some simple stuff (character, integer, etc) to wave sequence.    Name entity recognition (NER)</description>
    </item>
    
    <item>
      <title>Log-Linear Model, Conditional Random Field(CRF)</title>
      <link>https://tangliyan.com/blog/posts/crf/</link>
      <pubDate>Tue, 19 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/crf/</guid>
      <description>Log-Linear model Let $x$ be an example, and let $y$ be a possible label for it. A log-linear model assumes that
$$ p(y | x ; w)=\frac{\exp [\sum_{j=1}^J w_{j} F_{j}(x, y)]}{Z(x, w)} $$
where the partition function
$$ Z(x, w)=\sum_{y^{\prime}} \exp [\sum_{j=1}^J w_{j} F_{j}\left(x, y^{\prime}\right)] $$
Note that in $\sum_{y^{\prime}}$, we make a summation over all possible $y$. Therefore, given $x$, the label predicted by the model is
$$ \hat{y}=\underset{y}{\operatorname{argmax}} p(y | x ; w)=\underset{y}{\operatorname{argmax}} \sum_{j=1}^J w_{j} F_{j}(x, y) $$</description>
    </item>
    
    <item>
      <title>Hidden Markov Model (HMM)</title>
      <link>https://tangliyan.com/blog/posts/hmm/</link>
      <pubDate>Sun, 03 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/hmm/</guid>
      <description>Before reading this post, make sure you are familiar with the EM Algorithm and decent among of knowledge of convex optimization. If not, please check out my previous post
  EM Algorithm
  convex optimization primal and dual problem
  Let&amp;rsquo;s get started!
Conditional independence $A$ and $B$ are conditionally independent given $C$ if and only if, given knowledge that $C$ occurs, knowledge of whether $A$ occurs provides no information on the likelihood of $B$ occurring, and knowledge of whether $B$ occurs provides no information on the likelihood of $A$ occurring.</description>
    </item>
    
    <item>
      <title>Skip-gram</title>
      <link>https://tangliyan.com/blog/posts/skipgram/</link>
      <pubDate>Sat, 18 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/skipgram/</guid>
      <description>Comparison between CBOW and Skip-gram The major difference is that skip-gram is better for infrequent words than CBOW in word2vec. For simplicity, suppose there is a sentence &amp;ldquo;$w_1w_2w_3w_4$&amp;rdquo;, and the window size is $1$.
For CBOW, it learns to predict the word given a context, or to maximize the following probability
$$ p(w_2|w_1,w_3) \cdot P(w_3|w_2,w_4)$$
This is an issue for infrequent words, since they don’t appear very often in a given context.</description>
    </item>
    
    <item>
      <title>NLP Basics, Spell Correction with Noisy Channel</title>
      <link>https://tangliyan.com/blog/posts/nlp_basic/</link>
      <pubDate>Fri, 10 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/nlp_basic/</guid>
      <description>NLP = NLU + NLG  NLU: Natural Language Understanding NLG: Natural Language Generation  NLG may be viewed as the opposite of NLU: whereas in NLU, the system needs to disambiguate the input sentence to produce the machine representation language, in NLG the system needs to make decisions about how to put a concept into human understandable words.
Classical applications in NLP   Question Answering
  Sentiment Analysis</description>
    </item>
    
    <item>
      <title>Kaggle: Google Quest Q&amp;A Labeling - my solution</title>
      <link>https://tangliyan.com/blog/posts/kaggle_google_quest/</link>
      <pubDate>Sat, 04 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/kaggle_google_quest/</guid>
      <description>Kaggle: Google Quest Q&amp;amp;A Labeling summary General Part Congratulations to all winners of this competition. Your hard work paid off!
First, I have to say thanks to the authors of the following three published notebooks:
https://www.kaggle.com/akensert/bert-base-tf2-0-now-huggingface-transformer, https://www.kaggle.com/abhishek/distilbert-use-features-oof, https://www.kaggle.com/codename007/start-from-here-quest-complete-eda-fe.
These notebooks showed awesome ways to build models, visualize the dataset and extract features from non-text data.
Our initial plan was to take question title, question body and answer all into a Bert based model.</description>
    </item>
    
  </channel>
</rss>
