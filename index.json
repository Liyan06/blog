[{"content":"Authors: Philippe Laban, Tobias Schnabel, Paul N. Bennett, Marti A. Hearst Paper reference: https://arxiv.org/pdf/2111.09525.pdf\nContribution This paper proposes two models $SummaC_{ZS}$ (directly interpretable), $SummaC_{Conv}$ for inconsistency detection based on the aggregation of sentence-level entailment scores for each pair of input document and summary sentences.\nExperiments show that the choice of granularity affect the performance of models and combinations like (one sentence, one sentence) and (two sentences, one sentence) lead to the best performance (as NLI datasets are predominantly represented at the sentence level). Also the choice of NLI dataset has a stronger influence on overall performance compared to model architectures.\nIntroduce a SUMMAC Benchmark by standardizing existing six summary inconsistency detection datasets.\nDetails For each of (document, summary) pair, the document and the summary are divided into sentences and form a pair matrix.\n  $SummaC_{ZS}$ performs zero-shot aggregation by combining sentence-level scores using max and mean operators (parameters free). Disadvantage: sensitive to extrema, which can be noisy due to the presence of outliers and the imperfect nature of NLI models.\n  $SummaC_{Conv}$ is a trained model consisting of a single learned convolution layer to take into account the distribution of entailment scores for each summary sentence.\n  Limitations In the case of a summary performing a sentence fusion operation, an NLI model might not be able to correctly predict entailment of the fused sentence, seeing only one sen- tence at a time.\nIs it possible to dynamically selecting a granular level for each (document, summary) pair?\n","permalink":"https://tangliyan.com/blog/posts/summac/","summary":"Authors: Philippe Laban, Tobias Schnabel, Paul N. Bennett, Marti A. Hearst","title":"Paper Review - SUMMAC: Re-Visiting NLI-based Models for Inconsistency Detection in Summarization"},{"content":"Authors: Mingkai Deng, Bowen Tan, Zhengzhong Liu, Eric P. Xing, Zhiting Hu Paper reference: https://aclanthology.org/2021.emnlp-main.599.pdf\nContribution This paper use information alignment to define evaluation metics that is capable of measuring many key aspects of NLG tasks (dacpturing both consistency and relevance for summarization). Experiment on previously human annotated data show that their metrics achieve stronger or comparable correlations with human judgement compared to state-of-the-art metrics. [I just focus on the summarization part in the paper.]\nDetails Information Alignment Def (Information Alignment) Let $\\mathbf{a}$ be a piece of text of length $N$; $\\mathbf{b}$ be arbitrary data. The information alignment from text $\\mathbf{a}$ to $\\mathbf{b}$ is a vector of alignment scores: $$ \\operatorname{align}(\\mathbf{a} \\rightarrow \\mathbf{b})=\\left\\langle\\alpha_{1}, \\alpha_{2}, \\ldots, \\alpha_{N}\\right\\rangle $$ where $\\alpha_{n} \\in[0,1]$ is the confidence that the information of the $n$-th token in $\\mathbf{a}$ is grounded by $\\mathbf{b}$, i.e., the $n$-th token aligns with $\\mathbf{b}$.\nIn summarization (source $x$, output $y$):\n consistency score can be defined as the average alignment scores of tokens in $y$ w.r.t. $x$, i.e. $$ \\operatorname{CONSISTENCY}(\\mathbf{y}, \\mathbf{x})=\\operatorname{mean}(\\operatorname{align}(\\mathbf{y} \\rightarrow \\mathbf{x})) $$ Relevance can be defined as the following (I think this is measuring both precision and recall): $$ \\operatorname{RELEVANCE}(\\mathbf{y}, \\mathbf{x}, \\mathbf{r})= \\operatorname{mean}(\\operatorname{align}(\\mathbf{r} \\rightarrow \\mathbf{y})) \\times \\operatorname{mean}(\\operatorname{align}(\\mathbf{y} \\rightarrow \\mathbf{x})) $$  Alignment Estimation  Embedding Matching: clear in the figure. Discriminative Model: formulate the information alignment problem as sequence tagging. Aggregated Regression: similar to discriminative model, but directly estimate the single aggregated alignment score, such as mean and sum.  Training sequence tagging model Alignment models are trained by constructing weakly supervised data using texts in the domain of evaluation. It consist of following steps:\n Prepare (input $x$, target $y$) pairs. Paraphrase $y$ into $y'$ to make model more robust. Predict randomly masked $y'$ using a generation model. Label the infilled words as \u0026ldquo;not aligned\u0026rdquo; with $x$, and other words as \u0026ldquo;aligned\u0026rdquo;.  {(($x, y'$), labels on $y'$)} will be the training data.\nThey use the document as x, and generate its pseudo-summaries as y using TextRank. Reference summaries are not sued since they can contain hallucinations that don’t align with the article.\n","permalink":"https://tangliyan.com/blog/posts/compression_transduction_and/","summary":"Authors: Mingkai Deng, Bowen Tan, Zhengzhong Liu, Eric P. Xing, Zhiting Hu","title":"Paper Review - Compression, Transduction, and Creation: A Unified Framework for Evaluating Natural Language Generation"},{"content":"Authors: Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig Paper reference: https://arxiv.org/pdf/2107.13586.pdf\nIntroduction To use prompt-based models to perform prediction tasks, the original input $x$ is modified using a template into a textual string prompt $x'$ that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string $\\hat{x}$, from which the final output $y$ can be derived. The advantage of this method is that, given a suite of appropriate prompts, a single LM trained in an entirely unsupervised fashion can be used to solve a great number of tasks. However, this method introduces the necessity for prompt engineering, finding the most appropriate prompt to allow a LM to solve the task at hand.\nIt allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this paradigm, downstream tasks are reformulated to look more like those solved during the original LM training with the help of a textual prompt. In this way, by selecting the appropriate prompts we can manipulate the model behavior so that the pre-trained LM itself can be used to predict the desired output, sometimes even without any additional task-specific training.\nPrompt Basics A prompting function $f_{\\text {prompt }}(\\cdot)$ is applied to modify the input text $x$ into a prompt $x^{\\prime}=f_{\\text {prompt}}(x)$. This function consists of a two step process: \\\n Apply a template, which is a textual string that has two slots: an input slot $[X]$ for input $x$ and an answer slot $[Z]$ for an intermediate generated answer text $z$ that will later be mapped into $y$. \\ Fill slot $[X]$ with the input text $x$.  Prompt Engineering Prompt engineering is the process of creating a prompting function $f_{\\text {prompt}}(x)$ that results in the most effective performance on the downstream task. Annotated training samples are often used in the construction or validation of the prompts that the downstream task will use.\nShapes of prompts: close prompts and prefix prompts.\nManual Template Engineering Manually create intuitive templates based on human introspection. Some drawbacks: (1) creating and experimenting with these prompts takes time and experience. (2) Optimal prompts are not easy to discover.\nAutomated Template Learning It can be devided into (1) discrete prompts and continuous prompts; (2) static and dynamic prompts.\nDiscrete Prompts  Prompt Mining: Find templates given a set of training inputs $x$ and outputs $y$ by scraping a large text corpus (e.g. Wikipedia). Prompt Paraphrasing: Take in an existing seed prompt (e.g. manually constructed or mined), paraphrases it into a set of other candidate prompts, then picks ones that achieve the highest training accuracy. Gradient-based Search: Trigger the underlying pre-trained LM to generate the desired target prediction by a gradient-based search over actual tokens. Prompt Generation: Use generation models to generate templates.  Continuous Prompts  Prefix Tuning: Prepends a sequence of trainable continuous task-specific vectors to the input. Tuning Initialized with Discrete Prompts: Initialize the search for a continuous prompt using a prompt that has already been created or discovered using discrete prompt search methods. Hard-Soft Prompt Hybrid Tuning: Insert some tunable embeddings into a manual prompt template.  Answer Engineering Answer engineering aims to search for an answer space $Z$ and a map to the original output $Y$ that results in an effective predictive model.\nShapes of answers: tokens, spans, and sentences.\nManual Answer Design Unconstrained Spaces In many cases, the answer space $Z$ is the space of all tokens. In these cases, it is most common to use the identity mapping.\nConstrained Spaces This is often performed for tasks with a limited label space such as text classification. In these cases, it is necessary to have a mapping between the answer $Z$ and the underlying class $Y$.\nAs with manually created prompts, it is possible that manually created answers are sub-optimal for getting the LM to achieve ideal prediction performance.\nAutomated Answer Search Discrete Answer Search  Answer Paraphrasing: Start with an initial answer space $Z'$ , and then use paraphrasing to expand this answer space to broaden its coverage. Prune-then-Search: An initial pruned answer space of several plausible answers $Z'$ is generated, and then an algorithm further searches over this pruned space to select a final set of answers.  Continuous Answer Search Very few works exist.\nMulti-Prompt Learning Extend the single prompt learning to the use multiple prompts.\nPrompt Ensembling Advantages: (1) leverage the complementary advantages of different prompts; (2) alleviate the cost of prompt engineering, since choosing one best-performing prompt is challenging; (3) stabilize performance on downstream tasks.\n Uniform averaging: Combine the predictions by taking the average of probabilities from different prompts. Weighted averaging: Each prompt is associated with a weight. The weights are typically pre-specified based on prompt performance or optimized using a training set. Majority voting: Combine results for classification tasks. etc.  Prompt Augmentation Prompt augmentation provides a few additional answered prompts that can be used to demonstrate how the LM should provide the answer to the actual prompt instantiated with the input $x$. It leverages the template and answer, while larger context learning does not.\n Sample Selection: The choice of examples used in this few-shot scenario can result in very different performance, ranging from near state-of-the-art accuracy on some tasks to near random guess. Sample Ordering: The order of answered prompts provided to the model plays an important role in model performance.  Prompt Composition \u0026amp; Prompt Decomposition Use when tasks are based on more fundamental subtasks or multiple predictions should be performed for one sample (e.g., sequence labeling).\nTraining Strategies for Prompting Methods ","permalink":"https://tangliyan.com/blog/posts/pretrain_prompt/","summary":"Authors: Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig","title":"Paper Review - Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"},{"content":"Authors: Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Furu Wei Reference: https://arxiv.org/pdf/2104.08696.pdf\nContribution This paper proposes a knowledge attribution method to identify the (knowledge) neurons that express facts stored in pretrained Transformers. Specifically, the paper views feed-forward network (i.e., two layer perceptron) modules as knowledge memories in Transformer. The hidden state is fed into the feed-forward network and activates the knowledge neurons (keys). Then the second linear layer outputs the corresponding memory vectors (values).\nThe paper finds that suppressing and amplifying knowledge neurons can control the expressions of the corresponding knowledge, without effecting other facts, and that knowledge-probing prompts tend to activate knowledge neurons of the specific fact.\nIt then describes how to leverage knowledge neurons to explicitly edit (update/erase) factual knowledge in Transformers without any fine-tuning.\nDetails Locating Factual Knowledge The paper regards FFNs in Transformer as key-value memories, where the first layer serves as keys, the second layer serves as values, and each key-value pair forms a memory slot. Based on this analogy, it hypothesizes that factual knowledge is stored in FFN memories and expressed by the corresponding intermediate neurons, which we call knowledge neurons.\nIt proposes a knowledge attribution method and a refining strategy to identify knowledge neurons.\nKnowledge Assessing Task Given a triplet $\\langle h, r, t\\rangle$, the knowledge assessing task requires a pretrained model to answer a cloze-style query $x$ (aka knowledge-probing prompt), which expresses the relational fact but leaves the tail entity as a blank. Then compare the model prediction with ground truth.\nKnowledge Attribution The paper identifies knowledge neurons that contribute the most to the knowledge expression by Integrated Gradients.\nWith the knowledge attribution method, given a relational fact along with a prompt, we can coarsely locate the factual knowledge to neurons with attribution scores greater than a given threshold. We call these neurons coarse knowledge neurons.\nKnowledge Neuron Refining The main idea is to produce different and diverse prompts expressing the same fact, and then retain neurons shared by many of these prompts.\nUse case: Updating/Erasing Updating Knowledge Given a relational fact $\\langle h, r, t\\rangle$ remembered by a pretrained model, we aim to update it to $\\left\\langle h, r, t^{\\prime}\\right\\rangle$. We directly subtract $\\mathbf{t}$ from corresponding value slots (in the second feed forward layer $\\mathrm{FF}^{(\\text {val})}$ ), and add $\\mathbf{t}^{\\prime}$ to those value slots, where $\\mathbf{t}$ and $\\mathbf{t}^{\\prime}$ are the word embeddings of $t$ and $t^{\\prime}$, respectively.\nErasing Knowledge Goal: Erase all the relational facts with a specified relation in the model.\nFirst we identify knowledge neurons of all the relational facts. Then, we retain knowledge neurons that belong to at least $m$ relational facts (e.g. $m=5$), and filter out others. Finally, we set the value slots corresponding to these knowledge neurons to the word embedding of $[\\mathrm{UNK}]$.\nConcern: May influence the expression of other knowledge.\n","permalink":"https://tangliyan.com/blog/posts/knowledge_neurons/","summary":"Authors: Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Furu Wei","title":"Paper Review - Knowledge Neurons in Pretrained Transformers"},{"content":"Authors: Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul Christiano\\ Paper reference: https://arxiv.org/pdf/2009.01325.pdf\nContribution The paper shows that it is possible to significantly improve summary quality by training a model to optimize for human preferences (see details). Human feedback models generalize much better to new domains than supervised models. The paper motivates researchers to pay closer attention to how their training loss affects the model behavior they actually want.\nMoney, Money , and Money.\nDetails Misaligned Summarization Models Summarization models are usually fine-tuned using supervised learning, often to maximize the log probability of a set of human demonstrations. However, there is still a misalignment between this fine-tuning objective—maximizing the likelihood of human-written text—and what we care about—generating high-quality outputs as determined by humans.\nCauses of misalignment: (1) the maximum likelihood objective has no distinction between important errors (e.g. making up facts) and unimportant errors (e.g. selecting the precise word from a set of synonyms); (2) models are incentivized to place probability mass on all human demonstrations, including those that are low-quality; (3) distributional shift during sampling can degrade performance; \\\nQuality can often be improved significantly by non-uniform sampling strategies such as beam search, but these can lead to repetition and other undesirable artifacts. Optimizing for quality may be a principled approach to overcoming these problems.\nMethods Methods consist of three steps (use the TL;DR Reddit post dataset):\nStep 1: Collect samples from existing policies and send comparisons to humans. For each Reddit post, we sample summaries from several sources including the current policy, initial policy, original reference summaries and various baselines. We send a batch of pairs of summaries to our human evaluators, who are tasked with selecting the best summary of a given Reddit post.\nStep 2: Learn a reward model from human comparisons. Given a post and a candidate summary, we train a reward model to predict the log odds that this summary is the better one, as judged by our labelers (predict human-preferred summary).\nStep 3: Optimize a policy against the reward model. We treat the logit output of the reward model as a reward to fine-tune a summarization policy using reinforcement learning.\n","permalink":"https://tangliyan.com/blog/posts/learning_to_summarize/","summary":"Authors: Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul Christiano","title":"Paper Review - Learning to summarize from human feedback"},{"content":"Authors: Meng Cao, Yue Dong, Jackie Chi Kit Cheung Paper reference: https://arxiv.org/pdf/2109.09784.pdf\nContribution This paper proposes an approach that can detect and separate factual from non-factual hallucinations of entities in abstractive summarization based on an entity’s prior and posterior probabilities according to MLMs.\nExperiments show that more than half of the hallucinated entities are factual with respect to the source document and world knowledge, and the knowledge of many factual hallucinations comes from the training set.\nNote: The paper focuses on extrinsic hallucinations only.\nDetails Problem Formulation For an entity $e_k$ in a generated summary $G$, the task is to determine whether $e_k$ is hallucinated, and whether it is factual.\nThe Prior \u0026amp; Posterior Probability of an Entity Prior probability: the probability of an entity $e_k$ being in a summary $G$ without considering the source document $S$. It\u0026rsquo;s measured by a MLM (BART-Large), where $c_k = G \\setminus e_k$:\n$$ \\begin{aligned} p_{\\text {prior }}\\left(e_{k}\\right) \u0026amp;=P_{\\text {MLM }}\\left(e_{k} \\mid c_{k}\\right) \\\\ \u0026amp;=\\prod_{t=1}^{\\left|e_{k}\\right|} P_{\\text {MLM }}\\left(e_{k}^{t} \\mid e_{k}^{1 \\ldots . t-1}, c_{k}\\right) \\end{aligned} $$\nPosterior Probability: the probability of an entity being in a summary considering the source document. It\u0026rsquo;s measured by a conditional MLM (CMLM) using BART-Large.\n$$ p_{\\text {pos }}\\left(e_{k}\\right)=P_{\\mathrm{CMLM}}\\left(e_{k} \\mid c_{k}, S\\right) $$\nTraining for CMLM CMLM is trained on XSum or CNN/Dailymail. For each reference summary, randomly select one entity and mask it with a special [MASK] token. The training target is the reference summary without any masking given the document and the masked summary as input.\nTraining a Discriminator The paper trains a KNN classifier (k=15 in the paper), using the prior and posterior probabilities, plus a binary overlap feature that indicates whether the entity appears in the document.\nHere is a visualization of trained KNN with bounds: Rules for Determine Hallucination (1) If the entity can be directly entailed from the source document, then the entity is non-hallucinated; (2) Otherwise, decide whether the entity is factual using world knowledge; (3) If there is no information found to prove or disprove the hallucinated entity, it is labeled as non-factual (discarded all intrinsic hallucinations and with a focus on external hallucinations).\n","permalink":"https://tangliyan.com/blog/posts/inspecting_the_factuality/","summary":"Authors: Meng Cao, Yue Dong, Jackie Chi Kit Cheung","title":"Paper Review - Inspecting the Factuality of Hallucinated Entities in Abstractive Summarization"},{"content":"Authors: Chao-Chun Hsu, Chenhao Tan Paper reference: https://arxiv.org/pdf/2109.06896.pdf\nContribution This paper proposes a decision-focused summarization which summarizes relevant information for a decision. The goal is to build a summary by selecting representative sentences that lead to similar model (leveraging a predictive model) decisions as using the full text while accounting for textual non-redundancy.\nDetails Problem Formulation Decision-focused summarization task is to identify the most relevant information $\\tilde{X}$ from the input $X$ for a particular decision $y$ as a summary in support of human decision making. There also exists a training set analogous to supervised learning (for a predictive model $f$), $D_{train} = \\{(X_i , y_i)\\}$, which can provide insights on the relation between the text and the decision.\nDataset A future rating prediction task using Yelp. For each restaurant in Yelp, define $X$ as the text of the first $k=10$ reviews and $y$ is the average rating of the first $t=50$ reviews where $t \\gt k$ so that the task is to forecast future ratings. The problem is to select sentences from a restaurant’s first 10 reviews in support of predicting its future rating after 50 reviews.\nDecSum The goal of the approach is to: (1) develop a model $f$ (Longformer in the paper) to make the decision $y = f(X)$; (2) build summaries that can support $f$ in decision making and account for properties in text-only summarization.\nDecision-focues summaries should satisfy: (1) Decision Faithfulness. The selected sentences should lead to similar decisions as the full text: $f(\\tilde{X}) \\simeq f(X)$: $$ \\mathcal{L}_{\\mathrm{F}}(\\tilde{X}, X, f)=\\log |f(\\tilde{X})-f(X)| $$\n(2) Decision representativeness. The decision distribution of the summary (selected sentenes) $\\hat{Y}_{\\tilde{X}}=\\{f(x) \\mid x \\in \\tilde{X}\\}$ should be close to the decision distribution of all sentences in the full text $\\hat{Y}_{X}=\\{f(x) \\mid x \\in X\\} .$ The second loss function is the logarithm of the Wasserstein distance: $$ \\mathcal{L}_{\\mathrm{R}}(\\tilde{X}, X, f)=\\log \\left(W\\left(\\hat{Y}_{\\tilde{X}}, \\hat{Y}_{X}\\right)\\right) $$\n(3) Textual non-redundancy. The selected sentences should capture diverse contents and provide an overview of the textual information in the input text: $$ \\mathcal{L}_{\\mathrm{D}}(\\tilde{X})=\\sum_{x \\in \\tilde{X}} \\max_{x^{\\prime} \\in \\tilde{X}-{x}} \\operatorname{cossim}\\left(s(x), s\\left(x^{\\prime}\\right)\\right) $$\nwhere the cosine similarity is based on SentBERT sentence representation. The final objective function consists of the above three parts: $$ \\mathcal{L}(\\tilde{X}, X, f)=\\alpha \\mathcal{L}_{\\mathrm{F}}(\\tilde{X}, X, f)+\\beta \\mathcal{L}_{\\mathrm{R}}(\\tilde{X}, X, f)+\\gamma \\mathcal{L}_{\\mathrm{D}}(\\tilde{X}), $$ where $\\alpha, \\beta, \\gamma$ control the tradeoff between the three desiderata.\nImplementation Greedily selects a sentence (with beam search) that minimizes the loss function.\nResults The paper considers servral baselines to extract partial inputs as explanation:\n Text-only summarization baselines such as PreSumm (an extractive summarization method), BERT, randomly select sents; Attribution methods such as IG, attention to rank and select sentences.  Experiments show that baselines are not faithful ($\\log |f(\\tilde{X})-f(X)|$ is big)\n Footnote: The distance between $\\hat{Y}_{\\tilde{X}}$ and $\\hat{Y}_{X}$ is measured by Wasserstein Distance: $$W(\\hat{Y}_{\\tilde{X}}, \\hat{Y}_{X})=\\inf_{\\gamma \\in \\Gamma\\left(\\hat{Y}_{\\tilde{X}}, \\hat{Y}_{X}\\right)} \\int_{\\mathbb{R} \\times \\mathbb{R}} | f-f^{\\prime}|| d \\gamma\\left(f, f^{\\prime}\\right),$$ where $\\Gamma (\\hat{Y}_{\\tilde{X}}, \\hat{Y}_{X})$ denotes the collection of all measures on $\\mathbb{R} \\times \\mathbb{R}$ with marginals $\\hat{Y}_{\\tilde{X}}$ and $\\hat{Y}_{X}$ on the first and second factors respectively.\n","permalink":"https://tangliyan.com/blog/posts/decision_focused_sum/","summary":"Authors: Chao-Chun Hsu, Chenhao Tan","title":"Paper Review - Decision-Focused Summarization"},{"content":"Authors: Marina Danilevsky, Kun Qian, Ranit Aharonov, Yannis Katsis, Ban Kawas, Prithviraj Sen Paper reference: https://arxiv.org/pdf/2010.00711.pdf\nCategorization of Explanations Local vs Global  A local explanation provides justification for the model’s prediction on a specific input. A global explanation provides similar justification by revealing how the model’s predictive process works (understanding models' behavior), independently of any particular input.  Self-Explaining vs Post-Hoc  A self-explaining approach generates the explanation at the same time as the prediction, using information emitted by the model (such as attention score) as a result of the process of making that prediction. a post-hoc approach requires that an additional operation (such as gradient-based methods, input perturbations, train a surrogate model (LIME)) is performed after the predictions are made.  Explainability Techniques Feature importance (in frequent use) Derive explanation by investigating the importance scores of different features used to output the final prediction, such as attention, gradient-based methods.\nSurrogate model (in frequent use) Model predictions are explained by learning a second, usually more explainable model, as a proxy (such as LIME). The learned surrogate models and the original models may have completely different mechanisms to make predictions (concern about model faithfulness).\nExample-driven Explain the prediction of an input instance by identifying and presenting other instances, usually from available labeled data, that are semantically similar to the input instance (used in QA).\nProvenance-based The final prediction is the result of a series of reasoning steps.\nDeclarative induction Human-readable representations, such as rules and trees.\nA few visualization techniques:\n Saliency maps. Presents the learned declarative representations, logic rules and trees, Train a language model with human natural language explanations and coupling with a deep generative model (can be template based).  Evaluation  Informal examination of explanations. Discuss how examples of generated explanations align with human intuition. Comparison to ground truth. Such as comparing generated explanations to ground truth data; Employ automatic metrics including perplexity, BLEU; Having multiple annotators and reporting inter-annotator agreement or mean human performance to account for disagreements on the precise value of the ground truth. Human evaluation. Have multiple annotators, report inter-annotator agreement, and correctly deal with subjectivity and variance in the responses. Evaluate using counterfactuals. Fidelity (how much explanations reflect the actual workings of the underlying model), comprehensibility (how easy explanations are to understand by humans).  Future Directions  Define evaluation metrics for model explainability. Performance and explananility trade-off. Improve and evaluate faithfulness of models.  ","permalink":"https://tangliyan.com/blog/posts/a_survey_of/","summary":"Authors: Marina Danilevsky, Kun Qian, Ranit Aharonov, Yannis Katsis, Ban Kawas, Prithviraj Sen","title":"Paper Review - A Survey of the State of Explainable AI for Natural Language Processing"},{"content":"Authors: Yusen Zhang Ansong Ni Tao Yu Rui Zhang Chenguang Zhu Budhaditya Deb Asli Celikyilmaz Ahmed H. Awadallah Dragomir Radev Paper reference: https://arxiv.org/pdf/2109.04609.pdf\nContribution This paper is about an exploratory study on long dialogue summarization and provides several strategies for future works.\nDetails Challenge in Long Dialogue Summarization (1) Effectively use the current neural summarization models on dialogues that greatly exceed their length limits. (2) Dialogues are interactive, which makes it more context-dependent and the information in dialogues is more sparsely distributed over the text. (3) Language in dialogue is more informal, which leads to difficulties in modeling relevance and salience.\nCurrent Strategies for lengthy inputs  Retrieve-then-summarize Pipeline. Retrievers includes TF-IDF, BM25, Locator. etc. Suggestion: develop better utterance retrieval method. Experiments show that the two-step pipeline is better than end-to-end models. End-to-end Summarization Models. BART (truncate inputs), HMNet (a hierarchical network for dialogue summarization, with a token level and a turn level encoders), Longformer (16k maximum). Hierarchical network performs better for longer inputs. Pretraining on external dataset may help the performance, but it should be chosen carefully.  ","permalink":"https://tangliyan.com/blog/posts/an_exploratory_study/","summary":"Authors: Yusen Zhang Ansong Ni Tao Yu Rui Zhang Chenguang Zhu Budhaditya Deb Asli Celikyilmaz Ahmed H. Awadallah Dragomir Radev","title":"Paper Review - An Exploratory Study on Long Dialogue Summarization: What Works and What’s Next"},{"content":"Authors: Rex Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, Jure Leskovec Paper reference: https://arxiv.org/pdf/1903.03894.pdf\nContribution The paper proposes GNNEXPLAINER, a model-agnostic approach for providing interpretable explanations for predictions of any GNN-based model on any graph-based machine learning task.\nSpecifically, given an instance, GNNEXPLAINER identifies (1) a compact subgraph structure that maximizes the mutual information with GNN’s prediction and (2) a small subset of node features that have a crucial role in GNN’s prediction. Further, GNNEXPLAINER can generate consistent and concise explanations for an entire class of instances.\nDetails GNNEXPLAINER provides explanations for any GNN that can be formulated in terms of (1) message passing; (2) feature aggregation; (3) parameters update computations.\nSingle-instance explanations Given a node $v$, the goal is to identify a subgraph $G_{S} \\subseteq G_{c}$ and the associated features $X_{S} = \\{ x_{j} \\mid v_{j} \\in G_{S} \\}$ that are important for the GNN\u0026rsquo;s prediction $\\hat{y}$. The notion of importance is denoted by mutual information MI and formulate the GNNEXPLAINER as the following optimization framework:\n$$ \\max_{G_{S}} M I\\left(Y,\\left(G_{S}, X_{S}\\right)\\right)=H(Y)-H\\left(Y \\mid G=G_{S}, X=X_{S}\\right) $$\nWith a few approximation steps, it can be reformulated as\n$$ \\min_{\\mathcal{G}} H\\left(Y \\mid G=\\mathbb{E}_{\\mathcal{G}}\\left[G_{S}\\right], X=X_{S}\\right) $$\nNode feature selection The paper introduces a learnable feature mask F and reparametrize X as a function of F: $$X=Z+\\left(X_{S}-Z\\right) \\odot F$$ s.t. $\\sum_{j} F_{j} \\leq K_{F}$, where $Z$ is a $d$-dimensional random variable sampled from the empirical distribution and $K_{F}$ is a parameter representing the maximum number of features to be kept in the explanation.\nMulti-instance explanations To obtain a global explanation of class $c$, the rough idea is to covert muiti-instance explanations to single-instance explanation.\nExplaination Visualization GRAD: a gradient-based method, which computes gradient of the GNN’s loss function with respect to the adjacency matrix and the associated node features.\nATT: a graph attention GNN (GAT) that learns attention weights for edges in the computation graph.\n","permalink":"https://tangliyan.com/blog/posts/gnnexplainer/","summary":"Authors: Rex Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, Jure Leskovec","title":"Paper Review - GNNExplainer: Generating Explanations for Graph Neural Networks"},{"content":"Authors: Yuexiang Xie, Fei Sun, Yang Deng, Yaliang Li, Bolin Ding Paper reference: https://arxiv.org/pdf/2108.13134.pdf\nContribution The paper proposes a Counterfactual Consistency (CoCo) metric to evaluate the factual consistency in text summarization via counterfactual estimation by formulating the causal relationship among the source document, the generated summary, and the language prior. In particular, the goal is to remove the effect of language prior, which can cause hallucination, from the total causal effect on the generated summary. The proposed metric provides a simple yet effective way to evaluate consistency without relying on other auxiliary tasks such as textual entailment or QA.\nCoCo achieves a significant improvement against the existing automatic metrics for text summarization in terms of the correlation with human annotations.\nDetails The intuition is that when texts are generated more relying on the source document rather than the language prior, they should be more likely to be factually consistent w.r.t. the source documents.\nAlgorithm $$ \\mathrm{CoCo}=\\frac{1}{\\left|Y^{\\prime}\\right|} \\sum_{y_{t} \\in Y^{\\prime}} \\operatorname{Pr}\\left(y_{t} \\mid X, y_{\u0026lt;t}\\right)-\\operatorname{Pr}\\left(y_{t} \\mid X^{\\prime}, y_{\u0026lt;t}\\right) $$\n Scoring Model F. Adopt an independent summarization model BART as the scoring model in the instantiation of CoCo (since the factual consistency can be biased by the model that produced this evaluated summary). The decoding steps use teach forcing (apparently). Key tokens. Only count the probability of key tokens (denoted as Y') in the evaluated summary. The criteria of selecting key tokens can be task-oriented designed. Mask. Experiments show that span-level and sentence-level mask (containing key tokens) are more efficient; Empty input (pure LM) could make the summarization model fall into an ill-posed state.  Comments It\u0026rsquo;s capable at identifying extrinsic hallucinations by relying more on the source but may fail at intrinsic ones.\n","permalink":"https://tangliyan.com/blog/posts/factual_consistency_evaluation/","summary":"Authors: Yuexiang Xie, Fei Sun, Yang Deng, Yaliang Li, Bolin Ding","title":"Paper Review - Factual Consistency Evaluation for Text Summarization via Counterfactual Estimation"},{"content":"Authors: Soumya Sanyal, Xiang Ren Paper reference: https://arxiv.org/pdf/2108.13654.pdf\nContribution The paper proposes Discretized Integrated Gradients (DIG), which allows effective attribution along non-linear interpolation paths. Specifically, it monotonically interpolates between the input word embedding and baseline such that the intermediate points are close to real data samples. This would ensure that the interpolated points are more representative of the word embedding distribution, enabling more faithful model gradient computations.\n(The blue line, DIG, is a bit misleading\u0026hellip;)\nDetails Drawbacks of Integrated Gradient (IG) In IG, the interpolated points are not necessarily representative of the discrete word embedding distribution and can be very far-off from any original word in the embedding space. Using these out-of-distribution intermediate inputs to calculate gradients can lead to sub-optimal attributions.\nDiscretized Integrated Gradients (DIG) Procedure For a given input word embedding, (1) Find an anchor word. First search for an anchor word (AnchorSearch) from the vocabulary that can be considered as the next interpolation point. (2) Perturb the anchor word. Since the anchor point need not be monotonic w.r.t. the given input, the paper then optimally perturb the dimensions of the anchor word so that they satisfy the monotonicity constraints (Monotonize, for Riemann summation). (3) This perturbed point becomes our first interpolation.\nFor subsequent interpolation points, repeat the above steps using the previous anchor and perturbed points.\nComments (1) It seems like each time the moved distance between interpolation points $x_i$ and $x_{i+1}$ is varying (based on where the perturbation point is). Is it possible that the first $m-1$ interpolation points are clustered together and far way from the last interpolation point? (2) After perturbation of anchor words, the remaining embedding is still not \u0026ldquo;real\u0026rdquo;. From Table 4, it seems like anchor selection does not make the performance much better. (3) In Table 4, the DIG-MaxCount method does make the word approximation error low, but its performance is not as good as DIG-Greedy. How that explain the advantage of being close to real word embeddings?\n","permalink":"https://tangliyan.com/blog/posts/discretized_integrated_gradients/","summary":"Authors: Soumya Sanyal, Xiang Ren","title":"Paper Review - Discretized Integrated Gradients for Explaining Language Models"},{"content":"Authors: Griffin Adams, Emily Alsentzer, Mert Ketenci, Jason Zucker, Noémie Elhadad Paper reference: https://aclanthology.org/2021.naacl-main.382.pdf\nContribution This paper introduces a hospital-course summarization task in which the goal is to faithfully and concisely summarizing the EHR documentation for a patient’s specific inpatient visit, from admission to discharge (take the Brief Hospital Course, BHC, as a proxy reference).\nThen the paper provides a comprehensive analysis of the dataset and identify several implications for future hospital course summarization research.\nDetails Dataset The paper constructs a large-scale, multi-document summarization dataset CLINSUM (not public) covering a wide range of reasons for hospitalizations. It mainly relies on “Admission”, “Progress”, and “Consult” notes as source documents. It represents an incredibly challenging multi-document summarization task with diverse knowledge requirements.\nDataset Analysis and Implications Extractiveness v.s. abstractiveness CLINSUM appears very extractive according to widely used metrics. However, 64% of the extractive fragments are unigrams, and 25% are bigrams, which indicate a high level of re-writing.\nExtractive strategies find that, on average, one sentence accounts for roughly 50% of the overall ROUGE score. Afterwards, the marginal contribution of the next shrinks. In other words, the summary transitions from extractive to abstractive.\nThere is a great deal of redundancy in the source notes, but repetition is not indicative of salience in CLINSUM.\n Implications: (1) Require better understanding of the signal between lexical centrality and salience. (2) Require for dynamic hubris extraction-abstraction strategies.\n Comprehensiveness and conciseness BHC summaries are packed with medical entities, which are well-distributed across the source notes. This difficult task calls for a domain-specific approach to assessing faithfulness.\nSummaries are extremely dense with medical entities and it is necessary to read the entire set of notes to generate the summary despite diminishing marginal returns. Summaries also exhibits frequent, abrupt topic shifts, with few repeated entities.\n Implications: Entities are so densely packed in summaries makes models more susceptible to factual errors. Fact-based evaluation metrics which encode a deeper knowledge of clinical concepts and their complex semantic and temporal relation should be developed.\n Styles and content organization Clinical texts contain many obscure, abbreviations, misspellings, and sentence fragments. Summary sentences are actually longer on average than source sentences. Qualitative analysis confirms that most BHCs are organized around a patient’s disorders.\nRetrieval frameworks find that summaries adapt the style and problem-oriented structure of other summaries, but contain patient-specific information from the source notes.\nBHC summaries are silver standard. Discharge summaries and their associated BHC sections are frequently missing critical information or contain excessive or erroneous content. These quality issues occur for a number of reasons.\n Implications: (1) One approach can be using use the retrieve-rerank-rewrite framework to generate problem-oriented BHC summarization. (2) Develop heuristics to assess reference quality or scalable reference-free evaluations.\n ","permalink":"https://tangliyan.com/blog/posts/what_is_in/","summary":"Authors: Griffin Adams, Emily Alsentzer, Mert Ketenci, Jason Zucker, Noémie Elhadad","title":"Paper Review - What’s in a Summary? Laying the Groundwork for Advances in Hospital-Course Summarization"},{"content":"Authors: Amir Feder, Katherine A. Keith, Emaad Manzoor, Reid Pryzant, Dhanya Sridhar, Zach Wood-Doughty, Jacob Eisenstein, Justin Grimmer, Roi Reichart, Margaret E. Roberts, Brandon M. Stewart, Victor Veitch, Diyi Yang\nPaper reference: https://arxiv.org/pdf/2109.00725v1.pdf\nContext As NLP systems are increasingly deployed in challenging and high-stakes scenarios, we cannot assume that training and test data are identically distributed, and we may not be satisfied with uninterpretable black-box predictors. Moreover, the increasingly high-capacity neural architectures make no distinction between causes, effects, and confounders, and they make no attempt to identify causal relationships. Causality offers a promising path forward for these problems. There are potential uses of causal inference to improve the performance, robustness, fairness, and interpretability of NLP models.\nLearning Robust Predictors The NLP field has grown increasingly concerned with spurious correlations. Such observations have led to several proposals for novel evaluation methodologies to ensure that predictors are not “right for the wrong reasons”.\nThese evaluations generally take two forms: (1) invariance tests, which assess whether predictions are affected by perturbations that are causally unrelated to the label; (2) sensitivity tests, which apply perturbations that should in some sense be the minimal change necessary to flip the true label.\nA number of approaches have been proposed for learning predictors that pass tests of sensitivity and invariance. These approache fall into two main groups: counterfactual data augmentation and causally-motivated distributional criteria.\nData augmentation Idea: Elicit or construct counterfactual instances, and incorporate them into the training data.\nIn the case of invariance tests, additional focus can be provided by adding a term to the learning objective to explicitly penalize disagreements in the predictions for counterfactual pairs. In the case of interventions on the label Y, training on label counterfactuals can improve out-of-domain generalization and reduce sensitivity to noise.\nCounterfactual examples Counterfactual examples can be generated in several ways: (1) Manual post-editing. Manual editing is typically fluent and accurate but relatively expensive. (2) Heuristic replacement of keywords. It cannot guarantee fluency or coverage of all labels and covariates of interest. (3) Automated text rewriting. Fully generative approaches could potentially combine the fluency and coverage of manual editing, but these methods are still relatively immature.\nCounterfactual examples are a powerful resource because they directly address the missing data issues that are inherent to causal inference. However, in many cases it is difficult for even a fluent human to produce meaningful counterfactuals.\nDistributional Criteria Way1 Derive distributional properties of invariant predictors, and then ensure that these properties are satisfied by the trained model.\nIt can be shown that any counterfactually invariant predictor will satisfy $f(X) \\perp Z \\mid Y$ (the prediction f(X) is independent of the covariate Z conditioned on the true label Y). In this fashion, knowledge of the true causal structure of the problem can be used to derive observed-data signatures of the counterfactual invariance. Such signatures can be incorporated as regularization terms in the training objective.\nThis does not guarantee counterfactual invariance, but in practice it increases counterfactual invariance and improve performance in out-of-distribution settings without requiring counterfactual examples.\nWay2 Viewing the training data as arising from a finite set of environments, in which each environment is endowed a unique distribution over causes, but the causal relationship between X and Y is invariant across environments (environmental invariance). The goal is to learn a predictor that works well across a set of causally-compatible domain (domain generalization).\nWay3 Control for confounding such that $$ \\tilde{P}(Y \\mid X)=\\sum_{z} P(Y \\mid X, Z=z) \\operatorname{Pr}(Z=z). $$\nThese distributional approaches require richer training data than in the typical supervised learning setup: either explicit labels Z for the causes of the text that should not influence the prediction, or access to data gathered from multiple labeled environments. Whether obtaining such data is easier than creating counterfactual instances depends on the situation.\nFairness and bias NLP systems inherit and sometimes amplify undesirable biases that are encoded in text training data. A causal analysis is required to determine whether an observed distribution of data and predictions raises fairness concerns. Counterfactual data augmentation has been applied to reduce bias in text classification and in pre-trained contextualized word embedding models.\nCausal Model Interpretations Both attention and perturbation-based methods have important limitations. Attention-based explanations can be misleading and are generally possible only for individual tokens. Existing perturbation based methods often generate implausible counterfactuals. These methods do not allow for explaining predictions in terms of more abstract linguistic concepts or sentence-level concepts.\nFrom causal inference persepective, a natural approach to explanation is to generate counterfactual examples and then compare the prediction for each example and its counterfactual. A complementary approach is to generate counterfactuals with minimal changes that obtain a different model prediction. Such examples serve as explanations as they allow us to observe the changes required to change a model’s prediction.\nAnother solution is to identify invariances in a given trained model, and not with enforcing them during training.\n","permalink":"https://tangliyan.com/blog/posts/causal_inference_for/","summary":"Authors: Amir Feder, Katherine A. Keith, Emaad Manzoor, Reid Pryzant, Dhanya Sridhar, Zach Wood-Doughty, Jacob Eisenstein, Justin Grimmer, Roi Reichart, Margaret E. Roberts, Brandon M. Stewart, Victor Veitch, Diyi Yang","title":"Paper Review - Causal Inference in Natural Language Processing: Estimation, Prediction, Interpretation and Beyond"},{"content":"Authors: Kaj Bostrom, Xinyu Zhao, Swarat Chaudhuri, Greg Durrett Paper reference: https://arxiv.org/abs/2104.08825\nContribution This paper proposes an automated data generation method, ParaPattern, for building models capable of deductive reasoning and generating logical transformations (in particular substitution and contraposition) from diverse natural language inputs (premise statements) in an unsupervised way. Experiments show that the proposed data generation method leads to robust operation models capable of generating consistent logical transformations over a diverse range of natural language inputs.\nDetails Data Collection The proposed method ParaPattern crafts data with lexical variation and diversity based on (1) syntactic retrieval from Wikipedia; (2) templated-based example construction; (3) and a pretrained paraphrasing model.\nOnce data for an operation has been generated, the paper uses it to fine-tune a BART-Large model.\nSource Scraping Retrieve source sentences suitable for template expansion from Wikipedia article text. The paper uses six pattern variations to gather source sentences for the substitution template and two patterns for the contraposition template.\nTemplate expansion The template expansion algorithm expands source sentences into generated examples by breaking out dependency subtrees rooted at each match variable and rearranging them according to the template structure.\nParaphrase Augmentation Paraphrase template expanded input sentences with a PEGASUS model fine-tuned for paraphrasing.\nParaphrasing input sentences automatically adds a denoising component to the fine-tuning objective, resulting in more robust models and introduces lexical variation in the inputs which may not be captured by the template matches.\n","permalink":"https://tangliyan.com/blog/posts/flexible_operations_for/","summary":"Authors: Kaj Bostrom, Xinyu Zhao, Swarat Chaudhuri, Greg Durrett","title":"Paper Review - Flexible Operations for Natural Language Deduction"},{"content":"Authors: Jiacheng Xu, Zhe Gan, Yu Cheng, Jingjing Liu Paper reference: https://aclanthology.org/2020.acl-main.451.pdf\nContribution This paper proposes a discourse-aware extractive summarization model, DiscoBERT, which operates on a sub-sentential discourse unit level to perform compression with extraction simultaneously and reduce redundancy across sentences. It proposes two discourse-oriented graphs (RST Graph and Coreference Graph) as inductive bias to capture long-range dependencies among discourse units.\nDetails Discourse Analysis In the Rhetorical Structure Theory (RST) framework, the discourse structure of text can be represented in a tree format. The whole document can be segmented into contiguous, adjacent and non-overlapping text spans called Elementary Discourse Units (EDUs). Each EDU is tagged as either Nucleus or Satellite, which characterizes its nuclearity or saliency. Nucleus nodes are generally more central, and Satellite nodes are more peripheral and less important in terms of content and grammatical reliance.\nRST Graph RST Graph aims to provide both local paragraph-level and long-range document-level connections among EDUs\nCoreference Graph For each coreference cluster, all the discourse units containing the mention of the same cluster will be connected. This process is iterated over all the coreference mention clusters to create the final Coreference Graph.\nModel The paper formulates the extractive summarization as a sequential labeling task, where each EDU is scored by neural networks, and decisions are made based on the scores of all EDUs. The creation of oracle labels is operated on EDU level. The paper greedily picks up EDUs with their necessary dependencies until R-1 F1 drops.\nSteps: (1) A pretrained BERT model is first used to encode the whole document on the token level; (2) A self-attentive span extractor is used to obtain the EDU representations from the corresponding text spans (EDU); (3) The Graph Encoder takes the output of the Document Encoder as input and updates the EDU representations with Graph Convolutional Network based on the constructed discourse graphs; (4) Predict the oracle labels based on updated EDU representations.\n","permalink":"https://tangliyan.com/blog/posts/discourse_aware_neural/","summary":"Authors: Jiacheng Xu, Zhe Gan, Yu Cheng, Jingjing Liu","title":"Paper Review - Discourse-Aware Neural Extractive Text Summarization"},{"content":"Authors: Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, Arnold Overwijk Paper reference: https://openreview.net/pdf?id=zeFrfgyZln\nContribution This paper explores how to effectively select hard training negatives for dense text retrieval, which represents texts as dense vectors for approximate nearest neighbors (ANN) search. It theoretically shows the needs of harder negatives and the limitation of the widely used local (in-batch) negatives in contrastive learning. It therefore proposes a new mechanism called Approximate nearest neighbor Negative Contrastive Learning (ANCE) to select hard training negatives globally from the entire corpus, using an asynchronously updated ANN index.\nThe proposed ANCE achieves faster converge of model training and equally accurate text retrieval when compared with a number of baselines.\nDetails Dense Retrievals  Dense Retrieval (DR) aims to overcome the sparse retrieval bottleneck by matching in a continuous representation space learned via neural networks. One challenge in dense retrieval is to construct proper negative instances when learning the representation space.\n ANCE focuses on representation learning for dense retrieval and uses the ANN index to construct global hard negatives for contrastive learning. Specifically, it samples negatives from the top retrieved documents via the DR model from the ANN index. This is different from REALM, which focuses on grounded language modeling and uses the ANN index to find grounding documents.\nDense text retrieval has two phases:\n Phase I: Learn a representation model to project semantically similar texts to vectors of large similarity scores via similarity functions such as inner products or cosine similarity scores. Phase II: Adopt an approximate nearest neighbors (ANN) search algorithm to index these vectors and process queries.  This paper focuses on Phase I where it introduces a better negative sampling method to sample good dissimilar text pairs for training.\n","permalink":"https://tangliyan.com/blog/posts/approximate_nearst_neighbor/","summary":"Authors: Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, Arnold Overwijk","title":"Paper Review - Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval"},{"content":"Authors: Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang Paper reference: https://arxiv.org/pdf/2002.08909.pdf\nContribution This paper proposes a Retrieval-Augmented Language Model (REALM) framework, which augments language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference.\nExperiments show the effectiveness of REALM by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA).\nDetails For both pre-training and fine-tuning, REALM takes some input $x$ and learns a distribution $p(y | x)$ over possible outputs $y$. (1) For pre-training, the task is masked language modeling; (2) For fine-tuning, the task is Open-QA: $x$ is a question, and $y$ is the answer.\nSince the model structures in knowledge retriever and text generation are similar to previous post (Joint Retrieval and Generation Training for Grounded Text Generation), I only focus on the model pre-training part and how encodings for documents are updated asynchronously (for both works).\nInjecting Bias into Pre-Training The paper develops strategies in pre-training to guide model towards useful retrievals.\n(1) Salient span masking. Instead of uniform making, spans (e.g. named entities, dates) that requires world knowledge to predict are masked by leveraging a Bert-based tagger. (2) Null document. Add an empty document as one of selected $k$ documents since some salient span still do not require referring to any retrieved knowledge. (3) Prohibiting trivial retrievals. If the masked sentence $x$ comes from document $z$, the knowledge augmented encoder can trivially predict $y$ by looking at the unmasked version of $x$ in $z$, which encourages model to learn to look for exact string matches between $x$ and $z$. (4) Initialization (avoid cold-start problem). The paper uses a Inverse Cloze Task (ICT) to warm-start embeddings.\n Cold-Start Problem: At the beginning of training, if the retriever does not have good embeddings for $x$ and $z$, the retrieved documents $z$ will likely be unrelated to $x$. This causes the knowledge augmented encoder to learn to ignore the retrieved documents. Once this occurs, the knowledge retriever does not receive a meaningful gradient and cannot improve, creating a vicious cycle.\n Asynchronous Update  Remember that the previous post and this paper both use Maximum Inner Product Search (MIPS) algorithms to find the approximate top $k$ documents and each document is assigned with a MIPS index. They share similar asynchronous update method as the following.\n Updating the encodings for each document after every time step update of retriever is not realistic. The solution is to “refresh” the index by asynchronously re-embedding and re-indexing all documents every several hundred training steps.\nThese papers asynchronously refresh the MIPS index by running two jobs in parallel: a primary trainer job, which performs gradient updates on the parameters, and a secondary index builder job, which embeds and indexes the documents. (1) the trainer sends the index builder a snapshot of its parameters, $\\theta^{\\prime}$; (2) The trainer then continues to train while the index builder uses $\\theta^{\\prime}$ to construct a new index in the background.\nAs soon as the index builder is done, it sends the new index back to the trainer, and the process repeats. The MIPS index is slightly stale between refreshes, but it is only used to select the top $k$ documents. After documents are selected, all computations use the updated $\\theta$.\nAt inference time, the model uses fixed MIPS index after pre-training.\n","permalink":"https://tangliyan.com/blog/posts/realm/","summary":"Authors: Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang","title":"Paper Review - REALM: Retrieval-Augmented Language Model Pre-Training"},{"content":"Authors: Yizhe Zhang, Siqi Sun, Xiang Gao, Yuwei Fang, Chris Brockett, Michel Galley, Jianfeng Gao, Bill Dolan Paper reference: https://arxiv.org/pdf/2105.06597.pdf\nContribution This paper presents a joint training framework RetGen that trains a document retriever and a multi-document grounded generator in an end-to-end fashion with a language model signal and allows these modules to synergistically cooperate to optimize grounded text generation. Specifically, the document retriever efficiently selects relevant documents and the generator generates text by combining information from retrieved relevant documents and generate a single prediction. This framework alleviates the need for oracle parallel data (prose-document pairs) with which to train a grounded model.\nDetails Retrieval Grounded Text Generation Retrieval grounded text generation aims to predict the upcoming text $y$ that directly follows the existing source prompt (context queries) $x$ ($x, y$ are from a corpus $D$), with access to a document reference set $Z$ (not parallel with $D$).\nRetGen This paper proposes a framework RetGen to solve the Retrieval grounded text generation task. It consists of a dense document retriever and a knowledge-grounded text generator.\nDocument Retriever A document retriever (ANCE) narrows down the search for relevant document and accelerate join training with the text generator. Specifically, all documents and context queries are mapped to the same embedding space with separate learnable encoders. A score for each pair is computed by an inner-product, and top $K$ are selected as relevant documents (weight $p(z^{(k)}|x)$ is determined by softmax on scores). Maximum inner product search (MIPS) is employed to reduce searching time.\nRefresh Document embeddings Instead of encoding and refreshing embeddings for all documents after the retriever is updated each time, embeddings for retrieved documents are updated asynchronously after every few hundred steps. Refer to my next post (REALM: Retrieval-Augmented Language Model Pre-Training) for more details.\nRetriever Correction Scores for $(z, x)$ should be updated along generation and consider generated text so far. A correction factor term is derived and can be computed with negligible cost.\nKnowledge-Grounded Text Generator Knowledge grounded text generator (GPT-2/ DialoGPT) takes one document $z$, one context query $x$, and generated text so far as input and the goal is to generate text $y$ following $x$. Maximum Mutual Information (MMI) is employed to encourage the generation to tie better to $z$ and $x$.\nMulti-document Decoding The paper takes Mixture-of-Expert (MoE) approach from a previous work to combines information from retrieved relevant documents and generate a single prediction. Specifically, at each time step, the generator separately generates $K$ output distributions for the next token with same $x$ and generated text so-far but different document $z$. The assemble output is from the weighted distribution based on $p(z^{(k)}|x)$.\n","permalink":"https://tangliyan.com/blog/posts/joint_retrieval_and/","summary":"Authors: Yizhe Zhang, Siqi Sun, Xiang Gao, Yuwei Fang, Chris Brockett, Michel Galley, Jianfeng Gao, Bill Dolan","title":"Paper Review - Joint Retrieval and Generation Training for Grounded Text Generation"},{"content":"Authors: Trapit Bansal, Rishikesh Jha, Tsendsuren Munkhdalai, Andrew McCallum Paper reference: https://aclanthology.org/2020.emnlp-main.38.pdf\nContribution This paper proposes a self-supervised approach SMLMT (Subset Masked Language Modeling Tasks) which generates a large and diverse meta-learning task distribution from unlabeled sentences. It enables training of meta-learning methods for NLP at a large scale while also ameliorating the risk of over-fitting to the training task distribution. This meta-training leads to better few-shot generalization to novel/unseen tasks than pre-trained models (such as BERT) followed by fine-tuning.\nDetails Pre-training v.s. Meta-learning Large scale pre-training suffers from a train-test mismatch as the model is not optimized to learn an initial point that yields good performance when fine-tuned with few examples. Fine-tuning of a pre-trained model typically introduces new random parameters, which are hard to estimate robustly from the few examples.\nSelf-supervised Tasks for Meta-learning (SMLMT) Limitations of meta-learning in NLP:\n Large classification datasets with large label spaces are not readily available for all NLP tasks. Meta-learning approaches in NLP trains to generalize to new labels of a specific task, but doesn\u0026rsquo;t generalize to novel tasks.*  SMLMT Each SMLMT task is defined from a subset of vocabulary words. It consists of following steps to create one $N$-way classification task: (1) Randomly select $N$ unique vocabulary words; (2) Consider all sentences containing these $N$ words, and for each word randomly sample $k$ and $q$ sentences for the support set and query set. (3) Mask the corresponding chosen word from the sentences with the mask token $[m]$ in each of these $N$ sets; (4) Assign labels ${1, \u0026hellip;, N}$ to these classes and ignore original masked words.\nThe task is to predict a label for each sentence (unlike MLM which is a word-level classification task). The way of creating tasks like this enables large-scale meta-learning from unsupervised data.\nHybrid SMLMT combined with supervised tasks to encourage better feature learning and increase diversity in tasks for meta-learning. In each episode select an SMLMT task with probability $\\lambda$ or a supervised task with probability $(1 − \\lambda)$. The use of SMLMT jointly with supervised tasks ameliorates meta-overfitting.\nModel The paper uses the MAML algorithm to meta-learn an initial point that can generalize to novel NLP tasks, with a bit modification on the model so that it can handle diverse number of classes. Text encoder is BERT.\nTakeaways Few-shot generalization to unseen tasks The paper evaluates performance on novel tasks not seen during training. Without using supervised data, meta-trained SMLMT performs better than fine-tuning BERT, specially for small number of examples (\u0026lt;16). With supervised data, Hybrid-SMLMT outperform all baselines by a large margin.\nAmeliorate meta-overfitting Learned learning rates converge towards large non-zero values for most layers under Hybrid-SMLMT setup, which indicates that SMLMT helps in ameliorating meta-overfitting.\nLearned Representation Hybrid-SMLMT model is closest to the initial point after task-specific fine-tuning, indicating a better initialization point. Representations in lower layers are more similar before and after fine-tuning, and less in the top few layers across models.\n","permalink":"https://tangliyan.com/blog/posts/self_supervised_meta/","summary":"Authors: Trapit Bansal, Rishikesh Jha, Tsendsuren Munkhdalai, Andrew McCallum","title":"Paper Review - Self-Supervised Meta-Learning for Few-Shot Natural Language Classification Tasks"},{"content":"Authors: Chelsea Finn, Pieter Abbeel, Sergey Levine Paper reference: https://arxiv.org/pdf/1703.03400.pdf\nContribution The goal of meta-learning is to train a model on a variety of learning tasks, such that it can quickly solve new learning tasks using only a small number of training samples.\nThis paper proposes a Model-Agnostic Meta-Learning algorithm (MAML). The key idea underlying the proposed method is to train the model’s initial parameters such that the model has optimal performance on a new task after the parameters have been updated through one or more gradient steps computed with a small amount of data from that new task. There is no assumptions on the form of the model, but it requires that model parameters are smooth enough and can be updated through gradient descent.\nDetails Meta-Learning Meta-learning is also known as \u0026ldquo;learning to learn\u0026rdquo; and widely considered in few-shot learning. Meta-learning treats tasks as training examples and aasumes that the training tasks and the new task to be solved are from the same distribution. To solve a new task, we first collect lots of tasks, treating each as a training example and train a model to adapt to all those training tasks, finally this model is expected to work well for the new task.\nSince the new task only has $k$ labeled examples (few shot), each training task also keeps only $k$ labeled examples during the training. This is to make sure that the training examples (means those training tasks here) have the same distribution as the test example (means the new task here).\nModel-Agnostic Meta-Learning algorithm (MAML) Suppose we have the following: (1) A learning algorithm $f_{\\theta}$ with randomly initialized parameters $\\theta$. (2) A set of training tasks $\\mathcal{T}$. Each $\\mathcal{T}_i$ is a $N$-way $k$-shot task, where the total number of classes are greater than $N$. Each task $\\mathcal{T}i$ contains a support set and a query set (train set and test set) with $k$ and $q$ randomly sampled examples, respectively. (3) A testing/new task $\\mathcal{T}{\\text{test}}$ having a support set with $k$ examples and a query set.\nThe goal is to find parameters $\\theta_{\\text{final}}$ such that the learning algorithm has optimal performance on a new task after the parameters $\\theta$ have been updated ($\\theta \\to \\theta_{\\text{final}}$).\nHere are the steps for updating parameter $\\theta$ for the learning algorithm $f_{\\theta}$, and we can iterate these steps many times. The idea is to let the model learn how to do these tasks and then it learns how to generalize to an unseen task. (1) We first sample a batch of tasks $\\mathcal{T}_{i} \\sim p(\\mathcal{T})$. (2) For each task $\\mathcal{T}_{i}$, we train normally on the support set using a copied $f_\\theta$ and update perameters using gradient descent on the training loss $\\mathcal{L}_{\\mathcal{T}_i}(f_\\theta)$. We call the updated parameters $\\theta_i^\\prime$. (3) We calculate the loss on the query set of each task $\\mathcal{T}_{i}$ using $f_{\\theta_i^\\prime}$ and sum them up as the total loss $\\mathcal{L} = \\sum \\mathcal{L}_{\\mathcal{T}_i}(f_\\theta^\\prime)$ for learning algorithm $f_{\\theta}$. (4) Update model $f_{\\theta}$ by gradient descent on $\\mathcal{L}$ and obtain new $\\theta$.\nAfter the training is done, we obtain model $F_{\\text{final}}$. Then we train this model on the support set of the testing/new task (this is few-shot learning), and check it\u0026rsquo;s performance on the query set. $F_{\\text{final}}$ should quickly adapted to the new task after few training examples.\n Figure reference: https://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2019/Lecture/Meta1%20(v6).pdf\n","permalink":"https://tangliyan.com/blog/posts/model_agnostic_meta/","summary":"Authors: Chelsea Finn, Pieter Abbeel, Sergey Levine","title":"Paper Review - Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"},{"content":"Authors: Sewon Min, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer Paper reference: https://arxiv.org/pdf/2108.04106.pdf\nContribution This paper introduces a noisy channel approach by using large language models for few-shot text classification, via either in-context demonstration or prompt tuning, with no or very limited updates to the model parameters. Experiments show that channel models have superior performance compare to their direct counterparts since they have lower variance and significantly higher worst-case accuracy.\nThe paper provides recommendations for using channel prompt tuning when: (1) Training size is small; (2) Data is imbalanced or there are large number of classes; (3) required to generalize to unseen labels; (4) Task is close to language modeling.\nDetails Formulation and Definition This paper focuses on eleven text classification tasks. The goal is to learn a function $f: \\mathcal{X} \\rightarrow \\mathcal{C}$, where $\\mathcal{X}$ is a set of texts and $\\mathcal{C}=\\{c_{1} \\ldots c_{m}\\}$ is a set of labels. Meanwhile, assume a pre-defined verbalizer $v: \\mathcal{C} \\rightarrow \\mathcal{X}$ which maps each label into text. The paper considers the following three formulations with a causal language model (GPT-2 in the paper):\n Direct model $P(c_i|x)$. Direct++ model $\\frac{P\\left(c_{i} \\mid x\\right)}{P\\left(c_{i} \\mid \\text{NULL}\\right)}$ is a calibrated verision of Direct model and performs better. Channel model $\\propto P(x|c_i)$. Intuitively, channel models are required to explain every word in the input, potentially amplifying training signals in the low data regime.  In the few-shot setup, a model takes $K$ training examples $\\mathcal{D}=\\left\\{\\left(x^{1}, c^{1}\\right), \\cdots,\\left(x^{K}, c^{K}\\right)\\right\\}$. Examples are sampled uniformly (do not consider label imbalance).\nDemonstration Methods In the following three demonstration methods, there are no trainable parameters.\n Zero-shot. Concat-based demonstrations. Prepend a concatenation of $K$ training examples ${{x^j, v(c^j)}}$ to the input so that a language model can learn the task setup from the input. Ensemble-based demonstrations. Instead of concatenation, multiply probabilities from an LM $K$ times conditioned on one training example at a time. It eliminates the dependency on the ordering of training examples, which has been shown to significantly impact the model performance.  Each model formulation (from previous section) can pair with one of these demonstration methods, resulting in 9 combinations in paper\u0026rsquo;s experiments.\nExperiments show that: (1) in direct models, ensemble based method is better. (2) in the few-shot setting, channel models always outperform direct models. (3) channel models always outperform significantly under few-shot setting comparing to zero-shot setting.\nTuning Methods The paper explores three tuning methods which update limited number of model parameters. Two of them worths attention.\n Head Tuning. Head tuning finetunes the head - the fully connected layer that transfers the dimensions from hidden representation size $h$ to vocal size $\\mathcal{V}$. Other params of LM are fixed. Prompt Tuning. Prepend $n$ prompt tokens $u_1 \u0026hellip; u_n$ to the input. The parameters in the LM are frozen except the embeddings of $u_{1} \\ldots u_{n}$.  direct models compute $P\\left(c_{i} | x\\right)=$ $P_{\\mathrm{LM}}\\left(v\\left(c_{i}\\right) | u_{1} \\ldots u_{n}, x\\right)$; channel models compute $P\\left(x | c_{i}\\right)=P_{\\mathrm{LM}}\\left(x | u_{1} \\ldots u_{n}, v\\left(c_{i}\\right)\\right) .$    Experiments show that: (1) channel models consistently outperform direct models under prompt tuning. (2) head tuning is a very strong method, although ignored in previous works. (3) prompt tuning outperforms the demonstration method by using channel models. (Demonstration methods v.s. Tuning methods)\nVerbalizers ${v(c_i)}$ ","permalink":"https://tangliyan.com/blog/posts/noisy_channel_language/","summary":"Authors: Sewon Min, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer","title":"Paper Review - Noisy Channel Language Model Prompting for Few-Shot Text Classification"},{"content":"Authors: Fangwei Zhu, Shangqing Tu, Jiaxin Shi, Juanzi Li, Lei Hou, Tong Cui Paper reference: https://aclanthology.org/2021.acl-long.356.pdf\nContribution This paper proposes a two-stage Topic-guided Wikipedia Abstract Generation model (TWAG) that guides topic-aware abstract summarization. Specifically, TWAG first divides documents into paragraphs and assigns a topic for each paragraph. Then, it generates the abstract in a sentence-wise manner based on predicted time-wise topic-aware representation for a sentence.\nExperiments show that TWAG can generate comprehensive abstracts and outperform state-of-the-art models which view Wikipedia abstracts as plain text.\nDetails Task Formulation The task is to automatically generate Wikipedia abstracts based on the related documents collected from referred websites or search engines.\nBased on the observation that a Wikipedia article normally describes an object from different aspects/topics and that the abstract semantically corresponds to these topics, the paper proposes to utilize the topic information to guide sumarization.\nMethod Overview The paper proposes a two-stage model TWAG (Topic-guided Wikipedia Abstract Generation model) to guide abstractive summarization using topic information.\n Stage 1: Train a topic detector to predict a topic for each paragraph (normally a paragraph expresses relatively complete and compact semantics). Stage 2: Encode each topic separately and generate the abstract in a sentence-wise manner.  Stage 1 - Topic Detection The paper render topic detection as a classification problem. Given a paragraph $d_i$, the classification task is to predict the topic for $d_i$. The paper uses ALBert as the classification model.\nStage 2 - Topic-aware Abstract Generation Topic-aware abstract generator consist of three modules.\nTopic Encoder Topic detection assigns each paragraph a topic, and all paragraphs sharing the same topic are grouped together to form a topic-specific text group $G_j$.\nThis paper uses BiGRU to encode each topic-specific text group $G_j$ as a hidden state $\\mathbf{g_j}$.\nTopic Predictor TWAG generates the summary in a sentence-by-sentence manner. At each time step $t$, the topic predictor further fuses information from all hidden representations of text groups ${\\mathbf{g_j}}$ to a topic-aware hidden representation $\\mathbf{r_t}$.\nNote: Given that Wikipedia abstract sentences normally contain mixed topics, the paper decides to fuse information from all text groups (weighted differently each time apparently) at each time step.\nSentence Decoder The paper leverages a Pointer-Generator to generate a summary sentence $s_t$ with topic-aware representation $\\mathbf{r_t}$ obtained from topic predictor as the inital decoder hidden state.\nFinally, sentences having an overlap of over 50% with other sentences are removed to reduce redundancy.\n","permalink":"https://tangliyan.com/blog/posts/twag/","summary":"Authors: Fangwei Zhu, Shangqing Tu, Jiaxin Shi, Juanzi Li, Lei Hou, Tong Cui","title":"Paper Review - TWAG: A Topic-guided Wikipedia Abstract Generator"},{"content":"Authors: Feng Nan, Cicero Nogueira dos Santos, Henghui Zhu, Patrick Ng, Kathleen McKeown, Ramesh Nallapati, Dejiao Zhang, Zhiguo Wang, Andrew O. Arnold, Bing Xiang Paper reference: https://aclanthology.org/2021.acl-long.536.pdf\nContribution This paper proposes an efficient automatic evaluation metric QUALS to measure factual consistency of a generated abstractive summary and designs a contrastive learning algorithm to directly optimize the proposed metric while training without using Reinforce algorithm.\nBoth automatic metrics and human evaluations show that the summaries generated using designed algorithm have better quality and are more factually consistent compared to the ones generated by current SOTA summarization model (BART-large).\nDetails Background Automatic Evaluation Protocol QAGS Previous work proposed QAGS (Question Answering and Generation for Summarization), an interpretable evaluation protocol designed to identify factual inconsistencies in a generated summary given the source document and the summary.\nIt consists of four steps: (1) extracts named entities and noun phrases in the summary as candidate answers using an answer extraction model; (2) a question generation model takes in the summary, concatenating with each candidate answer to generate a corresponding question; (3) a QA model is used to answer each generated question in the context of the summary and the input document, separately; (4) the answers from the QA model based on the summary and the input document are compared using word-level overlap.\nThis paper discusses some drawbacks of this metric, mostly about computation inefficiency since it involves three separate models.\nQAGen QAGen is an end-to-end model capable of generating both questions and answers from a summary.\nAutomatic Evaluation Protocol QUALS This paper proposes a metric QUALS (QUestion Answering with Language model score for Summarization) to measure factual consistency.\n(1) It first employs a QAGen model (using BART) to generate diverse question-answer pairs from a summary and picks high-quality ones. (2) Then the paper evaluates the likelihood that the QAGen model generates the same question-answer pairs given the source document.\nRoughly speaking, QUALS score is defined as the degree that the same question-answer pairs can be generated by taking either the source document or the summary as input.\nExperiments show that QUALS correlates very well with QAGS.\nCONtrastive SEQ2seq learning (CONSEQ) The paper further designs a contrastive seq-to-seq learning algorithm (CONSEQ), which can maximize the proposed factual consistency metic during training phase without using Reinforce algorithm.\nCONSEQ After normal fine-tuning on training data, the model (using BART) goes through a round of training using newly constructed contrast set.\nThe new dataset is constructed by selecting high quality factual consistent/inconsistent summaries based on their QUALS scores from the training data. The model is trained to maxmize/minimize the likelihood of generating these factual consistent/inconsistent summaries.\n","permalink":"https://tangliyan.com/blog/posts/improving_factual_consistency/","summary":"Authors: Feng Nan, Cicero Nogueira dos Santos, Henghui Zhu, Patrick Ng, Kathleen McKeown, Ramesh Nallapati, Dejiao Zhang, Zhiguo Wang, Andrew O. Arnold, Bing Xiang","title":"Paper Review - Improving Factual Consistency of Abstractive Summarization via Question Answering"},{"content":"Authors: Jiacheng Xu, Greg Durrett Paper reference: https://aclanthology.org/2021.acl-long.539.pdf\nContribution This paper proposes a two-stage framework for interpreting the stepwise decisions of a summarization model and attributing generation decisions to the input document. In particular, it conducts an ablation study to determine the generation mode at each decoding step; and experiments with attribution methods to see if highlighted attributions are truly important for the generation of the next token.\nThe paper further presents a Sentence Fusion case study to demonstrate the potential use of the proposed method for fine-grained factuality evaluation.\nDetails Stage I - Ablation: Mapping Model Behavior The ablation study compares the predictions of different model and input configurations. The goal is to coarsely determine the mode of generation (discuss below).\nAblation Models This paper probes the model by predicting next words with various model ablations: (1) LM$\\mathbf{_\\emptyset}$. A basic BART model with no input; (2) S$\\mathbf{_\\emptyset}$. A BART summarization model with no input; (3) S$\\mathbf{_{part}}$. A BART summarization model with part of the document as input; (4) S$\\mathbf{_{full}}$. A BART summarization model with the full document as input.\nThese ablations tell when the stepwise prediction is context-independent or context-dependent.\nDistance Metric The paper uses the L1 norm to measure the distance of two distributions over tokens for each timestep.\nGeneration Mode and Existing Memorization/Bias LM (lower left): decisions that can be easily made by using only decoder information, even without training or knowledge of the input document.\nCTX (upper right): cases where the input is needed to make the prediction.\nFT (lower right): cases where the finetuned decoder-only model is a close match but the pre-trained model is not. This reflects memorization of training summaries/ bias from fine-tuning training summaries.\nPT (upper left): cases where LM$\\mathbf{_\\emptyset}$ agrees with S$\\mathbf{_{full}}$ but S$\\mathbf{_\\emptyset}$ does not; that is, finetuning a decoder-only model causes it to work less well. This reflects memorization of data (bias) from the pre-training corpus (prior knowledge).\nPOS Tags Distribution over regions Here is the top three POS tags of generated words for each region in XSum and the percentage of examples falling into each region. More than two thirds of generation steps actually do rely heavily on the context.\nStage II - Attribution The paper explores interpreting stepwise decisions that rely on the context using several different attribution methods. The goal is to see whether highlighted attributions are truly important for the generation of the next token.\nExperiements show that Integrated Gradient and Input Gradient are the best techniques to select content and reconstruct the model’s predicted token from perturbations of the input.\n","permalink":"https://tangliyan.com/blog/posts/dissecting_generation_modes/","summary":"Authors: Jiacheng Xu, Greg Durrett","title":"Paper Review - Dissecting Generation Modes for Abstractive Summarization Models via Ablation and Attribution"},{"content":"Authors: Yumo Xu, Mirella Lapata Paper reference: https://aclanthology.org/2021.acl-long.475.pdf\nContribution This paper proposes a weakly supervised framework for abstractive Query Focused Summarization (QFS) under low-resource setting (no query-related resources are required). In particular, the paper introduces a Masked ROUGE Regression framework (MARGE) for evidence sentences estimation and ranking, which relies on a unified representation for summaries and queries. As such, summaries in generic data can be converted into proxy queries for learning a query model.\nThe proposed framework achieves state-of-the-art results on both evidence ranking and abstractive QFS despite the weakly supervised setting.\nDetails Query-Focused Summarization (QFS) Given a specified query $Q$ and a cluster of documents $D$, the task is to generate a short summary $S$ answering the query. $Q$ often consists of a short title and a query narrative which is longer and more detailed.\nBackground Given that ${(D, Q, S)}$ is not readily available, recent work resort to distant supervision from query-relevant NLP resources including question answering. However, queries in QA datasets and QFS are two types of queries and therefore are not identically distributed. Also, it is practically infeasible to find appropriate query-related resources for all domains and topics.\nMethod The paper decomposes abstractive QFS into two subtasks: (1) Query Modeling. Find supportive evidence within a document collection/cluster for a query; (2) Query Focused Generation. Generate an abstractive summary based on found evidence.\nQuery Modeling Overview The paper trains a query model $q_{\\theta}(D| Q ; \\theta)$ to estimate whether each sentence in document cluster $D$ is relevant to query $Q$. The paper trains the model on (summary, sentence) pairs via distant supervision derived from generic summarization data ${(D, S)}$ (without $Q$), which is easier to obtain.\nAssumptions  Summaries themselves constitute a response to latent queries. Answers to queries can be found in sentences from document cluster $D$. If sentences have a high ROUGE score against the reference summary they are likely to contain an answer.  Unified Masked Representation (UMR) The paper renders queries and summaries in a Unified Masked Representation (UMR) which enables summaries to serve as proxy queries for model training. Therefore, even there are no real query during training, the ground truth summary can indeed be proxy queries to assist training.\n UMR for summaries: The UMR for a summary is indeed a summary where some arguments of propositions (using OpenIE) are randomly masked. UMR for quries: the paper manually collects a small set of query words and mask these words out. For example, how is A , what is B, describe A and tell me B.\n The paper uses proxy query $\\text{UMR}_S$ during training and an actual query $\\text{UMR}_Q$ during inference.\nEvidence Ranking The paper feeds the concatenation of a UMR query and a candidate sentence from document cluster into BERT in the form of \u0026ldquo;[CLS] $\\mathcal{U}$ [SEP] $\\mathcal{C}$ [SEP]\u0026rdquo; to estimates whether the sentence contains sufficient evidence to answer the query.\n$\\mathcal{U}$ is a sequence of tokens within a UMR query ($\\text{UMR}_S$/$\\text{UMR}_Q$) and $\\mathcal{C}$ a sequence of tokens in a document sentence. The highest ranked sentences are deemed query-relevant and passed on to the summary generation model.\nThe paper designs a masked ROUGE regression to update BERT during training by using ROUGE as a distant supervision signal.\nQuery Narrative Expansion In some cases queries may be relatively short and narratives absent. This can be problematic for the paper\u0026rsquo;s setup since summary-based query proxies are typically long and detailed. To match the distribution, query narratives are automatically created for short queries in an unsupervised fashion.\nQuery Focused Generation The generation model $p_{\\phi}(S | D, Q ; \\phi)$ generates query focused abstractive summary $S$ conditioned on evidence provided by the query model and (optionally) the query itself.\nSummarization Input For each instance, all sentences are ranked in descending order according to their ROUGE-2 score against the reference summary. The pretrained language model is fine-tuned against this evidence-ranked list of sentences. During inference, when actual queries are available, top sentences ranked by the query model are used as input to generate.\nSummary Length Control Inform the model the summary length by prepending an expected length token (e.g. [250]) to sentences selected by the evidence ranker.\n","permalink":"https://tangliyan.com/blog/posts/generating_query_focused/","summary":"Authors: Yumo Xu, Mirella Lapata","title":"Paper Review - Generating Query Focused Summaries from Query-Free Resources"},{"content":"Authors: Thibault Sellam, Dipanjan Das, Ankur P. Parikh Paper reference: https://aclanthology.org/2020.acl-main.704.pdf\nContribution This paper proposes BLEURT, a learned evaluation metric based on BERT that can model human judgments. It develops a novel pre-training scheme that uses millions of synthetic examples to improve model\u0026rsquo;s generalizability and robustness.\nBLEURT (Bilingual Evaluation Understudy with Representations from Transformers) provides state-of-the art performance for the WMT Metrics Shared task (2017-2019). The pre-trainig scheme (1) yields superior results even when the training data is scarce; and (2) makes BLEURT significantly more robust to quality drifts (out-of-distribution data) and quickly adapt to the new tasks.\nDetails WMT Metrics Shared Task is an annual benchmark in which translation metrics are compared on their ability to imitate human assessments.\nPre-training on Synthetic Data In general, the paper generates a large number of of synthetic reference-candidate pairs $(z, \\tilde{z})$, and trains a BERT on several supervision signals with a multi-task loss. BLEURT models are trained in three steps: (1) regular BERT pre-training; (2) second phase pre-training on synthetic data; and (3) fine-tuning on task-specific ratings.\nGenerating Sentence Pairs This paper generates synthetic sentence pairs $(z, \\tilde{z})$ by randomly perturbing 1.8 million segments $z$ from Wikipedia wit three techniques: mask-filling with BERT, backtranslation, and randomly dropping out words.\nPre-training Signals For each sentence pair, the model calculates a weighted loss from a set of pre-training signals, including: (1) Automatic Metrics: BLEU, ROUGE, BERTscore. (2) Backtranslation Likelihood. (3) Textual Extailment. (4) Backtranslation Flag.\n","permalink":"https://tangliyan.com/blog/posts/bleurt/","summary":"Authors: Thibault Sellam, Dipanjan Das, Ankur P. Parikh","title":"Paper Review - BLEURT: Learning Robust Metrics for Text Generation"},{"content":"Authors: Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed Paper reference: https://arxiv.org/pdf/2007.14062v2.pdf\nContribution This paper proposes a sparse attention mechanism, BigBird, that applys sparse, global, and random attention to approximate full attention model such as BERT and RoBERTa, and reduces the quadratic dependency to linear.\nWith this modification of attention mechanism, the model run under similar hardware can now handle longer text up to a length of 4096 at a much lower computational cost. As a consequence of the capability to handle longer context, BigBird has achieved state of the art results on various long document NLP tasks, such as question answering and summarization on a number of different datasets.\nDetails Previous methods in handling long text There are two lines of works in handling long text: (1) Use some mechanisms to select a smaller subset of relevant contexts to feed in the model and optionally iterate. The paper mentions that these methods often require significant engineering efforts and are hard to train. (2) Come up with approaches that do not require full attention to reduce the memory and computation requirements since full attention may not be necessary.\nDrawback: with these methods, the same model architecture do not attain SoTA on multiple standard benchmarks.\nBigBird The paper explains the proposed sparse attention mechanism with graphs. The BigBird model consists of three main parts:\n(a, For Long-range dependencies) All tokens attending to a set of $r$ random tokens.\n With random graph construction, a random graph can approximate the complete graph spectrally.\n (b, For Local dependencies) All tokens attending to a set of $w$ local neighboring tokens.\n In the terminology of graph theory, clustering coefficient is a measure of locality of connectivity and small world graphs exhibit high clustering coefficient. With this in mind, to capture local structures in the context, the paper defines a sliding window attention, so that during self attention of width $w$, query at location $i$ attends from $i − w/2$ to $i + w/2$ keys.\n (c, For Long-range dependencies) A set of $g$ global tokens attending on all parts of the sequence.\n Experiments show that random blocks and local window were insufficient in capturing all the context necessary to compete with the performance of BERT. With theoretical analysis of Sparse Attention Mechanism, the paper utilizes \u0026ldquo;global tokens\u0026rdquo; to attend over the entire sequence. There are two ways to define global tokens: (1) make some existing tokens “global”; (2) include additional “global” tokens such as CLS.\n This way, each query token attends only to a subset of all possible tokens while yielding a good approximation of full attention.\n","permalink":"https://tangliyan.com/blog/posts/bigbird/","summary":"Authors: Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed","title":"Paper Review - Big Bird: Transformers for Longer Sequences"},{"content":"Authors: Senci Ying, Yanzhao Zheng, Wuhe Zou Paper reference: https://aclanthology.org/2021.sdp-1.12.pdf\nContribution This paper proposes a session based automatic summarization model which uses an ensemble method with both extraction and abstraction based model to generate a long summary for a scientific document.\nThe results shows that the proposed method can generate sentences with high readability and cover important information of the paper. However, the fluency and coherence of the generated summary is insufficient.\nDetails This paper addresses the problem of summarizing scientific documents and generating very long summaries. The paper splits the task into the following steps.\nSession generation First, select the appropriate session size and a buffer (2048 and 128 words in the paper respectively). The buffer is used to keep the last text of the previous session as the context of the current session.\nDivide the ground truth summary into sentences, and assign sentences to each session according to the rouge score (continue adding more sentences until rouge score s of added sentences adds up to a certain value). In this way, a new dataset with (session, summary) pair is created.\nGenerate Summary with Abstraction and extraction based model This work trains both an abstraction-based and an extraction-based model (list a few in the paper). These two types of model both predict the summary for each session.\nEnsemble summaries The final summary is obtained by merging the summaries of two types of models through the ensemble method: (1) Drop overlapping summaries between sessions. Summary overlap is defined with certain threshold. (2) For each session, put abstract summary before extractive summary. Then concatenate the summary from each section. (3) Filter the combined summaries again by using TextRank algorithm to drop less important sentences.\n","permalink":"https://tangliyan.com/blog/posts/long_summ_2021/","summary":"Authors: Senci Ying, Yanzhao Zheng, Wuhe Zou","title":"Paper Review - LongSumm 2021: Session based automatic summarization model for scientific document"},{"content":"Authors: Tanya Goyal, Greg Durrett Paper reference: https://aclanthology.org/2021.naacl-main.114.pdf\nContribution Previous works assume that a factuality model trained on synthetic data can transfer to realistic settings. This paper investigates how factuality models (sentence-level and dependency-level classification models) trained on synthetic and human-labeled datasets perform on real generation errors.\nExperiments show that, in fact, synthetic datasets do not reflect the error distributions of actual generation models, and therefore models trained on this synthetic data perform poorly when evaluated on actual generation errors although they fit the synthetic data distributions well. Instead, models trained on small amount of human annotations can significantly boost performance.\nDetails Ways to create synthetic datasets Entity-centric Synthetic Data (Ent-C) Entity-centric synthetic data mostly consists of a few transformations: (1) Entity Swap, (2) Number Swap, (3) Pronoun Swap, (4) Negation, and (5) Paraphrase.\nGeneration-centric Synthetic Data (Gen-C) The assumption for generation-centric approach is: generated paraphrases at the bottom of a paraphrasing model’s beam are more likely to contain factual errors than 1-best generations, and new information in these generations can be labeled non-factual.\nFactuality Models Given a source document $D$, a factuality model trained on synthetic/human-annotated datasets predicts whether all the information in a generated summary $S$ is supported by $D$.\nSentence-Factuality Model Sentence-Factuality Model predicts by feeding the concatenation of the source document and the generated summary into BERT.\nArc-Factuality model The paper uses Dependency Arc Entailment (DAE) model to evaluate factuality at the dependency arc level. The DAE model predicts whether the relationship described by the arc is entailed by the input document. If any dependency arc is non-factual, the generated summary is labeled as non-factual.\nDownstream Application This paper proposes the training objective only maximizes the likelihood of factual words in the summary and ignore the tokens with unsupported facts. Training on unsupported acts will encourage the model to hallucinate new content.\n","permalink":"https://tangliyan.com/blog/posts/annotating_and_modeling/","summary":"Authors: Tanya Goyal, Greg Durrett","title":"Paper Review - Annotating and Modeling Fine-grained Factuality in Summarization"},{"content":"Authors: Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao Jiang, Graham Neubig Paper Reference: https://arxiv.org/pdf/2010.08014.pdf\nContribution This paper proposes a guided summarization framework (GSum) that can constrain the abstractive summarization by taking different types of guidance signals (automatic extracted or user-specified guidance) as input. Experiments show that the learned model does learn to depend on the guidance signals, and the generate text is faithful and has more novel n-grams. The framework works better when the dataset is more extractive.\nThe paper investigates four types of guidance signals and shows that they are complementary to each other. There is potential to aggregate their outputs together and obtain further improvements.\nDetails Model The framework builds upon a modification of BART, which consists of two encoders that respectively encodes the input source document and guidance signals.\n$$ \\begin{aligned} \u0026amp;\\mathbf{x}=\\mathrm{LN}(\\mathbf{x}+\\operatorname{SelfAttn}(\\mathbf{x})), \\\\ \u0026amp;\\mathbf{x}=\\mathrm{L N}(\\mathbf{x}+\\text {FeedForward}(\\mathbf{x})). \\end{aligned} $$\nThe decoder attends to both the source document and guidance signal. The guidance signal will inform the decoder which part of the source documents should be focused on:\n$$ \\begin{aligned} \u0026amp;\\mathbf{y}=\\mathrm{LN}(\\mathbf{y}+\\operatorname{SelfAttn}(\\mathbf{y})), \\\\ \u0026amp;\\mathbf{y}=\\mathrm{LN}(\\mathbf{y}+\\operatorname{CrossAttn}(\\mathbf{y}, \\mathbf{g})), \\\\ \u0026amp;\\mathbf{y}=\\mathrm{LN}(\\mathbf{y}+\\operatorname{CrossAttn}(\\mathbf{y}, \\mathbf{x})), \\\\ \u0026amp;\\mathbf{y}=\\mathrm{LN}(\\mathbf{y}+\\text {FeedForward}(\\mathbf{y})) . \\end{aligned} $$\nGuidance $$ \\arg \\max_{\\theta} \\sum_{\\left\\langle\\mathbf{x}^{i}, \\mathbf{y}^{i}, \\mathbf{g}^{i}\\right\\rangle \\in\\langle\\mathcal{X}, \\mathcal{Y}, \\mathcal{G}\\rangle} \\log p\\left(\\mathbf{y}^{i} \\mid \\mathbf{x}^{i}, \\mathbf{g}^{i} ; \\theta\\right) $$\nGuidance can be defined as some type of signal $g$ that is fed into the model in addition to the source document $x$. There are two ways to define guidance signal: (1) Oracle extraction. Use both $x$ and $y$ to deduce a signal $g$ that is most likely useful in generating $y$. (2) Automated prediction/extraction. Use an automated system (BertExt, BERTAbs) to infer the guidance signal $g$ from input $x$.\nThe paper investigate the following four types of guidance signal (Oracle extraction at training time, but automatic extracted or user-specified guidance at test time):\n Highlighted Sentences. Select salient sentences from the source document. Keywords. Select salient keywords from highlighted sentences. Relations. Select salient relational tuples (subject, relation, object). Retrived Summaries. Select most similar source documents/summaries.  The use of oracle guidance at training time has a large advantage of generating guidance signals that are highly informative, thus encouraging the model to pay more attention to them at test time.\nExperiements show that using highlighted sentences as guidance achieves the best performance.\n","permalink":"https://tangliyan.com/blog/posts/gsum/","summary":"Authors: Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao Jiang, Graham Neubig","title":"Paper Review - GSum: A General Framework for Guided Neural Abstractive Summarization"},{"content":"Contribution Authors: Darsh Shah, Lili Yu, Tao Lei, Regina Barzilay Paper Reference: https://aclanthology.org/2021.naacl-main.411.pdf\nThis paper develops a system for generating summaries that capture the consensus (similarities and contradictions) in multiple input documents. Specifically, instead of training an end-to-end generation model, it proposes a Content Selection and Aggregation model to identify key contents that should be put in the summary and then a Surface Realization model to convert these contents into a summary.\nBy separately training two components, the framework requires few parallel examples for training. Generated summaries from the proposed framework are evaluated to be faithful and cognizant of consensus in the input documents. The model is also capable of fusing new studies into original generated summary.\nDetail The paper uses a Food and Health summary dataset HealthLine, which consists of scientific abstracts as inputs and human written summaries as outputs. The task is to generate a text summary $y$ for a food from multiple scientific abstracts $X$.\nModel Entity extraction and relation classification For both input documents $X$ and the summary $y$, food health entity-entity relations are extracted from entity extraction and relation classification modules trained on corresponding annotations, and then converted into a mini-database of relation tuples.\nContent Selection and Aggregation For each instance, the content selection model takes produced mini-database of entity-entity relation tuples as input, and (1) identifies subset of key tuples $C$; (2) then identifies aggregation operator $O$.\nContent Selection. The paper uses a reinforcement learning algorithm to train the content selection model. It\u0026rsquo;s trained to gradually add more entity-entity relation pairs to selected content $C$, and encourages the model to concisely select relevant and diverse content.\nConsensus Aggregation. The paper lists a few pre-defined rules to determine the aggregation operator of $C$ in context of all entity-entity relation tuples in the mini-database. There are four aggregation operator: (1) Under-Reported; (2) Population Scoping; (3) Contradiction; and (4) Agreement.\nSurface Realization The surface realization model performs summarization and is trained using (1) ground truth summary $y$; (2) all entity-entity relation tuples from source document; (3) aggregation operator.\nDuring inference, the summary is conditioned on (guided by) (1) selected content $C$; (2) the aggregation operator $O$.\n","permalink":"https://tangliyan.com/blog/posts/nutri_bullets/","summary":"Authors: Darsh Shah, Lili Yu, Tao Lei, Regina Barzilay","title":"Paper Review - Nutri-bullets Hybrid: Consensual Multi-document Summarization"},{"content":"Authors: James Mullenbach, Yada Pruksachatkun, Sean Adler, Jennifer Seale, Jordan Swartz, T. Greg McKelvey, Hui Dai, Yi Yang, David Sontag Paper reference: https://arxiv.org/pdf/2106.02524.pdf\nContribution This paper creates a clinical dataset (CLIP) over 718 randomly selected MIMIC-III discharge notes. The task is to extract all sentences having action items from these notes. It regards this task as multi-aspect extractive summarization and approaches it as a multi-label sentence classification problem given the length of each note.\nThe paper also explores the impact of unsupervised learning on this task. It proposes a task-targeted pre-training method that can shorten pre-training time while maintain high performance. This method is useful if effective public models does not exist for a given task.\nDetails Task formulation  Action item: a statement in a discharge note that explicitly or implicitly directs the reader to an action that should be taken as a result of the hospital stay described in the document.\n The task is to select sentences that contain action items for primary care providers (PCPs) or patients from a discharge note. Given the length of these documents and the risk of missing information, the paper approaches the task as (1) Multi-label classification; (2) binary-label classification (whether there exists any label in a sentence).\nCLIP dataset CLIP dataset stands for ClinicalFollowUp, which is created over MIMIC-III discharge notes.\nStatistics     # discharge notes # sents # sents with labels       718 (random) 107,494 12,079     Of the sentences with labels, 28.6% have multiple labels. There are ~150 sentences per discharge note.\n   Label Type Frequency (sentence level)     Patient Instructions 6.55%   Appointments 4.59%   Medications 1.88%   Lab Tests 0.69%   Procedures 0.28%   Imaging 0.18%   Other 0.05%    Label Type and examples Model structures Baselines  BERT. MIMIC-DNote-BERT. BERT that further pre-trains on MIMIC-III discharge notes. MIMIC-Full-BERT. BERT that further pre-trains on all MIMIC-III notes.  Add neighboring context The paper shows that the neighboring context (one left + one right) is important gien two observations: (1) an individual sentence may not have the full picture on the type of the action; (2) neighboring sentences tend to share the same label (occurs for 27% of sentences).\nTask-targeted pre-training (TTP) Task-targeted pre-training (TTP) uses model predictions to select sentences for pre-training. From previous study, TTP requires less data and computation, yet attains comparable performance to pre-training on large in-domain datasets.\nThis technique is useful in scenarios in which users have large, domain-specific, private datasets and specific tasks, and if effective public models does not exist for a given task.\nTargeted Dataset Construction (1) Fine tune a vanilla BERT model on the multilabel classification task; (2) The learned model classifies all unlabeled sentences. (3) Create a dataset by selecting all sentences having action items (targeted sentences) using a threshold, plus neighboring sentences.\nThe size the of the created dataset can vary depending on the threshold (higher for task-focused and lower for general dataset).\nSecond phase Pre-training The paper pre-trains a BERT-Context model on the targeted dataset with two auxiliary tasks: (1) MLM. Mask tokens in the context sentence only.\n(2) Sentence switching. Swap the focus sentence with another randomly chosen sentence from the same document with certain probability, and predict whether the focus sentence was swapped using the context sentences.\nEvaluation Evaluation metrics are Micro/Macro F1 \u0026amp; AUC, and binary F1.\n(1) The results demonstrate the importance of domain-specific pre-training. (2) Using neighboring sentences (models with \u0026ldquo;+Context\u0026rdquo;) provides a performance boost across all metrics. (3) When using just 250k sentences from the MIMIC discharge notes for pre-training (TTPBERT-Context 250K), task results are competitive with and in some cases exceed MIMIC-DNoteBERT+Context, which is pre-trained on all MIMIC discharge notes.\nThe in-domain pre-training for MIMIC-DNote-BERT models provides gains for nearly all label types, and including context also gives a boost to the F1 score of most labels.\nError Analysis (1) Large amount of tokenization errors on clinical jargon, abbreviations, and misspellings. Add explicit clinical knowledge is necessary. (2) Temporal Expressions.\n","permalink":"https://tangliyan.com/blog/posts/clip/","summary":"Authors: James Mullenbach, Yada Pruksachatkun, Sean Adler, Jennifer Seale, Jordan Swartz, T. Greg McKelvey, Hui Dai, Yi Yang, David Sontag","title":"Paper Review - CLIP: A Dataset for Extracting Action Items for Physicians from Hospital Discharge Notes"},{"content":"Authors: Yang Liu, Sheng Shen, Mirella Lapata Paper reference: https://aclanthology.org/2021.naacl-main.56.pdf\nContribution Single reference dataset with maximum-likelihood training might not be optimal for summarization as there can be multiple valid summaries for a source input. This paper alleviates such problems by the use of self-knowledge distillation, where the teacher and student have identical neural network architectures.\nWith self-knowledge distillation, teacher outputs can be viewed as an enrichment of the single reference setting (multiple summaries) to prevent the student from becoming overconfident in its predictions (add uncertainty).\nThe paper shows that self-knowledge distillation (1) improves over teacher models in both pretrained and non-pretrained settings; and (2) leads the student significantly more succinct, informative, and factual consistent compared to the teacher at the expense of fluency. The injection of noise brings further improvements.\nDetails Add Noise The paper designs different noise mechanisms for the teacher and student.\nNoisy teacher: to inject noise into the distillation signals. It applies teacher dropout mechanism while generating teacher predictions for training the student.\nNoisy student: to inject noise into the training data. It perturbs the source document by (1) dropping words; (2) replacing synonyms; and (3) dropping sentences.\nThe training objective $$ \\mathcal{L}_{\\mathrm{FINAL}}=(1-\\lambda) \\mathcal{L}_{\\mathrm{NLL}}+\\lambda \\mathcal{L}_{\\mathrm{KD}} $$ is standard negative log likelihood plus a knowledge distillation loss (to imitate the teacher\u0026rsquo;s outputs) where\n$$ \\mathcal{L}_{\\mathrm{KD}}=\\sum_{t=1}^{T} \\mathrm{KL}\\left(\\tilde{p}_{T}^{\\alpha}\\left(y_{t} \\mid y_{1}^{t-1}, x\\right), p_{S}\\left(y_{t} \\mid y_{1}^{t-1}, \\tilde{x}\\right)\\right),\\ $$\n$\\tilde{p}_{T}^{\\alpha}$ indicates the predictions from the teacher model with active dropout $\\alpha$ and $\\tilde{x}$ is perturbed source input.\n","permalink":"https://tangliyan.com/blog/posts/noisy_sele_knowledge/","summary":"Authors: Yang Liu, Sheng Shen, Mirella Lapata","title":"Paper Review - Noisy Self-Knowledge Distillation for Text Summarization"},{"content":"Authors: Xinyu Hua, Lu Wang Paper reference: https://aclanthology.org/2020.emnlp-main.57.pdf\nContribution This work presents a novel content-controlled text generation framework, PAIR, with planning and Iterative refinement. It focuses on generating more relevant and coherent text by incorporating content plans into models.\nThe paper first designs a content planning model trained from BERT to automatically construct the initial content plan (template), which assigns keyphrases to different sentences and predicts their positions. Then it proposes a refinement algorithm to gradually improve the generation quality with BART by masking and filling updated templates.\nThe refinement algorithm can steadily boost performance and improves both content and fluency.\nDetails Task formulation: Given a sentence-level prompt and a set of keyphrases relevant to the prompt, the task is to do a long-form text generation that contains multiple sentences which reflect the keyphrases in a coherent way.\nContent Planning with BERT The content planning model is trained from BERT to assign keyphrases to different sentences and predict their corresponding positions.\nAdding Content Plan with a Template Mask-and-Fill Procedure Convert the content plan into a template. For each sentence, the assigned keyphrases are placed at their predicted positions, and empty slots are filled with [MASK] tokens. The decoder then generate an output by filling these mask.\nIterative Refinement At each iteration, the least confident tokens are replaced with [MASK] to serve as the next iteration\u0026rsquo;s template.\n","permalink":"https://tangliyan.com/blog/posts/pair/","summary":"Authors: Xinyu Hua, Lu Wang","title":"Paper Review - PAIR: Planning and Iterative Refinement in Pre-trained Transformers for Long Text Generation"},{"content":"Authors: Sihao Chen, Fan Zhang, Kazoo Sone, Dan Roth Paper reference: https://aclanthology.org/2021.naacl-main.475.pdf\nContribution This paper studies contrast candidate generation and selection as a post-processing method to correct extrinsic hallucinations on entities and quantities in abstractive summarization. Specifically, in the generation step, candidate summaries are created by replacing potentially hallucinated entities in a summary by ones with compatible semantic types that are present in the source. In the selection step, all variants of summaries are scored and the one with highest score will be the final summary. The corrected summaries present statistically significant improvements over the original ones.\nThe work points out some limitations of the proposed method. Replacing entities potentially introduces intrinsic hallucinations in the changed summary. Further, it is not sufficient to fully detect all hallucinations by only focusing on entities and quantities, and it might require commonsense reasoning and knowledge retrieval.\nDetails Contrast Candidate Generation The technique in this paper is based on the observation that a large fraction of extrinsic hallucinations is from named entities and quantities.\nIdentify potentially hallucinated entities Potentially hallucinated entities are identified by checking whether entities with similar surface forms have appeared in the source document using an NER system.\nContrast Candidate Selection This paper trains a classifier (BART + a linear layer) to score and rank the summariy variants. The classifier is trained with ground truth and synthetic negative summaries. The candidate with the highest score will be the final summary.\n","permalink":"https://tangliyan.com/blog/posts/improving_faithfulness_in/","summary":"Authors: Sihao Chen, Fan Zhang, Kazoo Sone, Dan Roth","title":"Paper Review - Improving Faithfulness in Abstractive Summarization with Contrast Candidate Generation and Selection"},{"content":"Authors: Logan Lebanoff, Kaiqiang Song, Franck Dernoncourt, Doo Soon Kim, Seokhwan Kim, Walter Chang, Fei Liu Paper reference: https://aclanthology.org/P19-1209.pdf\nContribution This paper falls into the line of extract-then-abstract line of summary generation. It focuses on selecting summary-worthy sentence singletons and pairs and uses them as the basis for summary generation. This is based on an observation that 60-85% of summary sentences are generated by fusing one or two source sentences from three datasets (CNN/DM, XSum, DUC-04) experimented with.\nMore specifically, it first creates all possible sentence singletons and pairs (call them instances) and then uses a BERT model to determine whether an instance is appropriate to be selected in the first place. These selected instances are then treated as the input for later abstractive summarization. At the inference time, the summarization receives an instance from BERT and outputs a summary sentence, then repeats this process to generate several sentences. There should be some coherence problems since these summary sentences are generated separately.\nDetails Model structure illustration: a sentence pair is chosen (red) and then merged to generate the first summary sentence. Next, a sentence singleton is selected (blue) and compressed for the second summary sentence.\n","permalink":"https://tangliyan.com/blog/posts/scoring_sentence_singletons/","summary":"Authors: Logan Lebanoff, Kaiqiang Song, Franck Dernoncourt, Doo Soon Kim, Seokhwan Kim, Walter Chang, Fei Liu","title":"Paper Review - Scoring Sentence Singletons and Pairs for Abstractive Summarization"},{"content":"Authors: Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, Lu Wang Paper reference: https://aclanthology.org/2021.naacl-main.112.pdf\nContribution This paper first proposes an efficient encoder-decoder attention with head-wise positional strides (HEPOS), which allows at least doubled input sequence size. HEPOS uses separate encoder-decoder heads on the same layer to cover different subsets of source tokens and all heads collectively attend to the full sequence.\nIt collects a new large-scale dataset GOVREPORT, which has significantly longer documents and summaries and salient content is spread throughout the documents.\nThe paper also proposes a new QA based evaluation metric for faithfulness. It is shown to be better correlated with human judgment.\nDetails Existing efficient self-attentions The paper lists some representative and efficient self-attention encoder variants that can build on BART.\nFixed Patterns:\n Sliding window attentions (Beltagy et al., 2020). Adaptive span (Sukhbaatar et al. 2019). Global tokens (Beltagy et al., 2020). Stride patterns (Child et al. 2019). Random attention (motivated by Zaheer et al., 2020).  Low-rank Methods:\n Linformer (Wang et al. 2020), it can be encoder or adapted for encoder-decoder attentions.  Learnable Patterns:\n Locality-sensitive hashing (LSH) attentions (Kitaev et al., 2020). Sinkhorn attentions (Tay et al., 2020a).  Among all encoder variants, learnable patterns perform the best, approaching the performance of full attentions.\nEncoder-decoder Attention with Head-wise Positional Strides (HEPOS) HEPOS allows models to consume longer sequences and it is based on two observations: (1) Attention heads are redundant; (2) Any individual head rarely attends to several tokens in a row.\nHEPOS is capable of both identifying the salient content and capturing the global context. Human judges rate the summaries generated using HEPOS to be more informative and faithful.\nComparing to truncation, models that read more text obtain higher ROUGE scores and the generated summaries are more faithful and informative.\n","permalink":"https://tangliyan.com/blog/posts/efficient_attentions_for/","summary":"Authors: Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, Lu Wang","title":"Paper Review - Efficient Attentions for Long Document Summarization"},{"content":"Authors: Tiezheng Yu, Zihan Liu, Pascale Fung \\ Paper reference: https://aclanthology.org/2021.naacl-main.471.pdf\nContribution This paper studies domain adaptation for the abstractive summarization task. Specifically, it investigates adding a second phase pre-training under low-resource setting across diverse domains, and shows that applying RecAdam can effectively maintain the pre-trained models' knowledge in the first stage of pre-training and alleviate catastrophic forgetting. At the end, it leaves future works some challenges in low-resource domain adaptation for abstractive summarization.\nPre-training settings This paper uses BART as the base model.\n Source Domain Pre-Training (SDPT): continue pre-training BART using the summarization task with the source (news) domain summarization data. The purpose of this pre-training is to inject the task knowledge into the pre-trained language model so that the model can quickly adapt to the same task in target domains. Domain-Adaptive Pre-Training (DAPT): continue pre-training BART using its original pre-training objective function on unlabeled domain-related data. Task-Adaptive Pre-Training (TAPT): continue pre-training BART on a set of the unlabeled documents in the target domain’s summarization task uses a much smaller but far more task-relevant pre-training corpus compared to TAPT\n RecAdam penalize the loss function if the learned parameters (after the second-phrase pre-training) is far way from the original parameters. This is why it can keep models' knowledge learnt from the first stage pre-training.\nTakeaways  SDPT and TAPT are able to generally improve the summarization performance for all domains. The effectiveness of DAPT depends on the relatedness (measured by vocal overlaps) between the pre-training data and the target domain task data. Extensive training data could result in a comparatively large loss from RecAdam since the model’s parameters tend to be greatly modified. Pre-training with relatively short document and summary is more effective for SDPT. The paper leaves how to effectively integrate the task and domain knowledge (SDPT and DAPT) to future works.  ","permalink":"https://tangliyan.com/blog/posts/adaptsum/","summary":"Authors: Tiezheng Yu, Zihan Liu, Pascale Fung","title":"Paper Review - AdaptSum: Towards Low-Resource Domain Adaptation for Abstractive Summarization"},{"content":"Authors: Esin Durmus, He He, Mona Diab Paper reference: https://aclanthology.org/2020.acl-main.454.pdf\nContribution This paper address the problems of evaluating faithfulness of a generated summary given its source document. It finds the trade-off between abstractiveness and faithfulness of a summary and proposes an automatic Question Answering based metric, FEQA, to evaluate faithfulness, which correlates better with human judgements. Later it points out some limitations of QA-based evaluation and suggests to still rely on human-in-the-loop system while evaluating the quality of summaries.\nDetails The Abstractiveness-Faithfulness Tradeoff This work evaluates the faithfulness of generated summary through CNN/DM (more extractive) and XSum (highly abstractive) datasets.\nExperiments show that, on both datasets, outputs having less word overlap with the source document are more likely to be unfaithful and the number of unfaithful sentences increases as the summary becomes more abstractive.\nFEQA: Faithfulness Evaluation with Question Answering Given a summary sentence and its corresponding source document, important text spans (e.g. noun phrases, entities) in the summary are masked using a constituency parser and an NER model. Then, each span is considered as the answer and generate its corresponding question using a BART model. Lastly, a QA model finds answers to these questions in the documents. More matched answers from the document implies a more faithful summary.\nMetric comparison  To evaluate content selection (e.g. ROUGE, BERTScore): compare the content of the generated summary and the content of the reference summary. To evaluate faithfulness (e.g. FEQA): compare the generated summary and the source document.\n The correlation between ROUGE, BERTScore and learned entailment models with human judgements of faithfulness drop significantly on highly abstractive summaries. These metrics rely more on lexical/concept overlap rather than the content overlap. The proposed metric FEQA has much higher correlation instead.\n","permalink":"https://tangliyan.com/blog/posts/feqa/","summary":"Authors: Esin Durmus, He He, Mona Diab","title":"Paper Review - FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization"},{"content":"Authors: Wojciech Kryściński, Nitish Shirish Keskar, Bryan McCann, Caiming Xiong, Richard Socher Paper reference: https://arxiv.org/pdf/1908.08960.pdf\nContribution This paper evaluates the research setup (up until Aug. 2019) of text summarization and highlights shortcomings from: (1) dataset, (2) evaluation metric, and (3) models. Through detailed experiments and human studies, this work suggests future work to add constraints while constructing datasets; diversify models' outputs and less fit to layout bias; design evaluation methods that reflect human judgements and take other dimensions such as factual consistency into account.\nDetails Dataset Under-constrained task The paper conducts studies demonstrating the difficulty and ambiguity of content selection in text summarization. Assessing the importance of information is difficult since it highly depends on the expectations and prior knowledge of readers.\nThe paper shows that under current setting in which models are simply given a document with one associated reference summary (no additional information), the summarization task is under-constrained and is too ambiguous to be solved by end-to-end models. Moreover, due to the abstractive nature of human written summaries, similar content can be described in unique ways.\nBias in news data Initial paragraphs contain the most information.\nNoise in scraped datasets Current summarization datasets are filled with noisy examples (links, code, placeholder texts, etc.)\nEvaluation Metric The current evaluation protocol depends primarily on the exact lexical overlap between reference and candidate summaries measured by ROUGE, but the correlation strength between ROUGE scores and human judgements is low.\nNo method explicitly examines the factual consistency of summaries.\nModels Performance of current models is relied too heavily on the layout bias of news corpora. But models should less fit to a particular domain bias.\nThe diversity of model outputs is low. Generated summaries from diverse models share a large part of the vocabulary on the token level, but differ on how they organize the tokens into longer phrases. Comparing results with the n-gram overlap between models and reference summaries shows a substantially higher overlap between any model pair than between the models and reference summaries.\n","permalink":"https://tangliyan.com/blog/posts/neural_text_summarization/","summary":"Authors: Wojciech Kryściński, Nitish Shirish Keskar, Bryan McCann, Caiming Xiong, Richard Socher","title":"Paper Review - Neural Text Summarization: A Critical Evaluation"},{"content":"Authors: Joshua Maynez, Shashi Narayan, Bernd Bohnet, Ryan McDonald Paper reference: https://aclanthology.org/2020.acl-main.173.pdf\nContributions This paper conducts human evaluations to better understand the types of hallucinations in abstractive summarization. Human evaluations show that pre-trained models are better summarizers in generating faithful and factual summaries. Experiments shows that textual entailment measures better correlate with faithfulness and assess the overall quality of summaries than standard metrics such as ROUGE and BERTScore, which are indicators of informativeness of summaries.\nDetails Hallucinations in Summarization  Factual hallucination: contains information not found in the input document but is factually correct. This may happen due to world knowledge stored during model\u0026rsquo;s pretraining. Intrinsic hallucination: hallucinates by using terms or concepts from the document but misrepresent information from the document (can be factual hallucination). The intrinsic hallucination reveals models’ tendency to misrepresent information in the document due to the lack of document-level understanding and inference. Extrinsic hallucination: hallucinates by adding information not introduced the input document (can be factual hallucination).  Hallucinations are not necessarily erroneous, but most hallucinations are erroneous.\nPretraining improves faithfulness Pretraining prepares models more aware of the domain of the document, models are more confident in predicting tokens from the document, hence, improving faithfulness.\nAutomatic measures for faithfulness The paper further studies the extent of hallucination by two semantic inference related measures: (1) textual entailment, and (2) question answering. The result shows that the textual entailment scores are best correlated with both faithful and factual human scores.\nExperiments suggest that textual entailment could be used (1) as an automatic measure for faithfulness, (2) as a model selection objective: use the probability that a summary is entailed by a document as a selection criteria to select a summary.\nAnother way of measure faithfulness is to directly train a model explicitly to predict faithfulness. The paper shows that this does slightly improve the ability to select faithful summaries.\n","permalink":"https://tangliyan.com/blog/posts/on_faithfulness_and/","summary":"Authors: Joshua Maynez, Shashi Narayan, Bernd Bohnet, Ryan McDonald","title":"Paper Review - On Faithfulness and Factuality in Abstractive Summarization"},{"content":"Authors: Sam Wiseman, Stuart M. Shieber, Alexander M. Rush Paper reference: https://arxiv.org/pdf/1808.10122.pdf\nContribution This paper proposes a data-driven neural generation system with a neural hidden semi-markov model (HSMM) decoder to generate template-like text. More specifically, HSMM learns latent, discrete templates while learning to generate. These templates make generation both more interpretable and controllable, and the performance is close to seq2seq generation models.\nThe proposed model is able to explicitly force the generation to use a chosen template, which is automatically learned (latent) from training data while leaving the database x constant.\nDetails Task Definition This paper tackles a problem of generating an adequate and fluent text description of a database $x$ with a collection of records.\nHSMM Hidden semi-markov model (HSMM) models latent segmentations in an output sequence. In HSMM, emissions may last multiple time-steps. The emission model models the generation of a text segment conditioned on a latent state and source information. It allows interdependence between tokens (but not segments) by having each next-token distribution depend on all the previously generated tokens.\nExtract Templates Templates are useful here because they make it clear what part of the generation is associated with which record in the knowledge base. Every segment in the generated $y$ is typed by its corresponding latent variable.\nGiven a database $x$ and reference generation $y$, hidden variables $z$ can be obtained through Maximum a posteriori (MAP) assignments in a way similar to HMM. These assignments generates a typed segmentation of $y$ and text-segments can be associated with discrete labels $z$ that frequently generate them. Most common sequence of hidden states $z$ after segmentations will be collected from the training data and become templates. These templates can be used to affect the wordordering, as well as which fields are mentioned in the generated text.\n","permalink":"https://tangliyan.com/blog/posts/learning_neural_templates/","summary":"Authors: Sam Wiseman, Stuart M. Shieber, Alexander M. Rush","title":"Paper Review - Learning Neural Templates for Text Generation"},{"content":"Authors: Sebastian Gehrmann, Yuntian Deng, Alexander M. Rush \\ Paper reference: https://aclanthology.org/D18-1443.pdf\nContribution This work proposes a content selection system to extract words that should be part of the summary and demonstrated that the content selector (bottom-up attention) itself is effective at identifying important words.\nWhile Pointer-Generator models have the ability to abstract in summary, the use of a copy mechanism causes the summaries to be mostly extractive. Summarizers can be further improved by using content selector to modify the copy attention distribution of abstractive summarizers and to restrict their ability to copy words from the source. The content selection system is data-efficient and can be applied in low-resource summarization settings.\nDetails Content Selection The paper defines the content selection task as a sequence-tagging problem, with the objective of identifying tokens from a document that are part of its summary. A word $x_i$ in the source document is labeled as 1 if it satisfies certain criteria, 0 otherwise.\nBottom-Up Copy Attention The paper trains, separately, both a pointer-generator model and a content selector on the full dataset. At inference time, the content selector computes selection probabilities $q_{1:n}$ for each token in a source document. A threshold $\\epsilon$ is chosen so that if the selection probability $q_i$ is greater than the threshold, then the original attention score from the pointer-generator model is kept; otherwise the attention score is set to $0$.\nThe selection probabilities from the content selector are used to modify the copy attention distribution to only include tokens identified by the selector. Experiments find that the performance of the abstractive system drops if does not have access to the full source document.\nDiscussion Content selector is quite effective at finding important words (ROUGE-1) but less effective at chaining them together (ROUGE2). The decrease in ROUGE-2 indicates a lack of fluency and grammaticality of the generated summaries.\nThe abstractor has low unigram novelty, which limits the extent of abstraction. The benefit of abstractive models has been less in their ability to produce better paraphrasing but more in the ability to create fluent summaries from a mostly extractive process.\n","permalink":"https://tangliyan.com/blog/posts/bottom_up_sum/","summary":"Authors: Sebastian Gehrmann, Yuntian Deng, Alexander M. Rush","title":"Paper Review - Bottom-Up Abstractive Summarization"},{"content":"Authors: Alexander R. Fabbri, Simeng Han, Haoyuan Li, Haoran Li, Marjan Ghazvininejad, Shafiq Joty, Dragomir Radev, Yashar Mehdad Paper reference: https://arxiv.org/pdf/2010.12836.pdf\nMain Contributions This work uses BART as a base pretrained model and empirically shows that the proposed method WikiTransfer, which creates auxiliary fine-tuning data from generic corpus (Wikipedia) by encoding characteristics of the target summarization dataset, improves zero-shot and few-shot summarization. The method shows further improvement in the few-shot settings when combined with proposed data augmentation and consistency regularization strategies.\nDetails WikiTransfer Fine-tuning The characteristics of the target summarization dataset includes the average length of input documents, the average summary length, and the level of abstraction of the desired summaries.\nAfter auxiliary data (for a target domain) is created by the WikiTransfer method, a pre-trained model (BART) will fine-tune on this dataset-specific WikiTransfer data before transferring to the target domain. This allows a model finetuned on this data to learn characteristics of the target dataset to improve zero-shot and few-shot transfer of the model.\nData Augmentation via Round-Trip Translation This work performs round-trip translation to generate paraphrases of both the source documents and summaries.\nGiven a dataset of $N$ data points, it translates the source and target sentencewise into a non-English language and keep the top $k$ beam hypotheses from beam search as output. Likewise for the back translation, resulting in $N * k^{2}$ augmented data points.\nConsistency regularization To balance learning from while not overfitting to the small number of supervised samples, the model must learn to be robust to small changes in input examples.\nThe work introduces a KL divergence loss to penalizes the model if the probability distribution of the output using the original input is far from the distribution using the round-trip translated input document.\nLet $y$ be a target summary, $\\hat{x}$ be a paraphrase of input document $x$ generated via round-trip translation. The KL divergence loss $L_{\\text {cons }}(x, \\hat{x}, y)$: $$ \\sum_{t=1}^{m} K L (f (\\cdot \\mid y_{0: t-1}, x) | f (\\cdot \\mid y_{0: t-1,}, \\hat{x}), \\theta )) $$\n","permalink":"https://tangliyan.com/blog/posts/improving_zero_and/","summary":"Authors: Alexander R. Fabbri, Simeng Han, Haoyuan Li, Haoran Li, Marjan Ghazvininejad, Shafiq Joty, Dragomir Radev, Yashar Mehdad","title":"Paper Review - Improving Zero and Few-Shot Abstractive Summarization with Intermediate Fine-tuning and Data Augmentation"},{"content":"Authors: Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, Yannis Kalantidis Paper reference: https://arxiv.org/pdf/1910.09217.pdf\nIntroduction When learning with long-tailed data, a common challenge is that instance-rich (or head) classes dominate the training procedure. The learned classification model tends to perform better on these classes, while performance is significantly worse for instance-scarce (or tail) classes (under-fitting).\nThe general scheme for long-tailed recognition is: classifiers are either learned jointly with the representations end-to-end, or via a two-stage approach where the classifier and the representation are jointly fine-tuned with variants of class-balanced sampling as a second stage.\nIn our work, we argue for decoupling representation and classification. We demonstrate that in a long-tailed scenario, this separation allows straightforward approaches to achieve high recognition performance, without the need for designing sampling strategies, balance-aware losses or adding memory modules.\nRecent Directions Recent studies' directions on solving long-tailed recognition problem:\n Data distribution re-balancing. Re-sample the dataset to achieve a more balanced data distribution. These methods include over-sampling, down-sampling and class-balanced sampling. Class-balanced Losses. Assign different losses to different training samples for each class. Transfer learning from head- to tail classes. Transfer features learned from head classes with abundant training instances to under-represented tail classes. However it is usually a non-trivial task to design specific modules for feature transfer.  Sampling Strategies For most sampling strategies presented below, the probability $p_j$ of sampling a data point from class $j$ is given by: $$ p_{j}=\\frac{n_{j}^{q}}{\\sum_{i=1}^{C} n_{i}^{q}} $$ where $q \\in[0,1]$, $n_j$ denote the number of training sample for class $j$ and $C$ is the number of training classes. Different sampling strategies arise for different values of $q$ and below we present strategies that correspond to $q=1, q=0,$ and $q=1 / 2$.\n Instance-balanced Sampling. This is the most common way of sampling data, where each training example has equal probability of being selected. Here $q=1$. For imbalanced datasets, instance-balanced sampling has been shown to be sub-optimal as the model under-fits for few-shot classes leading to lower accuracy, especially for balanced test sets. Class-balanced Sampling. Class-balanced sampling has been used to alleviate the drawback of Instance-balanced Sampling, as each class has an equal probability of being selected. Square-root sampling. Set $q = 1/2$. Progressively-balanced sampling. Combinations of the sampling strategies presented above. In practice this involves first using instance-balanced sampling for a number of epochs, and then class-balanced sampling for the last epochs. Here, we experiment with a softer version, that progressively \u0026ldquo;interpolates\u0026rdquo; between instance-balanced and class-balanced sampling as learning progresses class $j$ is now a function of the epoch $t,$ $$p_{j}^{\\mathrm{PB}}(t)=\\left(1-\\frac{t}{T}\\right) p_{j}^{\\mathrm{IB}}+\\frac{t}{T} p_{j}^{\\mathrm{CB}} $$ where $T$ is the total number of epochs.  Methods of Learning Classifiers Classifier Re-training (cRT) Re-train the classifier with class-balanced sampling. That is, keeping the representations fixed, we randomly re-initialize and optimize the classifier weights $W$ and $b$ for a small number of epochs using class-balanced sampling.\nNearest Class Mean classifier (NCM) First compute the mean feature representation for each class on the training set and then perform nearest neighbor search either using cosine similarity or the Euclidean distance computed on L2 normalized mean features. (the cosine similarity alleviates the weight imbalance problem via its inherent normalization)\n$\\tau$-normalized classifier $(\\tau$-normalized) We investigate an efficient approach to re-balance the decision boundaries of classifiers, inspired by an empirical observation:\n Empirical Observation: after joint training with instance-balanced sampling, the norms of the weights $\\left|w_{j}\\right|$ are correlated with the cardinality of the classes $n_j$, while, after fine-tuning the classifiers using class-balanced sampling, the norms of the classifier weights tend to be more similar.\n Inspired by the above observations, we consider rectifying imbalance of decision boundaries by adjusting the classifier weight norms directly through the following $\\tau$-normalization procedure. Formally, let $\\boldsymbol{W} = {w_{j}} \\in \\mathbb{R}^{d \\times C},$ where $w_{j} \\in \\mathbb{R}^{d}$ are the classifier weights corresponding to class $j .$ We scale the weights of $\\boldsymbol{W}$ to get $\\widetilde{\\boldsymbol{W}}= {\\widetilde{w_{j}} }$ by: $$ \\widetilde{w_{i}}=\\frac{w_{i}}{\\left|w_{i}\\right|^{\\tau}} $$ where $\\tau$ is a hyper-parameter controlling the \u0026ldquo;temperature\u0026rdquo; of the normalization, and $|\\cdot|$ denotes the $L_{2}$ norm. When $\\tau=1$, it reduces to standard $L_{2}$ -normalization. When $\\tau=0$, no scaling is imposed. We empirically choose $\\tau \\in(0,1)$ such that the weights can be rectified smoothly.\nLearnable weight scaling (LWS). Another way of interpreting $\\tau$-normalization would be to think of it as a re-scaling of the magnitude for each classifier $w_{i}$ keeping the direction unchanged. This could be written as $$ \\widetilde{w_{i}}=f_{i} * w_{i}, \\text { where } f_{i}=\\frac{1}{\\left|w_{i}\\right|^{\\tau}} $$ Although for $\\tau$-normalized in general $\\tau$ is chosen through cross-validation.\nNote: NCM and $\\tau$-normalized cases give competitive performance even though they are free of additional training and involve no additional sampling procedure.\nExperiments We perform extensive experiments on three large-scale long-tailed datasets.\nDatasets  Places-LT. Artificially truncated from the balanced version. Places-LT contains images from 365 categories and the number of images per class ranges from 4980 to 5. ImageNet-LT. Artificially truncated from the balanced version. ImageNet-LT has 1000 classes and the number of images per class ranges from 1280 to 5 images. iNaturalist 2018. iNaturalist 2018 is a real-world, naturally long-tailed dataset, consisting of samples from 8,142 species.  Evaluation Protocol To better examine performance variations across classes with different number of examples seen during training, we report accuracy on three splits of the set of classes: Many-shot (more than 100 images), Medium-shot (20∼100 images) and Few-shot (less than 20 images).\nThis figure compares different sampling strategies for the conventional joint training scheme to a number of variations of the decoupled learning scheme on the ImageNet-LT dataset.\nResults Sampling matters when training jointly  For Joint Learning, we see consistent gains in performance when using better sampling strategies. The trends are consistent for the overall performance as well as the medium- and few- shot classes, with progressively-balanced sampling giving the best results. Instance-balanced sampling gives the highest performance for the many-shot classes. This is well expected since the resulted model is highly skewed to the many-shot classes.  Instance-balanced sampling generalize well Among all decoupled methods, we see that Instance-balanced sampling gives the best results. This is particularly interesting, as it implies that data imbalance might not be an issue learning high-quality representations.\nDecoupled Learning strategy helps  For most cases, performance using decoupled methods is significantly better in terms of overall performance, as well as all splits apart from the many-shot case.  To further justify our claim that it is beneficial to decouple representation and classifier, we experiment with fine-tuning the backbone network (ResNeXt-50) jointly with the linear classifier. Here is the result: Fine-tuning the whole network yields the worst performance, while keeping the representation frozen performs best. This result suggests that decoupling representation and classifier is desirable for long-tailed recognition.\nWeight Norm Visualization This figure shows L2 norms of the weight vectors for all classifiers, as well as the training data distribution sorted in a descending manner with respect to the number of instances in the training set.\nCompare to SOTA We compare the performance of the decoupled schemes to other recent works that report state-of-the-art results on on three common long-tailed benchmarks. This is the result for ImageNet-LT.\nContributions  Instance-balanced sampling gives more generalizable representations that can achieve state-of-the-art performance after properly re-balancing the classifiers and without need of carefully designed losses or memory units. It is advantageous in long-tailed recognition to re-adjust the decision boundaries specified by the jointly learned classifier during representation learning (NCM, cRT, $\\tau$-normalized). By applying the decoupled learning scheme to standard networks, we achieve significantly higher accuracy than well established state-of-the-art methods on multiple long- tailed recognition benchmark datasets.   Reference:\n Decoupling Representation and Classifier for Long-Tailed Recognition. https://arxiv.org/abs/1910.09217.  ","permalink":"https://tangliyan.com/blog/posts/decoupling_representation/","summary":"Authors: Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, Yannis Kalantidis","title":"Paper Review - Decoupling Representation and Classifier for Long-Tailed Recognition"},{"content":"Information Extraction v.s. Relation Extraction Information Extraction: Information extraction is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents and other electronically represented sources.\nRelation extraction (RE) is an important task in IE. It focuses on extracting relations between entities. A complete relation RE system consists of\n a named entity recognizer to identify named entities from text. an entity linker to link entities to existing knowledge graphs. a relational classifier to determine relations between entities by given context (most difficult and important).  Existing Works of RE RE methods follows the typical supervised setting, from early pattern-based methods, statistical approaches, to recent neural models.\nPattern-based Methods The early methods use sentence analysis tools to identify syntactic elements in text, then automatically construct pattern rules from these elements. Later work involves larger corpora, more formats of patterns and more efficient ways of extraction.\nStatistical Relation Extraction Models Statistical Methods requires less human efforts so that statistical relation extraction (SRE) has been extensively studied. Approaches includes:\n feature-based methods which design lexical, syntactic and semantic features for entity pairs and their corresponding context (hard to design features). kernel-based methods measures the similarities between relation representations and textual instances (hard to design kernel functions). Graphical methods abstract the dependencies between entities, text and relations in the form of directed acyclic graphs, and then use inference models to identify the correct relations (still limited model capacities). embedding models. Encode text into low-dimensional semantic spaces and extract relations from textual embeddings. E.g. Knowledge Graph (KG) embeddings.  Neural Relation Extraction Methods The performance of SOTA RE models. The adoption of neural models began in 2013. Neural relation extraction (NRE) models can effectively capture textual information and generalize to wider range of data. NRE mainly utilizes both word embeddings and positional embeddings and focus on designing and utilizing various network architectures to capture the relational semantics within text. Methods includes:\n Recursive Neural Networks. Learn compositional representations for sentences recursively. Convolutional neural networks (CNNs). Model local textual patterns. Recurrent neural networks (RNNs). Handle long sequential data. Graph neural networks (GNNs). Build word/entity graphs for reasoning. Attention-based neural networks. Aggregate global relational information.  Currently, Transformers and Pre-trained LM models achieves SOTA on intra-sentence RE.\nFuture Directions Despite the success of existing RE methods, most of them still work in a simplified setting. These methods mainly focus on training models with large amounts of human annotations to classify two given entities within one sentence into pre-defined relations. However, the real world is much more complicated than this simple setting:\n collecting high-quality human annotations is expensive and time-consuming. many long-tail relations cannot provide large amounts of training examples. most facts are expressed by long context consisting of multiple sentences. using a pre-defined set to cover those relations with open-ended growth is difficult.  Utilizing More Data The researchers have formed a consensus that utilizing more data is a potential way towards more powerful RE models.\nDistant supervision (DS) assumption has been used to automatically label data by aligning existing KGs with plain text. For any entity pair in KGs, sentences mentioning both the entities will be labeled with their corresponding relations in KGs.\nHere is an illustration of DS relation extraction. With the fact (Apple Inc., product, iPhone), DS finds all sentences mentioning the two entities and annotates them with the relation product, which inevitably brings noise labels. Methods to Denoise DS Data  Adopt multi-instance learning by combining sentences with same entity pairs and then selecting informative instances from them. Incorporating extra context information such as KGs. Utilize sophisticated mechanisms and training strategies to enhance distantly supervised NRE models.  Open Problem for Utilizing More Data  Existing DS methods focus on denoising auto-labeled instances. Explore better DS schemes is valuable. Perform unsupervised or semi-supervised learning for utilizing large-scale unlabeled data as well as using knowledge from KGs and introducing human experts in the loop.  Performing More Efficient Learning Real-world relation distributions are long-tail and most relations have very limited relational facts and corresponding sentences. We can see the long tail distributions from two DS datasets in the following figure:\nFew-shot Learning Few-shot learning is a good fit for learning long-tail relations efficiently. It trains good representations of instances or learns ways of fast adaptation from existing large-scale data, and then transfer to new tasks.\nA typical few-shot learning setting is the N-way K-shot setting, where models are given N random-sampled new relations, along with K training examples for each relation. Here is an example. Give a few instances for new relation types, few-shot RE models classify query sentences into one of the given relations.\nHere are a few challenges for few-shot learning:\n Few-shot domain adaptation. Few-shot none-of-the-above detection. Conventional few-shot models have the difficulty to form a good representation for the none-of-the-above (NOTA) relation in the N-way K-shot setting. Therefore, it is crucial to study how to identify NOTA instances . Few-shot RE may result in a easy classification task if total amount of relations is small. As the following figure shows, as the number of relations increase, the performance drops. Current models cannot truly understand relations similar in semantics.   Handling More Complicated Context Most existing methods focus on intra-sentence RE and thus are inadequate for identifying relational facts expressed in a long document. Extracting relations from complicated context is a challenging task requiring reading, memorizing and reasoning for discovering relational facts across multiple sentences.\nHere is a post (Connecting the Dots: Document-level Neural Relation Extraction with Edge-oriented Graphs) I wrote for explaining using GNN on document-level relation extraction task.\nOrienting More Open Domains Our world undergoes growth of relations and it is not possible to pre-specify all relations by human experts. Thus, we need RE systems that do not rely on pre-defined relation schemas and can work in open scenarios. Here are current explorations in handling open relations:\n Open information extraction (Open IE), which extracts relation phrases and arguments (entities) from text.  Relation discovery, which discover unseen relation types from unsupervised data. Here is an example of casting relation discovery as a clustering task.   Note: There are many redundant extracted relations. Normalizing these phrases is crucial for downstream tasks. For example, relations on (Barack Obama, was born in , Honolulu) and (Obama, place of birth, Honolulu) are actually identical. So they should be normalized.\n Reference:\n More Data, More Relations, More Context and More Openness: A Review and Outlook for Relation Extraction. https://arxiv.org/pdf/2004.03186.pdf. Connecting the Dots: Document-level Neural Relation Extraction with Edge-oriented Graphs. https://arxiv.org/pdf/1909.00228.pdf. Distant supervision for relation extraction without labeled data. https://www.aclweb.org/anthology/P09-1113.pdf.  ","permalink":"https://tangliyan.com/blog/posts/relation_extraction/","summary":"Information Extraction v.s. Relation Extraction Information Extraction: Information extraction is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents and other electronically represented sources.\nRelation extraction (RE) is an important task in IE. It focuses on extracting relations between entities. A complete relation RE system consists of\n a named entity recognizer to identify named entities from text. an entity linker to link entities to existing knowledge graphs.","title":"Overlook of Relation Extraction"},{"content":"Authors: Fenia Christopoulou, Makoto Miwa, Sophia Ananiadou Paper reference: https://arxiv.org/pdf/1909.00228.pdf\nRelation Extraction (RE) Relation Extraction (RE): The extraction of relations between named entities in text.\nRelation Extraction is an important task of NLP. Most existing works focus on intra-sentence RE. In fact, in real-world scenarios, a large amount of relations are expressed across sentences. The task of identifying these relations is named inter-sentence RE.\nTypically, inter-sentence relations occur in textual snippets with several sentences, such as documents. In these snippets, each entity is usually repeated with the same phrases or aliases, the occurrences of which are often named entity mentions and regarded as instances of the entity.\nThe multiple mentions of the target entities in different sentences can be useful for the identification of inter-sentential relations, as these relations may depend on the interactions of their mentions with other entities in the same document. The figure above is an good example of identifying the relationship between \u0026ldquo;ethambutol\u0026rdquo;, \u0026ldquo;isoniazid\u0026rdquo; and \u0026ldquo;scotoma\u0026rdquo;, where they all interact with the green colored entity (and its alias).\ndocument-level RE   In concept, document-level RE the input is considered an annotated document. The annotations include concept-level entities as well as multiple occurrences of each entity under the same phrase of alias, i.e., entity mentions.\n  Objective: the objective of the task is given an annotated document, to identify all the related concept-level pairs in that document.\n  Document-level RE is not common in the general domain, as the entity types of interest can often be found in the same sentence. On the contrary, in the biomedical domain, document-level relations are particularly important given the numerous aliases that biomedical entities can have (as shown in the figure above).\nIntuition Graph-based neural approaches have proven useful in encoding long distance, inter-sentential information. These models interpret words as nodes and connections between them as edges. They typically perform on the nodes by updating the representations during training.\nThis paper: However, a relation between two entities depends on different contexts. It could thus be better expressed with an edge connection that is unique for the pair. A straightforward way to address this is to create graph-based models that rely on edge representations rather focusing on node representations, which are shared between multiple entity pairs.\nContribution  We propose a novel edge-oriented graph neural model for document-level relation extraction, which encodes information into edge representations rather than node representations. Analysis indicates that the document-level graph can effectively encode document-level dependencies. we show that inter-sentence associations can be beneficial for the detection of intra-sentence relations.  Overview of Proposed Model We presented a novel edge-oriented graph neural model (EoG) for document-level relation extraction using multi-instance learning. The proposed model constructs a document-level graph with heterogeneous types of nodes and edges, modelling intra- and inter-sentence pairs simultaneously with an iterative algorithm over the graph edges.\nHere is an illustration of the abstract architecture of the proposed approach. Proposed Model The proposed model consists of four layers: sentence encoding, graph construction, inference and classification layers. The model receives a document (with identified concept-level entities and their textual mentions) and encodes each sentence separately. A document-level graph is constructed and fed into an iterative algorithm to generate edge representations between the target entity nodes.\nSentence Encoding Layer We use a Bi-LSTM to encode each sentence and then get a contextualized word representations of the input sentence. The contextualized word representations from the encoder are then used to construct a document-level graph structure.\nGraph construction Layer Graph construction consists of Node Construction and Edge Construction.\nNode Construction They form three distinct types of nodes in the graph:\n Mention nodes (M) $n_m$. Mention nodes correspond to different mentions of entities in the input document. The representation of a mention node is formed as the average of the words ($w$) that the mention contains, i.e. $\\operatorname{avg}{w{i} \\in m}\\left(\\mathbf{w}_{i}\\right)$. Entity nodes (E) $n_e$. Entity nodes represent unique entity concepts. The representation of an entity node is computed as the average of the mention ($m$) representations associated with the entity, i.e. $\\operatorname{avg}{m{i} \\in e}\\left(\\mathbf{m}_{i}\\right)$. Sentence nodes (S) $n_s$. Sentence nodes correspond to sentences. A sentence node is represented as the average of the word representations in the sentence, i.e. $\\operatorname{avg}{w{i} \\in s}\\left(\\mathbf{w}_{i}\\right)$.  To distinguish different node types in the graph, they concatenate a node type ($t$) embedding to each node representation. The final node representations are then estimated as $\\mathbf{n}{m}=[\\operatorname{avg}{w_{i} \\in m}\\left(\\mathbf{w}{i}\\right) ; \\mathbf{t}{m}], \\mathbf{n}{e}=$ $[\\operatorname{avg}{m_{i} \\in e}\\left(\\mathbf{m}{i}\\right) ; \\mathbf{t}{e}], \\mathbf{n}{s}=[\\operatorname{avg}{w_{i} \\in s}\\left(\\mathbf{w}{i}\\right) ; \\mathbf{t}{s}]$.\nEdge Construction We pre-define the following edge types:\n  Mention-Mention (MM): Mention-to-mention edges are connected if the corresponding mentions reside in the same sentence. The edge representation between each mention pair $m_{i}$ and $m_{j}$ is generated by concatenating the representations of the nodes, the contexts $c_{m_{i}, m_{j}}$ and a distance embedding associated with the distance between the two mentions $d_{m_{i}, m_{j}},$ in terms of intermediate words: $\\mathbf{x}{\\mathbf{M M}} = [\\mathbf{n}{m_{i}} ; \\mathbf{n}{m{j}} ; \\mathbf{c}{m{i}, m_{j}} ; \\mathbf{d}{m{i}, m_{j}}]$, the context is calculated using $$\\begin{aligned} k \u0026amp;\\in {1,2}\\ \\alpha_{k, i} \u0026amp;=\\mathbf{n}{m{k}}^{\\top} \\mathbf{w}{i} \\ \\mathrm{a}{k, i} \u0026amp;=\\frac{\\exp \\left(\\alpha_{k, i}\\right)}{\\sum_{j \\in[1, n], j \\notin m_{k}} \\exp \\left(\\alpha_{k, j}\\right)} \\ \\mathrm{a}{i} \u0026amp;=\\left(\\mathrm{a}{1, i}+\\mathrm{a}{2, i}\\right) / 2 \\ \\mathrm{c}{m_{1}, m_{2}} \u0026amp;=\\mathbf{H}^{\\top} \\mathrm{a} \\end{aligned}$$ where $\\mathbf{H} \\in \\mathbb{R}^{w \\times d}$ is a sentence word representations matrix.\n  Mention-Sentence (MS): Mention-to-sentence nodes are connected if the mention is in the sentence. The initial edge representation is represented as $\\mathbf{x}{\\mathbf{MS}} = [\\mathbf{n}{m} ; \\mathbf{n}_{s}]$.\n  Mention-Entity (ME): Mention-to-entity nodes are connected if the mention is associated with the entity, $\\mathbf{x}{\\mathbf{ME}} = [\\mathbf{n}{m} ; \\mathbf{n}_{e}]$.\n  Sentence-Sentence (SS): To encode the distance between sentences, they concatenate to the sentence node representations their distance in the form of an embedding. They connect all sentence nodes in the graph, $\\mathbf{x}{\\mathbf{MS}} = [\\mathbf{n}{s_i} ; \\mathbf{n}{s_j}; \\mathbf{d}{s_i,s_j}]$.\n  Entity-Sentence (ES): Entity-to-sentence nodes are connected if at least one mention of the entity is in this sentence, $\\mathbf{x}{\\mathbf{ES}} = [\\mathbf{n}{e} ; \\mathbf{n}_{s}]$.\n  Then there is a linear transformation to make sure each edge representation has the same dimension. For different edge representations, the dimension of the linear transformation layer is different: $$\\mathbf{e}{z}^{(1)}=\\mathbf{W}{z} \\mathbf{x}{z}$$ where $\\mathbf{e}{z}^{(1)}$ is an edge representation of length 1. $\\mathbf{W}{z} \\in \\mathbb{R}^{d{z} \\times d}$ corresponds to a learned matrix and $z \\in[\\mathrm{MM}, \\mathrm{MS}, \\mathrm{ME}, \\mathrm{SS}, \\mathrm{ES}]$.\nInference Layer Note that entity-to-entity (EE) edge is not pre-defined in the previous step. We can only generate EE edge representations by representing a path between their nodes. we adapt two-step inference mechanism to encode interactions between nodes and edges in the graph and hence model EE associations.\nFirst Step Goal: generate a path between two nodes $i$ and $j$ using intermediate nodes $k$ (Intermediate nodes without adjacent edges to the target nodes are ignored).\nWe thus combine the representations of two consecutive edges $e_{ik}$ and $e_{kj}$, using a modified bilinear transformation. This action generates an edge representation of double length. We combine all existing paths between $i$ and $j$ through $k$:\n$$f\\left(\\mathbf{e}{i k}^{(l)}, \\mathbf{e}{k j}^{(l)}\\right)=\\sigma\\left(\\mathbf{e}{i k}^{(l)} \\odot\\left(\\mathbf{W} \\mathbf{e}{k j}^{(l)}\\right)\\right)$$ where $\\mathbf{W} \\in$ $\\mathbb{R}^{d_{z} \\times d_{z}}$ is a learned parameter matrix, $\\odot$ refers to element-wise multiplication, $l$ is the length of the edge and $\\mathbf{e}_{i k}$ corresponds to the representation of the edge between nodes $i$ and $k$.\nSecond Step During the second step, we aggregate the original (short) edge representation and the new (longer) edge representation resulted from Equation (3) as follows:$$ \\mathbf{e}{i j}^{(2 l)}=\\beta \\mathbf{e}{i j}^{(l)}+(1-\\beta) \\sum_{k \\neq i, j} f\\left(\\mathbf{e}{i k}^{(l)}, \\mathbf{e}{k j}^{(l)}\\right)$$ where $\\beta \\in[0,1]$.\nThe two steps are repeated a finite number of times N.\n Everytime when going through step one, we find an intermediate node. With initial edge length equal to 1, the first iteration results in edges of length up-to 2. The second iteration results in edges of length up-to 4. Similarly, after N iterations, the length of edges will be upto $2^N$.Here is an illustration after iterations:   There can be many valid node $k$ between node $i$ and node $j$, here is an illustration   Classification Layer We use the entity-to-entity edges (EE) of the document graph that correspond to the concept-level entity pairs to classify the concept-level entity pairs $$ \\mathbf{y}=\\operatorname{softmax}\\left(\\mathbf{W}{c} \\mathbf{e}{\\mathrm{EE}}+\\mathbf{b}{c}\\right) $$ where $\\mathbf{W}{c} \\in \\mathbb{R}^{r \\times d_{z}}$ and $\\mathbf{b}_{c} \\in \\mathbb{R}^{r}$ are learned parameters of the classification layer and $r$ is the number of relation categories.\nResult Three models being compared:\n Edge-oriented Graph (EoG) refers to our main model with edges {MM, ME, MS, ES, SS}. The EoG (Full) setting refers to a model with a fully connected graph, where the graph nodes are all connected to each other, including E nodes. The EoG (NoInf) setting refers to a no inference model, where the iterative inference algorithm is ignored. The EoG (Sent) setting refers to a model that was trained on sentences instead of documents.   Our model performs significantly better on intra- and inter-sentential pairs, even compared to most of the models with external knowledge. For the inter-sentence pairs, performance significantly drops with EoG(Full) and EoG(NoInf). The former might indicate the existence of certain reasoning paths that should be followed in order to relate entities residing in different sentences. The intra-sentence pairs substantially benefit from the document-level information.   Removal of ES edges reduces the performance of all pairs, as encoding of EE edges becomes more difficult and requires long inference paths as shown in figure (a). [we enable identification of pairs across sentences only through MM and ME edge. In this case, the minimum inference length is 6 (E-M-M-E-M-M-E)]  As shown in figure (b), the introduction of S nodes results in a path with half the length, which we expect to better represent the relation.   The figure above illustrates that for long-distanced pairs, EoG has lower performance, indicating a possible requirement for other latent document-level information.  Bad Cases Analysis  The model cannot find associations between entities if they are connected by \u0026ldquo;and\u0026rdquo;. MIssing coreference connections. Incomplete entity linking.   Reference:\n Connecting the Dots: Document-level Neural Relation Extraction with Edge-oriented Graphs. https://arxiv.org/pdf/1909.00228.pdf.  ","permalink":"https://tangliyan.com/blog/posts/connecting_the_dots/","summary":"Authors: Fenia Christopoulou, Makoto Miwa, Sophia Ananiadou","title":"Paper Review - Connecting the Dots: Document-level Neural Relation Extraction with Edge-oriented Graphs"},{"content":"Cross-lingual learning Most languages do not have training data available to create state-of-the-art models and thus our ability to create intelligent systems for these languages is limited as well.\nCross-lingual learning (CLL) is one possible remedy to solve the lack of data for low-resource languages. In essence, it is an effort to utilize annotated data from other languages when building new NLP models. When CLL is considered, target languages usually lack resources, while source languages are resource-rich and they can be used to improve the results for the former.\nCross-lingual resources The domain shift, i.e. the difference between source and target languages, is often quite severe. The languages might have different vocabularies, syntax or even alphabets. Various cross-lingual resources are often employed to address the gap between languages.\nHere is a short overview of different resources that might be used.\nMultilingual distributional representations With multilingual word embeddings (MWE), words from multiple languages share one semantic vector space. In this space semantically similar words are close together independently on the language they come from.\nDuring the training MWE usually require additional cross-lingual resources, e.g. bilingual dictionaries or parallel corpora. Multilingual sentence embeddings work on similar principle, but they use sentences instead of words. Ideally, corresponding sentences should have similar representations.\nEvaluation of multilingual distributional representations Evaluation of multilingual distributional representations can be done either intrinsically or extrinsically.\n  With intrinsic evaluation authors usually measure how well does semantic similarity reflect in the vector space, i.e. how far apart are semantically similar words or sentences.\n  On the other hand, extrinsic evaluation measure how good are the representations for downstream tasks, i.e. they are evaluated on how well they perform for CLL.\n  Parallel corpus Parallel corpora are one of the most basic linguistic resources. In most cases, sentence-aligned parallel corpus of two languages is used. Wikipedia is sometimes used as a comparable parallel corpus, although due to its complex structure it can also be used as a multilingual knowledge base.\nParallel corpora are most often created for specific domains, such as politics, religion or movie subtitles. Parallel corpora are also used as training sets for machine translation systems and for creating multilingual distributional representations, which makes parallel corpora even more important.\nWord Alignments In some cases, sentence alignment in parallel corpora might not be enough.\nFor word alignments, one word from a sentence in language $\\ell_A$ can be aligned with any number of words from corresponding sentence in language $\\ell_B$. In most cases automatic tools are used to perform word alignment over existing parallel sentences. Machine Translation systems can also often provide word alignment information for their generated sentences.\nMachine Translation Machine translation (MT) can be used instead of parallel corpora to generate parallel sentences. Parallel corpus generated by MT is called pseudo parallel corpus. Although in recent years MT achieved great improvements by using neural encoder-decoder models, machine translation is still far away from providing perfect translations. MT models are usually trained from parallel corpora.\nBy using samples generated by MT systems we inevitably inject noise into our models; The domain shift between a language $\\ell_A$ and what MT systems generate as language $\\ell_A$ needs to be addressed.\nUniversal features (out of fashion) Universal features are inherently language independent to some extent, e.g. emojis or punctuation. These can be used as features for any language. As such, model trained with such universal features should be easily applied to other languages.\nThe process of creating language independent features for words is called delexicalization. Delexicalized text has words replaced with universal features, such as POS tags. We lose the lexical information of the words in this process, thus the name delexicalization.\nBilingual dictionary Bilingual dictionaries are the most available cross-lingual resource in our list. They exist for many language pairs and provide a very easy and natural way of connecting words from different languages. However, they are often incomplete and context insensitive.\nPre-trained multilingual language models Pre-trained language models are a state-of-the-art NLP technique. A large amount of text data is used to train a high capacity language model. Then we can use the parameters from this language model to initialize further training with different NLP tasks. The parameters are fine-tuned with the additional target task data. This is a form of transfer learning, where we use language modeling as a source task. The most well known pre-trained language models are BERT.\nMultilingual language models (MLMs) are an extension of this concept. A single language model is trained with multiple languages at the same time.\n  This can be done without any cross-lingual supervision, i.e. we feed the model with text from multiple languages and we do not provide the model with any additional information about the relations between the languages. Such is the case of multilingual BERT model (mBERT). Interestingly enough, even with no information about how are the languages related, the representations this model creates are partially language independent. The model is able to understand the connections between languages even without being explicitly told to do so. To know more about mBERT, check my previous post about common MLMs. \n  The other case are models that directly work with some sort of cross-lingual supervision, i.e. they use data that help them establish a connection between different languages. Such is the case of XLM, which make use of parallel corpora and machine translation to directly teach the model about corresponding sentences. To know more about XLM, check my previous post about common MLMs. \n  Transfer learning techniques for Cross-lingual Learning Four main categories for CLL:\n Label transfer: Labels or annotations are transferred between corresponding $L_S$ and $L_T$ samples. Feature transfer: Similar to label transfer, but sample features are transferred instead (transfer knowledge about the features of the sample). parameter transfer: Parameter values are transferred between parametric models. This effectively transfers the behaviour of the model. Representation transfer: The expected values for hidden representation are transferred between models. The target model is taught to create desired representations.  Note: Representation transfer is similar to feature transfer. However, instead of simply transferring the features, it teaches the $L_T$model to create these features instead.\nLabel transfer Transferring labels means transferring these labels between the samples from different languages. First, a correspondence is established between $L_S$ and $L_T$ samples. Correspondence means that the pair of samples should have the same label. Then the labels are transferred from one language to the other – this step is also called annotation projection. The projected labels can than be used for further training.\nThere are three types of distinct label transfer and they differ in the language the resulting model takes as an input.\nTransferring labels during training (To train an $L_T$ model) To create an $L_T$ model with label transfer we need an existing annotated dataset or model in $L_S$. Then we establish a correspondence between $L_S$ annotated samples and $L_T$ unannotated samples. The annotations are projected to $L_T$ and resulting distantly supervised $L_T$ dataset is used to train an $L_T$ model.\nWhen machine translation is used to generate the corresponding samples we can\n either take existing $L_S$ samples and translate them into $L_T$ along with their annotations. Or, we can take unannotated $L_T$ data, translate them into $L_S$ and then annotate these translated $L_S$ samples, and finally, the annotations are projected back to the original samples. The former is the more frequent alternative.  Transferring labels during inference (Use existing $L_S$ for inference) An unannotated $L_T$ sample has a corresponding $L_S$ sample generated (e.g. by MT) and then a pre-existing $L_S$ model is used to infer its label. This label is then projected back to the original $L_T$ sample. This approach is quite slow and prone to errors.\nParallel Model This is the least frequent type of label transfer, so I skip it here.\nCommons of label transfer techniques  All label-based transfer techniques require a process to generate corresponding samples. Two main approaches are using machine translation and parallel corpora.   Machine translation. The biggest disadvantage is that MT systems are still not perfect and only generate very specific dialects of the output languages. This shift between the natural lan- guage and the generated language is a source of noise in CLL. parallel corpus. Parallel corpora can be used to avoid the problem with noisy translation. In parallel corpus both sides are written in natural language. We can then use existing model to annotate the LS side of the corpus and then project the labels to the other half. However, the annotations from the existing model is the source of noise as well. Usually NLP models have limited accuracy and some samples will be mislabeled. Manually labeled parallel corpora exist, but they are very rare.   Cross-lingual projection of the labels is the step when the transfer of knowledge between languages happens for label-based approaches.\n  $L_S$ model can be trained with additional data translated from $L_T$. This can improve the results during inference, since the model was already exposed to the translated data during training and does not suffer from the domain shift that much.\n  Label based transfer is notoriously noisy. It consists of several steps and each step can be a source of noise, e.g. imperfect machine translations, imperfect word alignments, imperfect pre-existing models, domain shift between parallel corpora and testing data, etc.\n  Feature Transfer This is not a frequent type of transfer, so I skip it.\nParameter transfer In parameter transfer, the behavior of the model is transfered. The transfer of knowledge happens only on shared layers. The most important technologies for parameter transfer are different language independent representations and pre-trained multilingual language models. There are three scenarios for parameter transfer.\nZero-shot transfer No $L_T$ data are used during the training. We train the model with $L_S$ only and then apply it to other languages. This is sometimes called direct transfer or model transfer. It is most often used together with language independent representations (e.g. MWE or MLMs. Contextualized word embedding from BERT is an example of MWE).\nJoint learning Both $L_S$ and $L_T$ data are used during the training, and they are used at the same time. The $L_T$ and $L_S$ models share a subset of their parameters, i.e. when one model updates its parameters, it affects the other model(s) as well. This technique of working with parameters is called parameter sharing.\u000f\nThere are two strategies of training:\n  Mixed dataset, which can be applied only when all the parameters are shared. During this strategy the training samples for all the languages are mixed into one training set. Then during the training, one batch can contain samples from multiple languages.\n  Alternate training, which can be applied even when only a subset of parameters is shared. During this strategy, the batch is sampled from one language only. Then this batch is run through the language-specific model and the parameter update is propagated to other models, that share some of the parameters.\n  Parameter sharing strategies (for each layer):\n Shared. The parameters are shared between multiple languages.\u000f Partially-private. Part of the layer isshared, while the other part is private. The most common way to implement this strategy is to have two distinct parallel layers, one with private strategy and the other with shared strategy. Then, the output of these two layers is concatenated.\u000f Private. The parameters are specific for one language.  The sharing of parameters can be realized in two ways:\n Hard sharing. With hard sharing, the shared layers are exactly the same. Soft sharing. Parameters can also be bound by a regularization term instead. E.g., add an additional term $\\left|\\theta_{S}-\\theta_{T}\\right|{2}^{2},$ where $\\theta{S}$ and $\\theta_{T}$ are the shared parameters for source model and target model, respectively.  Three different parameter sharing strategies: a) Mixed batch with samples from both languages ($X_{ST}$) is processed by the model. All theparameters are shared betweenLSandLTLoss functionJSTis used for these batches.\nb) Alternate training is used with all parameters shared, each batch is created in either $L_S$ or $L_T$. Loss function $J_{ST}$ is still the same for both cases.\nc) Alternate training with only a subset of parameters shared. In this example, the second layer is language-specific(private), while the other layers are shared. Loss function is calculated differently for each language.\nCascade learning Both $L_S$ and $L_T$ data are used during the training, but not at the same time. Instead we pre-train a model with $L_S$ data (or simply take an existing $L_S$ model) and then fine-tune it with $L_T$ data.\nPre-trained multilingual language models Pre-trained multilingual language models can be used for parameter based transfer as well.\n We can use them as a source of multilingual distributed representations and then build a model on these representations. We can use the MLM as an initialization of a multilingual model. Then we fine-tune the parameters to perform a target task.  Multilingual language models were able to achieve state-of-the-art results recently and they might become the predominant cross-lingual learning paradigm in the near future.\nRepresentation transfer With representation transfer, the knowledge about how hidden representations should look like within a model is transferred. It is a technique to extend other approaches. It is often used with deep models that use hidden representations during the calculation. This technique is often using corresponding samples or words from different languages, i.e. we usually need a parallel corpus or a bilingual dictionary.\nFuture Directions   Multilingual datasets. We consider the lack of multilingual datasets to be currently the biggest challenge for CLL. We believe that it is important to provide standardized variants of the datasets in the future for better reproducibility in these additional settings as well.\n  Standardization of linguistic resources. It is hard to compare between various resources when the corpora they are trained on might differ. It is then hard to distinguish, whether an eventual performance improvement comes from a better method or from a better corpus used for training.\n  Pre-trained multilingual language models. Training, fine-tuning and inference are all costly for MLMs. We expect to see methods that optimize the training and inference of these behemoths. As of today, the simple $L_S$ fine-tuning might lead to catastrophic forgetting, including forgetting related to the cross-lingual knowledge.\n  Truly low-resource languages and excluded languages. The CLL methods often rely on an existence of various linguistic resources, such as parallel corpora or MT systems. However,truly low-resource languages might not have these resources in this quantity and/or quality.The fact that methods are currently evaluated on resource-rich languages might then create unrealistic expectations about how well would the methods work on truly low-resource languages.\n  Curse of Multilinguality. Researchers report that adding more languages help initially, but after certain number the performance of the models actually starts to degrade. This is the curse of multilinguality, the apparent inability of models to learn too many languages. This curse is caused by a limited capacity of current parametric models. It can be addressed by increasing the capacity of the models, but the models are costly to train even today and increasing their size even further has only diminishing returns.\n  Combination with multitask learning. We believe,that a combination of cross-lingual and cross-task supervision might lead to more universal models, that are able to solve multiple tasks in multiple languages and then also generalize their broad knowledge into new tasks and new languages more easily.\n  Machine translation. One open question is how to mitigate the domain shift between natural languages and languages generated by MT systems.\n   Reference\n Cross-lingual learning for text processing: A survey. https://www.sciencedirect.com/science/article/pii/S0957417420305893?via%3Dihub  ","permalink":"https://tangliyan.com/blog/posts/corss_lingual/","summary":"Cross-lingual learning Most languages do not have training data available to create state-of-the-art models and thus our ability to create intelligent systems for these languages is limited as well.\nCross-lingual learning (CLL) is one possible remedy to solve the lack of data for low-resource languages. In essence, it is an effort to utilize annotated data from other languages when building new NLP models. When CLL is considered, target languages usually lack resources, while source languages are resource-rich and they can be used to improve the results for the former.","title":"Cross-Lingual Learning"},{"content":"BERT Recap Overview  Bert (Bidirectional Encoder Representations from Transformers) uses a \u0026ldquo;masked language model\u0026rdquo; to randomly mask some tokens from the input and predict the original vocabulary id of the masked token. Bert shows that \u0026ldquo;pre-trained representations reduce the need for many heavily-engineered task-specific architectures\u0026rdquo;.  BERT Specifics There are two steps to the BERT framework: pre-training and fine-tuning   During pre training, the model is trained on unlabeled data over different pre-training tasks.\n  Each down stream task has separate fine-tuned models after each is first initialized with pre-trained parameters.\n  Input Output Representations   In order to handle a variety of down-stream tasks, the input must be able to represent a single sentence and sentence pair in one sequence.\n  The first token of every sequence is always a classification token [CLS].\n  Sentence pairs are separated by a special token [SEP].\n  Learned embeddings are added to every token indication whether it belongs to sentence A or sentence B.\n  Tasks Masked Language Modeling (MLM)\n In order to train a deep bidirectional representation, they simply mask some percentage of the input tokens at random, and then predict those tokens. Specifically, 15% of all Word Piece tokens in each sequence are masked at random  Next sentence prediction (NSP)\n In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction. Specifically, 50% of the time, sentence B is the actual sentence that follows sentence.  results Ablation studies Effect of Pre-training Tasks   No NSP: A bidirectional model which is trained using the MLM but no next sentence prediction. This model shows to significantly hurt performance on QNLI, NMLI, and SQuAD 1.1\n  LTR and No NSP: A left-context-only model which is trained using a standard Left-to-Right LM rather than MLM. This model performs worst than the MLM model on all tasks, with large drops on MRPC and SQuAD .\n  Effect of Model Sizes   It has long been known that increasing the model size will lead to continual improvements on large-scale tasks.\n  We believe this is the first work to demonstrate convincingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks.\n  Replication study of BERT pre training that includes the specific Modifications   Training the model for longer\n  Removing the Next sentence prediction objective\n  Training on longer sequences\n  Dynamically changing the masking patterns applied to the data\n  Training Procedure Analysis   Static vs Dynamic Masking:\nThe original BERT performed masking once during data preprocessing, resulting in a single static mask.\nWe compare this strategy with dynamic masking where we generate the masking pattern every time we feed a sequence\n   Model input format and Next Sentence Prediction:\nThe NSP loss was hypothesized to be and important factor in training the original BERT model, but recent work has questioned the necessity of the NSP loss.\nThey found that using individual sentences hurts performance on downstream tasks and that removing the NSP loss matches or slightly improves downstream task performance.\n  Training with larger batches: They observe that training with large batches improves perplexity for the masked language modeling objective, as well as end-task accuracy.  RoBERTA tests and results  The aggregate of the BERT improvements are combined to form RoBERTa for Robustly optimized BERT approach. Specifically, RoBERTA is trained with dynamic masking, FULL-SENTENCES without NSP loss, large mini-batches and a larger byte-level BPE.  Results  RoBERTa achieves state of the art results on all 9 of the GLUE task development sets Crucially, RoBERTa uses the same masked language modeling pretraining-objective and architecture as BERT large, yet outperforms it.   Reference: This post is mostly written by my friend Emilio Rogalla in the NLP reading group.\n BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: https://arxiv.org/pdf/1810.04805.pdf RoBERTa: A Robustly Optimized BERT Pretraining Approach: https://arxiv.org/pdf/1907.11692.pdf  ","permalink":"https://tangliyan.com/blog/posts/bert_roberta/","summary":"BERT Recap Overview  Bert (Bidirectional Encoder Representations from Transformers) uses a \u0026ldquo;masked language model\u0026rdquo; to randomly mask some tokens from the input and predict the original vocabulary id of the masked token. Bert shows that \u0026ldquo;pre-trained representations reduce the need for many heavily-engineered task-specific architectures\u0026rdquo;.  BERT Specifics There are two steps to the BERT framework: pre-training and fine-tuning   During pre training, the model is trained on unlabeled data over different pre-training tasks.","title":"BERT and RoBERTa "},{"content":"Authors: Kevin Clark, Urvashi Khandelwal, Omer Levy, Christopher D. Manning Paper reference: https://arxiv.org/pdf/1906.04341.pdf\nBefore we start In this post, I mainly focus on the conclusions the authors reach in the paper, and I think these conclusions are worth sharing.\nIn this paper, the authors study the attention maps of a pre-trained BERT model. Their analysis focuses on the 144 attention heads in BERT.\nSurface-Level Patterns in Attention  There are heads that specialize to attending heavily on the next or previous token, especially in earlier layers of the network.   A substantial amount of BERT’s attention focuses on a few tokens. For example, over half of BERT’s attention in layers 6-10 focuses on [SEP]. One possible explanation is that [SEP] is used to aggregate segment-level information which can then be read by other heads.\nHowever, if this explanation were true, they would expect attention heads processing [SEP] to attend broadly over the whole segment to build up these representations. However, they instead almost entirely (more than 90%) attend to themselves and the other [SEP] token.\nThey speculate that attention over these special tokens might be used as a sort of “no-op” when the attention head’s function is not applicable.\n   Some attention heads, especially in lower layers, have very broad attention. The output of these heads is roughly a bag-of-vectors representation of the sentence.\n  They also measured entropies for all attention heads from only the [CLS] token. The last layer has a high entropy from [CLS], indicating very broad attention. This finding makes sense given that the representation for the [CLS] token is used as input for the \u0026ldquo;next sen- tence prediction\u0026rdquo; task during pre-training, so it attends broadly to aggregate a representation for the whole input in the last layer.\n  Probing Individual Attention Heads   There is no single attention head that does well at syntax “overall”.\n  They do find that certain attention heads specialize to specific dependency relations, sometimes achieving high accuracy.\n   Despite not being explicitly trained on these tasks, BERT’s attention heads perform remarkably well, illustrating how syntax-sensitive behavior can emerge from self-supervised training alone.\n  While the similarity between machine-learned attention weights and human-defined syntactic relations are striking, they note these are relations for which attention heads do particularly well on. They would not say individual attention heads capture dependency structure as a whole.\n  Probing Attention Head Combinations The probing classifiers are basically graph-based dependency parsers. Given an input word, the classifier produces a probability distribution over other words in the sentence indicating how likely each other word is to be the syntactic head of the current one.\n  Their results from probing both individual and combinations of attention heads suggest that BERT learns some aspects syntax purely as a by-product of self-supervised training.\n  A growing body of work indicating that indirect supervision from rich pre-training tasks like language modeling can also produce models sensitive to language’s hierarchical structure.\n  Clustering Attention Heads  Heads within the same layer are often fairly close to each other, meaning that heads within the layer have similar attention distributions. This finding is a bit surprising given that Tu et al. (2018) show that encouraging attention heads to have different behaviors can improve Transformer performance at machine translation.  Computing the distances between all pairs of attention heads. Formally, they measure the distance between two heads $\\mathrm{H}_{i}$ and $\\mathrm{H}_{j}$ as:\n$$ \\sum_{\\text {token } \\in \\text { data }} J S\\left(\\mathrm{H}{i}(\\text { token }), \\mathrm{H}{j}(\\text { token })\\right) $$\nWhere $J S$ is the Jensen-Shannon Divergence between attention distributions. Using these distances, they visualize the attention heads by applying multidimensional scaling to embed each head in two dimensions such that the Euclidean distance between embeddings reflects the Jensen-Shannon distance between the corresponding heads as closely as possible.\n Heads within the same layer are often fairly close to each other, meaning that heads within the layer have similar attention distributions. This finding is a bit surprising given that Tu et al. (2018) show that encouraging attention heads to have different behaviors can improve Transformer performance at machine translation.\n  Many attention heads can be pruned away without substantially hurting model performance. Interestingly, the important attention heads that remain after pruning tend to be ones with identified behaviors.\n   Reference:\n What Does BERT Look At? An Analysis of BERT’s Attention: https://arxiv.org/pdf/1906.04341.pdf  ","permalink":"https://tangliyan.com/blog/posts/bert_attn/","summary":"Authors: Kevin Clark, Urvashi Khandelwal, Omer Levy, Christopher D. Manning","title":"Paper Review - What Does BERT Look At? An Analysis of BERT’s Attention"},{"content":"Authors: Kenneth Marino, Ruslan Salakhutdinov, Abhinav Gupta Paper reference: https://arxiv.org/pdf/1612.04844.pdf\nOverview Note: This previous post I wrote might be helpful for reading this paper summary:\n Introduction to Graph Neural Network (GNN)  This paper investigates the use of structured prior knowledge in the form of knowledge graphs and shows that using this knowledge improves performance on image classification.\nIt introduce the Graph Search Neural Network (GSNN) as a way of efficiently incorporating large knowledge graphs into a vision classification pipeline, which outperforms standard neural network baselines for multi-label classification.\nIntuition While modern learning-based approaches can recognize some categories with high accuracy, it usually requires thousands of labeled examples for each of these categories. This approach of building large datasets for every concept is unscalable. One way to solve this problem is to use structured knowledge and reasoning (prior knowledge, this is what human usually do but current approaches do not).\nFor example, when people try to identify the animal shown in the figure, they will first recognize the animal, then recall relevant knowledge, and finally reason about it. With this information, even if we have only seen one or two pictures of this animal, we would be able to classify it. So we hope a model could also have similar reasoning process.\nPrevious Work There has been a lot of work in end-to-end learning on graphs or neural network trained on graphs. Most of these approaches either extract features from the graph or they learn a propagation model that transfers evidence between nodes conditional on the type of edge. An example of this is the Gated Graph Neural Network which takes an arbitrary graph as input. Given some initialization specific to the task, it learns how to propagate information and predict the output for every node in the graph.\nPrevious works are focusing on building and then querying knowledge bases rather than using existing knowledge bases as side information for some vision task.\nThis work not only uses attribute relationships that appear in our knowledge graphs, but also uses relationships between objects and reasons directly on graphs rather than using object-attribute pairs directly.\nMajor Contribution   The introduction of the GSNN as a way of incorporating potentially large knowledge graphs into an end-to-end learning system that is computationally feasible for large graphs;\n  Provide a framework for using noisy knowledge graphs for image classification (In vision problems, graphs encode contextual and common-sense relationships and are significantly larger and noisier);\n  The ability to explain image classifications by using the propagation model (Interpretability).\n  Graph Search Neural Network (GSNN) GSNN Explanation The idea is that rather than performing the recurrent update over all of the nodes of the graph at once, it starts with some initial nodes based on the input and only choose to expand nodes which are useful for the final output. Thus, the model only compute the update steps over a subset of the graph.\nSteps in GSNN:\n Determine Initial Nodes  They determine initial nodes in the graph based on likelihood of the visual concept being present as determined by an object detector or classifier. For their experiments, they use Faster R-CNN for each of the 80 COCO categories. For scores over some chosen threshold, they choose the corresponding nodes in the graph as our initial set of active nodes. Once they have initial nodes, they also add the nodes adjacent to the initial nodes to the active set.\n Propagation  Given the initial nodes, they want to first propagate the beliefs about the initial nodes to all of the adjacent nodes (propagation network). This process is similar to GGNN.\n Decide which nodes to expand next  After the first time step, they need a way of deciding which nodes to expand next. Therefore, a per-node scoring function is learned to estimates how \u0026ldquo;important\u0026rdquo; that node is. After each propagation step, for every node in the current graph, the model predict an importance score\n$$ i_{v}^{(t)}=g_{i}\\left(h_{v}, x_{v}\\right) $$\nwhere $g_{i}$ is a learned network, the importance network. Once we have values of $i_{v}$, we take the top $P$ scoring nodes that have never been expanded and add them to the expanded set, and add all nodes adjacent to those nodes to our active set.\nThe above two steps (Propagation, Decide which nodes to expand next) repeated $T$ times ($T$ is a hyper-parameter).\nLastly, at the final time step $T$, the model computes the per-node-output (output network) and re-order and zero-pad the outputs into the final classification net. Re-order it so that nodes always appear in the same order into the final network, and zero pad any nodes that were not expanded.\nThe entire process is shown in the figure above.\nThree networks   Propagation network: normal Graph Gated Neural Network (GGNN). GGNN is a fully end-to-end network that takes as input a directed graph and outputs either a classification over the entire graph or an output for each node. Check my previous post to know more details about GGNN and how propagation works.\n  Output network: After $T$ time steps, we have our final hidden states. The node level outputs can then just be computed as $$ o_{v}=g\\left(h_{v}^{(T)}, x_{v}\\right) $$ where $g$ is a fully connected network, the output network, and $x_{v}$ is the original annotation for the node.\n  Importance network: learn a per-node scoring function that estimates how \u0026ldquo;important\u0026rdquo; that node is. To train the importance net, they assign target importance value to each node in the graph for a given image. Nodes corresponding to ground-truth concepts in an image are assigned an importance value of 1. The neighbors of these nodes are assigned a value of $\\gamma$. Nodes which are two-hop away have value $\\gamma^2$ and so on. The idea is that nodes closest to the final output are the most important to expand. After each propagation step, for each node in the current graph, they predict an importance score $$ i_{v}^{(t)}=g_{i}\\left(h_{v}, x_{v}\\right) $$\n  Diagram visualization First $x_{init}$, the detection confidences initialize $h_{i n i t}^{(1)}$, the hidden states of the initially detected nodes (Each visual concept (e.x., person, horse, cat, etc) in the knowledge graph is represented in a hidden state). We then initialize $h_{a d j 1}^{(1)}$, the hidden states of the adjacent nodes, with $0$. We then update the hidden states using the propagation net. The values of $h^{(2)}$ are then used to predict the importance scores $i^{(1)}$ which are used to pick the next nodes to add $adj2$. These nodes are then initialized with $h_{a d j 2}^{(2)}=0$ and the hidden states are updated again through the propagation net. After $T$ steps, we then take all of the accumulated hidden states $h^{T}$ to predict the GSNN outputs for all the active nodes. During backpropagation, the binary cross entropy (BCE) loss is fed backward through the output layer, and the importance losses are fed through the importance networks to update the network parameters.\nOne final detail is the addition of a \u0026ldquo;node bias\u0026rdquo; into GSNN. In GGNN, the per-node output function $g\\left(h_{v}^{(T)}, x_{v}\\right)$, takes in the hidden state and initial annotation of the node $v$ to compute its output. Our output equations are now $g\\left(h_{v}^{(T)}, x_{v}, n_{v}\\right)$ where $n_{v}$ is a bias term that is tied to a particular node $v$ in the overall graph. This value is stored in a table and its value are updated by backpropagation.\nAdvantage   This new architecture mitigates the computational issues with the Gated Graph Neural Networks for large graphs which allows our model to be efficiently trained for image tasks using large knowledge graphs.\n  Importantly, the GSNN model is also able to provide explanations on classifications by following how the information is propagated in the graph.\n  Incorporate the graph network into an image pipeline We take the output of the graph network, re-order it so that nodes always appear in the same order into the final network, and zero pad any nodes that were not expanded. Therefore, if we have a graph with 316 node outputs, and each node predicts a 5-dim hidden variable, we create a 1580-dim feature vector from the graph. We also concatenate this feature vector with fc7 layer (4096-dim) of a fine-tuned VGG-16 network and top-score for each COCO category predicted by Faster R-CNN (80-dim). This 5756-dim feature vector is then fed into 1-layer final classification network trained with dropout.\nYou can think of this as a way of feature engineering where you concatenate the output from models with different structures.\nDataset COCO: COCO is a large-scale object detection, segmentation, and captioning dataset, which includes 80 object categories.\nVisual Genome: a dataset that represents the complex, noisy visual world with its many different kinds of objects, where labels are potentially ambiguous and overlapping, and categories fall into a long-tail distribution (skew to the right).\nVisual Genome contains over 100,000 natural images from the Internet. Each image is labeled with objects, attributes and relationships between objects entered by human annotators. They create a subset from Visual Genome which they call Visual Genome multi-label dataset or VGML (They took 316 visual concepts from the subset).\nUsing only the train split, we build a knowledge graph connecting the concepts using the most common object-attribute and object-object relationships in the dataset. Specifically, we counted how often an object/object relationship or object/attribute pair occurred in the training set, and pruned any edges that had fewer than 200 instances. This leaves us with a graph over all of the images with each edge being a common relationship.\nVisual Genome + WordNet: including the outside semantic knowledge from WordNet. The Visual Genome graphs do not contain useful semantic relationships. For instance, it might be helpful to know that dog is an animal if our visual system sees a dog and one of our labels is animal. Check the original paper to know how to add WordNet into the graph.\nConclusion In this paper, they present the Graph Search Neural Network (GSNN) as a way of efficiently using knowledge graphs as extra information to improve image classification.\nNext Step:\nThe GSNN and the framework they use for vision problems is completely general. The next steps will be to apply the GSNN to other vision tasks, such as detection, Visual Question Answering, and image captioning.\nAnother interesting direction would be to combine the procedure of this work with a system such as NEIL to create a system which builds knowledge graphs and then prunes them to get a more accurate, useful graph for image tasks.\n Reference:\n Visual Genome: https://visualgenome.org The More You Know: Using Knowledge Graphs for Image Classification: https://arxiv.org/abs/1612.04844  ","permalink":"https://tangliyan.com/blog/posts/the_more_you_know/","summary":"Authors: Kenneth Marino, Ruslan Salakhutdinov, Abhinav Gupta","title":"Paper Review - The More You Know: Using Knowledge Graphs for Image Classification"},{"content":"Fourier Transform Virtually everything in the world can be described via a waveform - a function of time, space or some other variable. For instance, sound waves, the price of a stock, etc. The Fourier Transform gives us a unique and powerful way of viewing these waveforms: All waveforms, no matter what you scribble or observe in the universe, are actually just the sum of simple sinusoids of different frequencies.\nHere is the mathematical definition of Fourier Transform $\\mathcal{F}$:\n$$ \\mathcal{F}[f(t)]=\\int f(t) e^{-i \\omega t} d t \\tag 1 $$\nFourier Transform decomposes a function defined in the space/time domain into several components in the frequency domain. In other words, the Fourier transform can change a function from the spatial domain to the frequency domain. Check the Graph Fourier Transform section for more details.\nConvolution Theorem Convolution Theorem: The convolution of two functions in real space is the same as the product of their respective Fourier transforms in Fourier space.\nEquivalent statement:\n  Convolution in time domain equals multiplication in frequency domain.\n  Multiplication in time equals convolution in the frequency domain.\n  $$ \\mathscr{F}[(f * g)(t)] = \\mathscr{F}(f(t)) \\odot \\mathscr{F}(g(t)) \\tag 2$$\nIn other words, one can calculate the convolution of two functions $f$ and $g$ by first transforming them into the frequency domain through Fourier transform, multiplying the two functions in the frequency domain, and then transforming them back through inverse Fourier transform. The mathematical expression of this idea is\n$$(f * g)(t) = \\mathscr{F}^{-1}[\\mathscr{F}(f(t)) \\odot \\mathscr{F}(g(t))] \\tag 3$$\nwhere $\\odot$ is the element-wise product. we denote the Fourier transform of a function $f$ as $\\hat{f}$.\nGraph Fourier Transform A few definitions Spectral-based methods have a solid mathematical foundation in graph signal processing. They assume graphs to be undirected.\n  Adjacency $\\operatorname{matrix} \\mathbf{A}$: The adjacency $\\operatorname{matrix} \\mathbf{A}$ is a $n \\times n$ matrix with $A_{i j}=1$ if $e_{i j} \\in E$ and $A_{i j}=0$ if $e_{i j} \\notin E$.\n  Degree $\\operatorname{matrix} \\mathbf{D}$: The degree matrix is a diagonal matrix which contains information about the degree of each vertex (the number of edges attached to each vertex). It is used together with the adjacency matrix to construct the Laplacian matrix of a graph.\n  Laplacian $\\operatorname{matrix} \\mathbf{L}$: The Laplacian matrix is a matrix representation of a graph, which is defined by $$\\mathbf{L} = \\mathbf{D} - \\mathbf{A} \\tag 4$$\n  Symmetric normalized Laplacian $\\operatorname{matrix} \\mathbf{L}^{sys}$: The normalized graph Laplacian matrix is a mathematical representation of an undirected graph $$ \\mathbf{L}^{sys} = \\mathbf{D}^{-\\frac{1}{2}} \\mathbf{L} \\mathbf{D}^{-\\frac{1}{2}} \\tag 5$$\n  The symmetric normalized graph Laplacian matrix possesses the property of being real symmetric positive semidefinite. With this property, the normalized Laplacian matrix can be factored as\n$$\\mathbf{L}^{sys}=\\mathbf{U} \\mathbf{\\Lambda} \\mathbf{U}^{T} \\tag 6$$\nwhere\n$$\\mathbf{U}=\\left[\\mathbf{u}_{\\mathbf{0}}, \\mathbf{u}_{\\mathbf{1}}, \\cdots, \\mathbf{u}_{\\mathbf{n}-1}\\right] \\in \\mathbf{R}^{n \\times n} \\tag 7$$\nis the matrix of eigenvectors ordered by eigenvalues and $\\mathbf{\\Lambda}$ is the diagonal matrix of eigenvalues (spectrum), $\\Lambda_{i i}=\\lambda_{i}$. The eigenvectors of the normalized Laplacian matrix form an orthonormal space, in mathematical words $\\mathbf{U}^{T} \\mathbf{U}=\\mathbf{I}$.\nGraph Fourier transform In graph signal processing, a graph signal $\\mathbf{x} \\in \\mathbf{R}^{n}$ is a feature vector of all nodes of a graph where $x_{i}$ is the value of the $i^{t h}$ node.\nThe graph Fourier transform to a signal $\\mathbf{\\hat{f}}$ is defined as\n$$\\mathscr{F}(\\mathbf{f})=\\mathbf{U}^{T} \\mathbf{f} \\tag 8$$\nand the inverse graph Fourier transform is defined as\n$$\\mathscr{F}^{-1}(\\mathbf{\\hat{f}})=\\mathbf{U} \\mathbf{\\hat{f}} \\tag 9$$\nwhere $\\mathbf{\\hat{f}}$ represents the resulted signal from the graph Fourier transform.\nNote:\n  $$ \\mathbf{\\hat{f}} = \\left(\\begin{array}{c} \\hat{f}\\left(\\lambda_{1}\\right) \\\\ \\hat{f}\\left(\\lambda_{2}\\right) \\\\ \\vdots \\\\ \\hat{f}\\left(\\lambda_{n}\\right) \\end{array}\\right)= \\mathbf{U}^{T} \\mathbf{f} = \\left(\\begin{array}{cccc} u_{1}(1) \u0026amp; u_{1}(2) \u0026amp; \\ldots \u0026amp; u_{1}(n) \\\\ u_{2}(1) \u0026amp; u_{2}(2) \u0026amp; \\ldots \u0026amp; u_{2}(n) \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ u_{n}(1) \u0026amp; u_{n}(2) \u0026amp; \\ldots \u0026amp; u_{n}(n) \\end{array}\\right)\\left(\\begin{array}{c} f(1) \\\\ f(2) \\\\ \\vdots \\\\ f(n) \\end{array}\\right) \\tag {10} $$ where $\\lambda_i$ are ordered eigenvalues (biggest to smallest), $N$ is the number of nodes.\n  $$\\mathbf{\\hat{f}}(\\lambda_{l})=\\sum_{i=1}^{n} \\mathbf{f}(i) u_{l}(i) \\tag {11}$$\n  $$ \\mathbf{f} = \\left(\\begin{array}{c} f(1) \\\\ f(2) \\\\ \\vdots \\\\ f(n) \\end{array}\\right)= \\mathbf{U} \\mathbf{\\hat{f}} = \\left(\\begin{array}{cccc} u_{1}(1) \u0026amp; u_{2}(1) \u0026amp; \\ldots \u0026amp; u_{n}(1) \\\\ u_{1}(2) \u0026amp; u_{2}(2) \u0026amp; \\ldots \u0026amp; u_{n}(2) \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ u_{1}(n) \u0026amp; u_{2}(n) \u0026amp; \\ldots \u0026amp; u_{n}(n) \\end{array}\\right)\\left(\\begin{array}{c} \\hat{f}\\left(\\lambda_{1}\\right) \\\\ \\hat{f}\\left(\\lambda_{2}\\right) \\\\ \\vdots \\\\ \\hat{f}\\left(\\lambda_{n}\\right) \\end{array}\\right) \\tag {12} $$\n  The graph Fourier transform projects the input graph signal to the orthonormal space where the basis is formed by independent eigenvectors ($\\mathbf{u}_{\\mathbf{0}}, \\mathbf{u}_{\\mathbf{1}}, \\cdots, \\mathbf{u}_{\\mathbf{n}-1}$) of the normalized graph Laplacian. Elements of the transformed signal $\\mathbf{\\hat{f}}$ are the coordinates of the graph signal in the new space so that the input signal can be represented as $\\mathbf{f}=\\sum_{i} \\mathbf{\\hat{f}}(\\lambda_{i}) \\mathbf{u}_{i}$ (linear combination of independent eigenvectors, coefficients are entries in vactor $\\mathbf{\\hat{f}}$), which is exactly the inverse graph Fourier transform.\nNow the graph convolution of the input signal $\\mathbf{f}$ with a filter $\\mathbf{g} \\in R^{n}$ is defined as\n$$ \\begin{aligned} \\mathbf{f} *_G \\mathbf{g} \u0026amp;=\\mathscr{F}^{-1}(\\mathscr{F}(\\mathbf{f}) \\odot \\mathscr{F}(\\mathbf{g})) \\\\ \u0026amp;=\\mathbf{U}\\left(\\mathbf{U}^{T} \\mathbf{f} \\odot \\mathbf{U}^{T} \\mathbf{g}\\right) \\\\ \u0026amp;=\\mathbf{U}\\left(\\mathbf{\\hat f} \\odot \\mathbf{\\hat g}\\right) \\end{aligned} \\tag {13} $$\nNote that (13) uses formula from (3)(8)(9).\nWe know from (10) and (11) that\n$$ \\mathbf{\\hat{f}} = \\left(\\begin{array}{c} \\hat{f}\\left(\\lambda_{1}\\right) \\\\ \\hat{f}\\left(\\lambda_{2}\\right) \\\\ \\vdots \\\\ \\hat{f}\\left(\\lambda_{n}\\right) \\end{array}\\right) = \\left(\\begin{array}{c} \\mathbf{\\hat{f}}(\\lambda_{1})=\\sum_{i=1}^{n} \\mathbf{f}(i) u_{1}(i) \\\\ \\mathbf{\\hat{f}}(\\lambda_{2})=\\sum_{i=1}^{n} \\mathbf{f}(i) u_{2}(i) \\\\ \\vdots \\\\ \\mathbf{\\hat{f}}(\\lambda_{n})=\\sum_{i=1}^{n} \\mathbf{f}(i) u_{n}(i) \\end{array}\\right) \\tag {14} $$\nTherefore, we could write the element-wise product $\\mathbf{\\hat f} \\odot \\mathbf{\\hat g}$ as\n$$ \\mathbf{\\hat f} \\odot \\mathbf{\\hat g}= \\mathbf{\\hat g} \\odot \\mathbf{\\hat f}= \\mathbf{\\hat g} \\odot \\mathbf{U}^{T} \\mathbf{f}= \\left(\\begin{array}{ccc} \\mathbf{\\hat g}\\left(\\lambda_{1}\\right) \u0026amp; \u0026amp; \\\\ \u0026amp; \\ddots \u0026amp; \\ \u0026amp; \u0026amp; \\mathbf{\\hat g} \\left(\\lambda_{n}\\right) \\end{array}\\right) \\mathbf{U}^{T} \\mathbf{f} \\tag {15} $$\nIf we denote a filter as $\\mathbf{g}_{\\theta}(\\mathbf{\\Lambda})=\\operatorname{diag}\\left(\\mathbf{U}^{T} \\mathbf{g}\\right),$ then the spectral graph convolution is simplified as\n$$ \\mathbf{f} *_G \\mathbf{g} = \\mathbf{U g}_{\\theta}(\\mathbf{\\Lambda}) \\mathbf{U}^{T} \\mathbf{f} \\tag {16} $$\nSpectral-based ConvGNNs all follow this definition. The key difference lies in the choice of the filter $\\mathrm{g}_{\\theta}$. In the rest of the post, I list two designs of filter for making a sense of what Spectral Convolutional Neural Network is.\nSpectral-based ConvGNNs Definition:\n (the number of) Input/output channels: dimensionality of node feature vectors.  Spectral Convolutional Neural Network assumes the filter $\\mathbf{g}_{\\theta}=\\Theta_{i, j}^{(k)}$ is a set of learnable parameters and considers graph signals with multiple channels. The graph convolutional layer of Spectral CNN is defined as\n$$ \\mathbf{H}_{:, j}^{(k)}=\\sigma\\left(\\sum_{i=1}^{f_{k-1}} \\mathbf{U} \\Theta_{i, j}^{(k)} \\mathbf{U}^{T} \\mathbf{H}_{:, i}^{(k-1)}\\right) \\quad\\left(j=1,2, \\cdots, f_{k}\\right) \\tag {17} $$\nwhere $k$ is the layer index, $\\mathbf{H}^{(k-1)} \\in \\mathbf{R}^{n \\times f_{k-1}}$ is the input graph signal, $\\mathbf{H}^{(0)}=\\mathbf{X}$ (node feature matrix of a graph), $f_{k-1}$ is the number of input channels and $f_{k}$ is the number of output channels, $\\Theta_{i, j}^{(k)}$ is a diagonal matrix filled with learnable parameters.\nDue to the eigen-decomposition of the Laplacian matrix, Spectral CNN faces three limitations:\n  Any perturbation to a graph results in a change of eigenbasis.\n  The learned filters are domain dependent, meaning they cannot be applied to a graph with a different structure.\n  Eigen-decomposition requires $O\\left(n^{3}\\right)$ computational complexity ($n$: the number of nodes).\n  Naive Design of $\\mathbf{g}_{\\theta}(\\mathbf{\\Lambda})$ The naive way is to set $\\mathbf{\\hat g}(\\lambda_l) = \\theta_l$, that is,\n$$ g_{\\theta}(\\mathbf{\\Lambda})=\\left(\\begin{array}{ccc} \\theta_{1} \u0026amp; \u0026amp; \\\\ \u0026amp; \\ddots \u0026amp; \\\\ \u0026amp; \u0026amp; \\theta_{n} \\end{array}\\right) \\tag {18} $$\nLimitations:\n  In each forward-propagation step, we need to calculate the product of the three matrices The amount of calculation is too large especially for relatively large graphs.\n  Convolution kernel does not have Spatial Localization.\n  The number of $\\theta_l$ depends on the total number of nodes, which is unacceptable for large graph.\n  In follow-up works, ChebNet, for example, reduces the computational complexity by making several approximations and simplifications.\nChebyshev Spectral CNN (Recursive formulation for fast filtering) Polynomial parametrization for localized filters Limitations mentioned in the last section can be overcome with the use of a polynomial filter, where\n$$\\mathbf{\\hat g}(\\lambda_l) = \\sum_{i=0}^{K} \\theta_{l} \\lambda^{l} \\tag{19}$$\nWritten in the matrix format, we have\n$$ g_{\\theta}(\\mathbf{\\Lambda})=\\left(\\begin{array}{ccc} \\sum_{i=0}^{K} \\theta_{i} \\lambda_{1}^{i} \u0026amp; \u0026amp; \\\\ \u0026amp; \\ddots \u0026amp; \\ \u0026amp; \u0026amp; \\sum_{i=0}^{K} \\theta_{i} \\lambda_{n}^{i} \\end{array}\\right)=\\sum_{i=0}^{K} \\theta_{i} \\Lambda^{i} \\tag{20} $$\nThen, replacing new defined $g_{\\theta}(\\mathbf{\\Lambda})$ in (16), we have\n$$\\mathbf{f} *_G \\mathbf{g} = U (\\sum_{i=0}^{K} \\theta_{i} \\Lambda^{i}) U^{T} \\mathbf{f} =\\sum_{i=0}^{K} \\theta_{i} (U \\Lambda^{i} U^{T}) \\mathbf{f} =\\sum_{i=0}^{K} \\theta_{i} L^{i} \\mathbf{f} \\tag {21}$$\nBy using polynomial parametrization,\n  Parameters reduce from $n$ to $K+1$.\n  We no longer have to multiply three matrices, but instead we only need to calculate powers of matrix $L$.\n  It can be shown that this filter is localized in space (Spatial Localization). In other words, spectral filters represented by $K$th-order polynomials of the Laplacian are exactly K-localized (K-localized means the model is aggregating the information from neighbors within $K$th order, see the figure above).\n  Chebyshev Spectral CNN (ChebNet) Chebyshev Spectral CNN (ChebNet) approximates the filter $g_{\\theta}(\\mathbf{\\Lambda})$ by Chebyshev polynomials of the diagonal matrix of eigenvalues, i.e,\n$$\\mathrm{g}_{\\theta}(\\mathbf{\\Lambda})=\\sum_{i=0}^{K} \\theta_{i} T_{i}(\\tilde{\\boldsymbol{\\Lambda}}) \\tag{22}$$\nwhere\n  $\\tilde{\\boldsymbol{\\Lambda}}=2 \\mathbf{\\Lambda} / \\lambda_{\\max }- \\mathbf{I}_{\\mathbf{n}}$, a diagonal matrix of scaled eigenvalues that lie in $[−1, 1]$.\n  The Chebyshev polynomials are defined recursively by $$T_{i}(\\mathbf{f})=2 \\mathbf{f} T_{i-1}(\\mathbf{f})-T_{i-2}(\\mathbf{f}) \\tag{23}$$ with $T_{0}(\\mathbf{f})=1$ and $T_{1}(\\mathbf{f})=\\mathbf{f}$.\n  As a result, the convolution of a graph signal $\\mathbf{f}$ with the defined filter $\\mathrm{g}_{\\theta}(\\mathbf{\\Lambda})$ is $$ \\mathbf{f} *_{G} \\mathbf{g}_{\\theta}=\\mathbf{U}\\left(\\sum_{i=0}^{K} \\theta_{i} T_{i}(\\tilde{\\boldsymbol{\\Lambda}})\\right) \\mathbf{U}^{T} \\mathbf{f} \\tag{24} $$\nDenote scaled Laplacian $\\tilde{\\mathbf{L}}$ as\n$$\\tilde{\\mathbf{L}}=2 \\mathbf{L} / \\lambda_{\\max }-\\mathbf{I}_{\\mathbf{n}} \\tag {25}$$\nThen by induction on $i$ and using (21), the following equation holds:\n$$T_{i}(\\tilde{\\mathbf{L}})=\\mathbf{U} T_{i}(\\tilde{\\boldsymbol{\\Lambda}}) \\mathbf{U}^{T} \\tag {26}$$\nThen finally ChebNet takes the following form:\n$$ \\mathbf{f} *_{G} \\mathbf{g}_{\\theta}=\\sum_{i=0}^{K} \\theta_{i} T_{i}(\\tilde{\\mathbf{L}}) \\mathbf{f} \\tag {27} $$\nThe benefits of ChebNet is similar to those of polynomial parametrization mentioned above.\nComparison between spectral and spatial models Spectral models have a theoretical foundation in graph signal processing. By designing new graph signal filters, one can build new ConvGNNs. However, spatial models are preferred over spectral models due to efficiency, generality, and flexibility issues.\nFirst, spectral models are less efficient than spatial models. Spectral models either need to perform eigenvector computation or handle the whole graph at the same time. Spatial models are more scalable to large graphs as they directly perform convolutions in the graph domain via information propagation. The computation can be performed in a batch of nodes instead of the whole graph.\nSecond, spectral models which rely on a graph Fourier basis generalize poorly to new graphs. They assume a fixed graph. Any perturbations to a graph would result in a change of eigenbasis. Spatial-based models, on the other hand, perform graph convolutions locally on each node where weights can be easily shared across different locations and structures.\nThird, spectral-based models are limited to operate on undirected graphs. Spatial-based models are more flexible to handle multi-source graph inputs such as edge inputs, directed graphs, signed graphs, and heterogeneous graphs, because these graph inputs can be incorporated into the aggregation function easily.\n Reference:\n Introduction to the Fourier Transform: http://www.thefouriertransform.com/#introduction Degree Matrix: https://en.wikipedia.org/wiki/Degree_matrix Introduction to Graph Signal Processing: https://link.springer.com/chapter/10.1007/978-3-030-03574-7_1 Convolution Theorem: https://www.sciencedirect.com/topics/engineering/convolution-theorem The Convolution Theorem with Application Examples: https://dspillustrations.com/pages/posts/misc/the-convolution-theorem-and-application-examples.html  Paper:\n A Comprehensive Survey on Graph Neural Networks: https://arxiv.org/pdf/1901.00596.pdf Spectral Networks and Deep Locally Connected Networks on Graphs https://arxiv.org/pdf/1312.6203.pdf Spectral Networks and Deep Locally Connected Networks on Graphs: https://arxiv.org/pdf/1312.6203.pdf Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering: https://arxiv.org/pdf/1606.09375.pdf  ","permalink":"https://tangliyan.com/blog/posts/spectral_conv/","summary":"Fourier Transform Virtually everything in the world can be described via a waveform - a function of time, space or some other variable. For instance, sound waves, the price of a stock, etc. The Fourier Transform gives us a unique and powerful way of viewing these waveforms: All waveforms, no matter what you scribble or observe in the universe, are actually just the sum of simple sinusoids of different frequencies.","title":"Graph Convolutional Neural Network -  Spectral Convolution"},{"content":"Note This is the second post of the Graph Neural Networks (GNNs) series.\nConvolutional graph neural networks (ConvGNNs) Convolutional graph neural networks (ConvGNNs) generalize the operation of convolution from grid data to graph data. The main idea is to generate a node $v$’s representation by aggregating its own features $\\mathbf{x}_{v}$ and neighbors’ features $\\mathbf{x}_{u}$, where $u \\in N(v)$. Different from RecGNNs, ConvGNNs stack fixed number of multiple graph convolutional layers with different weights to extract high-level node representations.\nConvGNNs fall into two categories:\n  spatial-based GCN: Spatial-based approaches inherit ideas from RecGNNs to define graph convolutions by information propagation.\n  spectral-based GCN: Spectral based approaches define graph convolutions by introducing filters from the perspective of graph signal processing where the graph convolutional operation is interpreted as removing noises from graph signals.\n  Spatial-based methods have developed rapidly recently due to its attractive efficiency, flexibility, and generality. In this post, we mainly focus on spatial-based GCN and leave spectral-based GCN to the next post. Let\u0026rsquo;s get started.\nGCN Framework As shown in the figure above, the input of GCN is the entire graph. In each convolution layer, a convolution operation is performed on the neighbors of each node, and the center node representation is updated with the result of the convolution. Then an activation function such as ReLU is used before going through the next layer of convolution layer. The above process continues until the number of layers reaches the expected depth (a hyper-parameter).\nGCN v.s. RecGNN The main difference between GCN and RecGNN is that each convolutional layer of GCN has unique weights, and, on the other hand, in RecGNN the weights of each layer are shared.\nWhat is Convolution In mathematics, convolution is a mathematical operation on two functions $f$ and $g$ that produces a third function $(f∗g)$ expressing how the shape of one is modified by the other.\nThe term convolution is defined as the integral of the product of the two functions after one is reversed and shifted. The mathematical definition is the following:\n$$ \\begin{array}{c} (f * g)(t)=\\int_{-\\infty}^{\\infty} f(\\tau) g(t-\\tau) \\quad (\\text {continuous}) \\\\ (f * g)(t)=\\sum_{\\tau=-\\infty}^{\\infty} f(\\tau) g(t-\\tau) \\quad(\\text {discrete}) \\end{array} $$\nThe convolution formula can be described as a weighted average of the function $f(\\tau)$ at the moment $t$ where the weighting is given by $g(-\\tau)$ simply shifted by amount $t$. As $t$ changes, the weighting function emphasizes different parts of the input function.\nAs the figure shown above, the filter is moved over by one pixel and this process is repeated until all of the possible locations in the image are filtered. At each step, the convolution takes the weighted average of pixel values of the center pixel along with its neighbors. Since the center pixel is changing at each time step, the convolution is emphasizing on different parts of the image.\nSpatial-based ConvGNNs Analogous to the convolutional operation of a conventional CNN on an image, spatial-based methods define graph convolutions based on a node’s spatial relations.\nImages can be considered as a special form of graph with each pixel representing a node. Each pixel is directly connected to its nearby pixels, as illustrated in the figure above (left). A filter is applied to a $3 \\times 3$ patch by taking the weighted average of pixel values of the central node and its neighbors across each channel.\nSimilarly, the spatial-based graph convolutions convolve the central node’s representation with its neighbors’ representations to derive the updated representation for the central node. From another perspective, spatial-based ConvGNNs share the same idea of information propagation/message passing with RecGNNs. The spatial graph convolutional operation essentially propagates node information along edges.\nMessage Passing Neural Network (MPNN) Introduction to MPNN Message Passing Neural Network (MPNN) outlines a general framework of spatial-based ConvGNNs (It is not actually a model). It treats graph convolutions as a message passing process in which information can be passed from one node to another along edges directly. MPNN runs K-step message passing iterations to let information propagate further. The message passing function (namely the spatial graph convolution) is defined as\n$$ \\mathbf{h}_{v}^{(k)}=U_{k}\\left(\\mathbf{h}_{v}^{(k-1)}, \\sum_{u \\in N(v)} M_{k}\\left(\\mathbf{h}_{v}^{(k-1)}, \\mathbf{h}_{u}^{(k-1)}, \\mathbf{x}_{(v,u)}\\right)\\right) \\tag 1 $$\nor we can write it separately as:\n$$ \\begin{array} \\mathbf{m}_v^{k} = \\sum_{u \\in N(v)} M_{k}\\left(\\mathbf{h}_{v}^{(k-1)}, \\mathbf{h}_{u}^{(k-1)}, \\mathbf{x}_{(v,u)}\\right) \\ \\mathbf{h}_{v}^{(k)}=U_{k}\\left(\\mathbf{h}_{v}^{(k-1)}, \\mathbf{m}_v^{k}\\right) \\tag 2\\ \\end{array} $$\nNote:\n $\\mathbf{m}_v^{k}$ represents messages at node $v$ at time step $k$; $\\mathbf{h}_{v}^{(0)}=\\mathbf{x}_{v}$; $U_{k}(\\cdot)$ and $M_{k}(\\cdot)$ are functions with learnable parameters. From (2), we see that MPNN has two main steps: information passing and state update. $M_{k}(\\cdot)$ is the function for information passing and $U_{k}(\\cdot)$ is the function for state update. The formula can be interpreted as each node first collects information from neighbors and then updates its own hidden state.  For example, here is an illustration of updating node $A$:\nAfter deriving the hidden representations of each node, $\\mathbf{h}_{v}^{(K)}$ can be passed to an output layer to perform node-level prediction tasks or to a readout function to perform graph-level prediction tasks. The readout function generates a representation of the entire graph based on node hidden representations (I will talk about readout function in a later post). It is generally defined as $$ \\mathbf{h}_{G}=R\\left(\\mathbf{h}_{v}^{(K)} \\mid v \\in G\\right) $$ where $R(\\cdot)$ represents the readout function with learnable parameters, and $\\mathbf{h}_{G}$ is a representation of the entire graph.\nNote that MPNN can cover many existing GNNs by assuming different forms of $U_{k}(\\cdot), M_{k}(\\cdot),$ and $R(\\cdot)$.\nShortage of the MPNN framework The shortage of MPNN is that it convolves on the entire graph, which means that all nodes must be put into the memory to perform the convolution operation. But for large-scale graph, the convolution operation on the entire graph is not realistic. In the next section, I\u0026rsquo;m going to introduce a model (GraphSAGE) to overcome this problem.\nGraphSAGE (SAmple and aggreGatE) Overview of GraphSAGE As the number of neighbors of a node can vary from one to a thousand or even more, it is inefficient to take the full size of a node\u0026rsquo;s neighborhood. GraphSage adopts sampling to obtain a fixed number of neighbors for each node. It performs graph convolutions by\n$$ \\mathbf{h}_{v}^{(k)}=\\sigma(\\mathbf{W}^{(k)} \\cdot f_{k}\\left(\\mathbf{h}_{v}^{(k-1)},\\left{\\mathbf{h}_{u}^{(k-1)}, \\forall u \\in S_{\\mathcal{N}(v)}\\right}\\right) $$\nwhere $\\mathbf{h}_{v}^{(0)}=\\mathbf{x}_{v}, f_{k}(\\cdot)$ is an aggregation function, $S_{\\mathcal{N}(v)}$ is a random sample of the node $v$ \u0026rsquo;s neighbors. The aggregation function should be invariant to the permutations of node orderings such as a mean, sum or max function.\nSpecifically, the sampling process in GraphSage is divided into three steps (corespond to the figure above):\n  First, several nodes are randomly sampled in the graph. Then for each node, they uniformly sample a fixed-size set of neighbors, instead of using full neighborhood sets. These fixed-size of neighbors are uniformly drawn from all possible neighbors, and they draw different uniform samples at each iteration. Here, neighbors are not necessarily first-order neighbors, but also second-order.\n  Update sampled node\u0026rsquo;s information by aggregating the information of neighbor nodes through some aggregate functions.\n  Calculate the loss at the sampled node. If it is an unsupervised task, we hope that the neighborhood nodes on the graph share similar node representations; if it is a supervised task, we can calculate the loss based on the task label of the specific node.\n  Aggregator Fuctions Ideally, an aggregator function would be invariant to permutations of its inputs. This property of the aggregation function ensures that a neural network model canbe trained and applied to arbitrarily ordered node neighborhood feature sets. In the paper, they examed three aggregation functions: Mean Aggregator, LSTM Aggretator, Pooling Aggretator.\nMean Aggregator: Their first examed aggregator function is the mean operator, where we simply take the elementwise mean of the vectors in $\\left{\\mathbf{h}{u}^{(k-1)}, \\forall u \\in S{\\mathcal{N}(v)}\\right}$. That is:\n$$ \\mathbf{h}_{v}^{(k)}=\\sigma(\\mathbf{W}^{(k)} \\cdot \\text{MEAN}\\left({\\mathbf{h}_{v}^{(k-1)}} \\cup \\left{\\mathbf{h}_{u}^{(k-1)}, \\forall u \\in S_{\\mathcal{N}(v)}\\right}\\right) $$\nLSTM Aggregator: Compared to the mean aggregator, LSTMs have the advantage of larger expressive capability. However, it is important to note that LSTMs are not permutation invariant, since they process their inputs in a sequential manner. They adapt LSTMs to operate on an unordered set by simply applying the LSTMs to a random permutation of the node’s neighbors.\nPooling Aggregator: each neighbor’s vector is independently fed through a fully-connected neural network; following this transformation, an elementwise max-pooling operation is applied to aggregate information across the neighbor set:\n$$ \\text {AGGREGATE}_{k}^{\\text {pool}}=\\max \\left({\\sigma\\left(\\mathbf{W}_{\\text {pool }} \\mathbf{h}_{u}^{(k)}+\\mathbf{b}\\right), \\forall u \\in S_{\\mathcal{N}(v)}}\\right) $$\nwhere $\\max$ denotes the element-wise max operator and $\\sigma$ is a nonlinear activation function.\nThe key point of the aggregstion function is that it can handle input with vaiable length.\nPATCHY-SAN Another distinct line of works achieve weight sharing across different locations by ranking a node’s neighbors based on certain criteria and associating each ranking with a learnable weight.\nOverview of PATCHY-SAN Definitions:\n  Node degree: The degree of a node is the number of edges connected to the node.\n  Graph labelings: Graph labelings are essentially node scores which can be derived by node degree, centrality, and Weisfeiler-Lehman algorithm (a procedure for partitioning the vertices of a graph. It is also known as color refinement).\n  PATCHY-SAN orders neighbors of each node according to their graph labelings and selects the top $\\omega$ neighbors. As each node now has a fixed number of ordered neighbors, graph-structured data can be converted into grid-structured data. PATCHY-SAN applies a standard 1D convolutional filter to aggregate neighborhood feature information where the order of the filter’s weights corresponds to the order of a node’s neighbors. The ranking criterion of PATCHY-SAN only considers graph structures.\nTwo problems considered in PATCHY-SAN   Given a collection of graphs, learn a function that can be used for classification and regression problems on unseen graphs.\n  Given a large graph, learn graph representations that can be used to infer unseen graph properties such as node types and missing edges.\n  Steps of PATCHY-SAN PATCHY-SAN solves the above two problems through the following three steps:\n  Node Squenece Selection: Specify an order in the graph for each node through some human-defined rules (e.x., a node with a large degree has a high score). Then take the first $\\omega$ nodes as the representative of the whole graph. If the number of nodes is smaller than $\\omega$, empty nodes should be used for padding.\n  Neighborhood graph construction: Take the nodes selected in the previous step as the center. Get their neighbors (can be first-order or second-order), which form $\\omega$ cliques. Sort the neighbors in each clique according to the node ranking obtained in the previous step, and then take the first $k$ neighbors and arrange them in order, that is, form $\\omega$ ordered cliques. If the number of nodes is smaller than $k$, empty nodes should be used for padding.\n  Graph Normalization: The normalization imposes an order on the nodes of the neighborhood graph so as to map from the unordered graph space to a vector space with a linear order. According to the graph labeling in each clique, all cliques can be converted into a fixed-length sequence $(k+1)$, including the center node. Then they can be concatenated according to the order of the center nodes to obtain a sequence with length of $\\omega \\cdot (k +1)$, which represents the entire graph. In this way, we can directly use the 1D convolution to model the sequence.\n   Reference:\n Convolution: https://en.wikipedia.org/wiki/Convolution From graph to graph convolution (2): https://www.cnblogs.com/SivilTaram/p/graph_neural_network_2.html http://www.matlog.net/icml2016_slides.pdf  Paper\n A Comprehensive Survey on Graph Neural Networks: https://arxiv.org/pdf/1901.00596.pdf Inductive Representation Learning on Large Graphs: https://arxiv.org/pdf/1706.02216.pdf Neural Message Passing for Quantum Chemistry: https://arxiv.org/pdf/1704.01212.pdf Learning Convolutional Neural Networks for Graphs: https://arxiv.org/pdf/1605.05273.pdf  ","permalink":"https://tangliyan.com/blog/posts/spatial_conv/","summary":"Note This is the second post of the Graph Neural Networks (GNNs) series.\nConvolutional graph neural networks (ConvGNNs) Convolutional graph neural networks (ConvGNNs) generalize the operation of convolution from grid data to graph data. The main idea is to generate a node $v$’s representation by aggregating its own features $\\mathbf{x}_{v}$ and neighbors’ features $\\mathbf{x}_{u}$, where $u \\in N(v)$. Different from RecGNNs, ConvGNNs stack fixed number of multiple graph convolutional layers with different weights to extract high-level node representations.","title":"Graph Convolutional Neural Network - Spatial Convolution"},{"content":"Note This is the first post of the Graph Neural Networks (GNNs) series.\nBackground and Intuition There is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. For examples, in e-commence, a graph-based learning system can exploit the interactions between users and products to make highly accurate recommendations. In chemistry, molecules are modeled as graphs.\nAnother example is the image data. We can represent an image as a regular grid in the Euclidean space. A convolutional neural network (CNN) is able to exploit the shift-invariance, local connectivity, and compositionality of image data. As a result, CNNs can extract local meaningful features that are shared with the entire data sets for various image analysis.\nAs graphs can be irregular, a graph may have a variable size of unordered nodes, and nodes from a graph may have a different number of neighbors, resulting in some important operations (e.g., convolutions) being easy to compute in the image domain, but difficult to apply to the graph domain.\nFurthermore, a core assumption of existing machine learning algorithms is that instances are independent of each other. This assumption no longer holds for graph data because each node is related to others by links of various types, such as citations, friendships, and interactions.\nThe complexity of graph data has imposed significant challenges on existing machine learning algorithms, and nowadays many studies on extending deep learning approaches for graph data have emerged.\nIntro to Graph Neural Networks Graph neural networks (GNNs) are categorized into four groups:\n Recurrent graph neural networks (RecGNNs) Convolutional graph neural networks (ConvGNNs) Graph autoencoders (GAEs) Spatial-temporal graph neural networks (STGNNs).  These early studies fall into the category of recurrent graph neural networks (RecGNNs). They learn a target node’s representation by propagating neighbor information in an iterative manner until a stable fixed point is reached. This process is computationally expensive, and recently there have been increasing efforts to overcome these challenges.\nEncouraged by the success of CNNs in the computer vision domain, a large number of methods that re-define the notion of convolution for graph data are developed in parallel. These approaches are under the umbrella of convolutional graph neural networks (ConvGNNs). ConvGNNs are divided into two main streams, the spectral-based approaches and the spatial-based approaches.\nGNNs Framework With the graph structure and node content information as inputs, the outputs of GNNs can focus on different graph analytics tasks with one of the following mechanisms:\n  Node-level outputs relate to node regression and node classification tasks. RecGNNs and ConvGNNs can extract high-level node representations by information propagation/graph convolution. With a multi-perceptron or a softmax layer as the output layer, GNNs are able to perform node-level tasks in an end-to-end manner.\n  Edge-level outputs relate to the edge classification and link prediction tasks. With two nodes’ hidden representations from GNNs as inputs, a similarity function or a neural network can be utilized to predict the label/connection strength of an edge.\n  Graph-level outputs relate to the graph classification task. To obtain a compact representation on the graph level, GNNs are often combined with pooling and readout operations.\n  Definition A graph is represented as $G=(V, E)$ where\n $V$ is the set of nodes/ vertices, $v_{i} \\in$ $V$ denotes a node in $V$; $E$ is the set of edges; The neighborhood of a node $v$ is defined as $N(v)={u \\in V \\mid(v, u) \\in E}$.  We are going to use the following notations in the post:\n $X \\in \\mathbf{R}^{n \\times d}$: The feature matrix of the graph, which is a concatenation of all node representations of the graph. $n$ is the number of nodes and $d$ is the dimension of a node feature vector. $\\mathbf{x}_{v}$: feature at node $v$. $\\mathbf{x}_{(v, u)}$: feature at the edge between node the $v$ and the node $u$. $\\mathbf{H} \\in \\mathbf{R}^{n \\times b}$: The node hidden feature matrix, where $b$ is the dimension of a hidden node feature vector. $\\mathbf{h}_{v} \\in \\mathbf{R}^b$: hidden state embedding for the node $v$.  Recurrent graph neural networks (RecGNNs) Introduction to RecGNNs Recurrent graph neural networks (RecGNNs) mostly are pioneer works of graph neural networks which are based on the fixed point theorem. The primary goal of RecGNNs is to learn an embedding for each node (node representation). More specifically, given a graph G, each node $x_v$ and edge $x_{(v, u)}$ connecting two nodes $x_v, x_u$ has its own feature, and the goal is to learn these features.\nRecGNNs aim to learn these node representations with recurrent neural architectures. They apply the same set of parameters recurrently over nodes in a graph to extract high-level node representations. They assume a node in a graph constantly exchanges information/ message with its neighbors until a stable equilibrium is reached. RecGNNs are conceptually important and inspired later research on convolutional graph neural networks.\nBased on an information diffusion mechanism, RecGNNs updates nodes' states by exchanging neighborhood information recurrently until a stable equilibrium is reached. A node\u0026rsquo;s hidden state is recurrently updated by\n$$ \\mathbf{h}_{v}^{(t)}=\\sum_{u \\in N(v)} f\\left(\\mathbf{x}_{v}, \\mathbf{x}_{(v, u)}, \\mathbf{x}_{u}, \\mathbf{h}_{u}^{(t-1)}\\right) \\tag 1 $$\nwhere\n $f(\\cdot)$ is a parametric function (also called local transaction function) which could be approximated by a neural network $\\mathbf{h}_{v}^{(0)}$ is initialized randomly The sum operation enables RecGNNs to be applicable to all nodes, even if the number of neighbors differs.  To ensure convergence, the recurrent function $f(\\cdot)$ must be a contraction mapping, which shrinks the distance between two points after projecting them into a latent space. Notice that when a convergence criterion is satisfied, the last step node hidden states are the final node embeddings/ representations. Then a local output function $g$ is added for the following downstream tasks. For example, we use\n$$ \\mathbf{o}_{v} = g\\left(\\mathbf{h}_{v}, \\mathbf{x}_{v}\\right) \\tag 2$$\nto describe how the output is produced. The pipeline could be visualized as the following figure:\nNote how the connections (green lines) match the sample graph on the left. From (1), the input of the first cell is $\\mathbf{x_1}, \\mathbf{x_{(1,3)}}, \\mathbf{x_3}, \\mathbf{h_3}$. After convergence, a local output function $g$ is added to produce output.\nBanach\u0026rsquo;s Fixed Point Theorem Let $F$ be a function obtained by stacking several $f$ together (aka global transition function). The fixed point theorem suggests that if $F$ is a contraction mapping, then $H$ will always converge to a fixed point regardless of what $H^0$ is.\nIn other words, after contraction mapping $F$, the image after mapping must be smaller than the original space. Imagine that this compression process continues, eventually all points in the original space will be mapped to one point.\n contraction mapping: a function $F$ from $M$ to itself, with the property that there is some nonnegative real number $0 \\leq k\u0026lt;1$ such that for all $x$ and $y$ in $M$,  $$ d(F(x), F(y)) \\leq k d(x, y) \\tag 3 $$\nLet F be a neural netwrok, to make sure F is a contraction mapping, a penalty term has to be imposed on the Jacobian matrix of parameters. To illustrate the point, let x, y be two vectors in an one dimensional space, then from the definition of the contraction mapping, we have\n$$ \\begin{array}{c} |F(x)-F(y)| \\leq c|x-y|, \\ 0 \\leq c\u0026lt;1 \\\\ \\frac{|F(x)-F(y)|}{|x-y|} \\leq c \\\\ \\frac{|F(x)-F(x-\\Delta x)|}{|\\Delta x|} \\leq c \\\\ \\left|F^{\\prime}(x)\\right|=\\left|\\frac{\\partial F(x)}{\\partial x}\\right| \\leq c \\end{array} $$\nNotice that to be a contraction mapping, the gradient has to be less than $1$. We use Lagrange Multiplier to convert this constrained problem to an un-constrained problem and the training objective can be written as\n$$ J=\\operatorname{Loss}+\\lambda \\cdot \\max \\left(\\frac{|\\partial F|}{|\\partial \\mathbf{h}|}-c, 0\\right), c \\in(0,1) \\tag 4 $$\nwith hyper-parameter $\\lambda$. The penalty term is a hinge loss.\nAfter setting up labels for nodes' outputs, we could then update parameters by calculating the gradient of the back-propagation based on the loss from the forward-propagation.\nRecGNNs v.s. RNNs For simplicity, assuming that there are three nodes $\\mathbf{x_1}, \\mathbf{x_2}, \\mathbf{x_3}$ in GNN. Correspondingly, there is a sequence $(\\mathbf{x_1}, \\mathbf{x_2}, \\mathbf{x_3})$ in RNN. The followings are the differences:\n  RecGNNs are based on the fixed point theorem, which means that the length of RecGNNs unfolded along time is dynamic and determined according to the convergence conditions, while the length of RNNs unfolded along time is equal to the length of the sequence itself.\n  The input of each time step of RecGNNs is features of all nodes and edges, and the input of each time step of RNNs is the corresponding input at that moment. At the same time, the information flow between time steps is not the same, the former is determined by the edge, and the latter is determined by the order of the input sequence.\n  The goal of RecGNNs is to obtain a stable hidden state for each node, so it has outputs only after the hidden state converges; while RNN can output at each time step.\n  RecGNNs uses Almeida-Pineda (AP) algorithm to optimize back-propagation, while RNNs uses Back Propogation Through Time (BPTT). The former has requirements for convergence, while the latter does not.\n  Limitation of RecGNNs   There is limited influence of edges on the hidden state of nodes, and RecGNNs do not have learnable parameters for edges, which means that certain features of edges cannot be learned by the model.\n  Fixed point theorem will lead to too much information sharing between the hidden states of nodes, resulting in the over-smoothing problem, and node representations become less informative (all look similiar). Here is an illustration of over-smoothing problem:\n  Note: The purpose of using the fix point theorem is to make sure RecGNNs is stable. However, if the model is too deep, all hidden states of nodes will be similar (over-smoothing as stated above).\nGated Graph Neural Networks (GGNNs) Introduction to GGNNs Gated Graph Neural Network (GGNN) employs a gated recurrent unit (GRU) as a recurrent function, reducing the recurrence to a fixed number of steps. The advantage is that it no longer needs to constrain parameters to ensure convergence. A node hidden state is updated by its previous hidden states and its neighboring hidden states, defined as\n$$ \\mathbf{h}_{v}^{(t)}=G R U\\left(\\mathbf{h}_{v}^{(t-1)}, \\sum_{u \\in N(v)} \\mathbf{W}_{edge} \\mathbf{h}_{u}^{(t-1)}\\right) $$\nwhere $\\mathbf{h}^{(0)}_v =\\mathbf{x}_v$ and $\\mathbf{W}_{edge}$ consists of learnable parameters for edges. Notice that different from RecGNNs, node features $\\mathbf{x}_v$ do not appear in the formula of GGNN, but instead used as initialization of hidden states of nodes.\nGGNNs v.s. RecRNNs The major difference between GGNNs and RecRNNs is that GGNNs are longer based on the fixed point theorem, which means that the convergence is no longer required.\nOther than that, GGNNs use the BPTT algorithm to learn the model parameters. This can be problematic for large graphs, as GGNNs need to run the recurrent function multiple times over all nodes, requiring the intermediate states of all nodes to be stored in memory.\n Reference:\n https://en.wikipedia.org/wiki/Contraction_mapping Big thanks to this blog: From graph to graph convolution (1): https://www.cnblogs.com/SivilTaram/p/graph_neural_network_1.html https://en.wikipedia.org/wiki/File:Graph_Laplacian_Diffusion_Example.gif  Paper:\n A Comprehensive Survey on Graph Neural Networks: https://arxiv.org/pdf/1901.00596.pdf The graph neural network model: https://persagen.com/files/misc/scarselli2009graph.pdf Gated Graph Sequence Neural Networks: https://arxiv.org/pdf/1511.05493.pdf  ","permalink":"https://tangliyan.com/blog/posts/gnn/","summary":"Note This is the first post of the Graph Neural Networks (GNNs) series.\nBackground and Intuition There is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. For examples, in e-commence, a graph-based learning system can exploit the interactions between users and products to make highly accurate recommendations. In chemistry, molecules are modeled as graphs.","title":"Introduction to Graph Neural Network (GNN)"},{"content":"Before we start Two of my previous post might be helpful in getting a general understanding of the top solutions of this competition. Please feel free to check them out.\n Knowledge Distillation clearly explained Common Multilingual Language Modeling methods (M-Bert, LASER, MultiFiT, XLM)  Jigsaw Multilingual Toxic Comment Classification  Use TPUs to identify toxicity comments across multiple languages.\n Overview of the competition Jigsaw Multilingual Toxic Comment Classification is the 3rd annual competition organized by the Jigsaw team. It follows Toxic Comment Classification Challenge, the original 2018 competition, and Jigsaw Unintended Bias in Toxicity Classification, which required the competitors to consider biased ML predictions in their new models. This year, the goal is to use English only training data to run toxicity predictions on foreign languages (tr, ru, it, fr, pt, es).\nKagglers are predicting the probability that a comment is toxic. A toxic comment would receive a 1.0. A benign, non-toxic comment would receive a 0.0. In the test set, all comments are classified as either a 1.0 or a 0.0. The whole test set was visible in this competition.\nData  jigsaw-toxic-comment-train.csv: from Jigsaw Toxic Comment Classification Challenge (2018).      0.0 1.0 total     count 202165 21384 223549     jigsaw-unintended-bias-train.csv: from Jigsaw Unintended Bias in Toxicity Classification (2019)      0.0 1.0 total     count 1789968 112226 1902194     validation.csv: comments from Wikipedia talk pages in different non-English languages test.csv: comments from Wikipedia talk pages in different non-English languages  Here is the the value counts in valid data:\n    0.0 1.0 total     es 2078 422 2500   it 2012 488 2500   tr 2680 320 3000    Here is the the value counts in test data: As you can see, the test set comments contains 6 non-English languages (tr, ru, it, fr, pt, es) and the validation set contains only three non-English comments (es, it, tr).\n1st place solution Ensembling to mitigate Transformer training variability Since the performance of Transformer models is impacted heavily by initialization and data order, they went with an iterative blending approach, refining the test set predictions across submissions with a weighted average of the previous best submission and the current model’s predictions. They began with a simple average, and gradually increased the weight of the previous best submission.\nNote: The predictions are an exponential moving average of all past model predictions and the current model\u0026rsquo;s prediction.\nPseudo-Labeling They observed a performance improvement when they used test-set predictions as training data - the intuition being that it helps models learn the test set distribution. Using all test-set predictions as soft-labels worked better than any other version of pseudo-labelling (e.g., hard labels, confidence thresholded PLs, etc). Towards the end of the competition, we discovered a minor but material boost in LB when we upsampled the PLs.\nMultilingual XLM-Roberta models As with most teams, they began with a vanilla XLM-Roberta model, incorporating translations of the 2018 dataset in the 6 test-set languages as training data. They used a vanilla classification head on the [CLS] token of the last layer with the Adam optimizer and binary cross entropy loss function, and finetuned the entire model with a low learning rate. Given Transformer models having several hundred million trainable weights put to the relatively simple task of making a binary prediction, they didn\u0026rsquo;t spend too much time on hyper-parameter optimization, architectural tweaks, or pre-processing.\nTrain foreign language monolingual Transformer Inspired by the MultiFiT paper, they observed a dramatic performance boost when they used pretrained foreign language monolingual Transformer models from HuggingFace for the test-set languages(e.g., Camembert for french samples, Rubert for russian, BerTurk for turkish, BETO for spanish, etc).\nThey finetuned models for each of the 6 languages:\n Combining translations of the 2018 Toxic Comments together with pseudo-labels for samples from the test set and hard labels for samples from the val set in that specific language. These pseudo-labels initially come from the XLM-R multilingual models, then they are continuously refined by these monolingual transformer models.  Note: A training run would for example have 10K test set samples + 70K subsampled 2018 translations samples + 2.5K val samples. These models are quite good at few-shot learning so \u0026lt;100K is sufficient to learn.\n Training the corresponding monolingual model, predicting the same samples then blending it back with the “main branch” of all predictions. It was synergistic in that training cross-lingual model -\u0026gt; monolingual models -\u0026gt; cross-lingual model, etc. lead to continual performance improvements.  For each model run, they would reload weight initalizations from the pretrained models to prevent overfitting. In other words, the continuing improvements of the predictions were being driven by refinements in the pseudo-labels we were providing to the models as training data.\nFor a given monolingual model, predicting only test-set samples in that language worked best. Translating test-set samples in other languages to the model\u0026rsquo;s language and predicting them worsened performance.\nFinetuning pre-trained foreign language monolingual FastText models After they exhausted the HuggingFace monolingual model library, they trained monolingual FastText Bidirectional GRU models on 2018 Toxic Comments, using pretrained embeddings for the test-set languages, to continue refining the test set predictions (albeit with a lower weight when combined with the main branch of predictions) and saw a small but meaningful performance boost.\nPost-processing Intuition:\nThey consider the trend of subsequent submissions of a specific language (e.g. Russian) for each example in the test dataset. If the trend of that example is positive, they nudge the example further in the positive direction and vice versa.\nThey measure the trend by taking the differences of all subsequent submissions for the specific language and averaging those differences. The nudge that they give to the new submission is based on a predefined weight, typically a weight of 1 or 1.5.\nPseudo-code Given the following:\nweight = predefined weight (typically 1 or 1.5) pred_best = current best predictions on LB diff_avg = average of differences of consecutive subs (trend) Then for each example in test of the specific language (e.g. Turkish):\nif diff_avg \u0026lt; 0: # negative trend pred_new = (1+weight*diff_avg)*pred_best # nudge downwards else: # positive trend pred_new = (1-weight*diff_avg)*pred_best + diff_avg # nudge upwards 4th place solution special Speeding up training   One way is to downsample negative samples to get a more balanced dataset and a smaller dataset at the same time. If you only use as many negative as positive then dataset is reduced 5x roughly, same for time to run one epoch.\n  Another speedup comes from padding by batch. The main idea is to limit the amount of padding to what is necessary for the current batch. Inputs are padded to the length of the longest input in the batch rather than a fixed length. It has been used in previous competitions and it accelerates training significantly. They refined the idea by sorting the samples by their length so that the samples in a given batch have similar length. This reduces even further the need for padding. If all inputs in the batch have the same length then there is no padding at all. Given samples are sorted, they cannot be shuffled in the training mode. They rather shuffled batches. This yields an extra 2x speedup compared to the original padding by batch.\n  To learn how to implement padding by batch, read my previous summary on Tweeter Sentiment Extraction section sequence bucketing (dynamic padding).\nTraining strategy One main ingredient for their good result is a stepwise finetuning towards single language models for tr, it and es. Below is the illustration of their model architecture:\nAs input data, they used the english 2018 Toxic Comments and its six translations available as public datasets which combined are roughly 1.4M comments. Training a model directly on the combined dataset is not optimal, as each comment appears 7x in each epoch (although in different languages) and the model overfits on that. So they divided the combined dataset into 7 stratified parts, where each part contains a comment only once. For each fold they then fine-tuned a transformer in a 3 step manner:\n Step 1: finetune to all 7 languages for 2 epochs Step 2: finetune only to the full valid.csv which only has tr, it and es Step 3: 3x finetune to each language in valid resulting in 3 models  We then use the step1 model for predicting ru, the step2 model for predicting pt and fr and the respective step3 models for tr, it and es. Using the step2 model for pt and fr gave a significant boost compared to using step1 model for those due. Most likely due to the language similarity between it, es, fr and pt.\nPost-processing The competition metric is sensitive to the global rank of all predictions and not language specific. So they took care of the languages having a correct relation to each other, by shifting the predictions of each language individually.\ntest_df.loc[test_df[\u0026#34;lang\u0026#34;] == \u0026#34;es\u0026#34;, \u0026#34;toxic\u0026#34;] *= 1.06 test_df.loc[test_df[\u0026#34;lang\u0026#34;] == \u0026#34;fr\u0026#34;, \u0026#34;toxic\u0026#34;] *= 1.04 test_df.loc[test_df[\u0026#34;lang\u0026#34;] == \u0026#34;it\u0026#34;, \u0026#34;toxic\u0026#34;] *= 0.97 test_df.loc[test_df[\u0026#34;lang\u0026#34;] == \u0026#34;pt\u0026#34;, \u0026#34;toxic\u0026#34;] *= 0.96 test_df.loc[test_df[\u0026#34;lang\u0026#34;] == \u0026#34;tr\u0026#34;, \u0026#34;toxic\u0026#34;] *= 0.98 I tried out this post-processing and it boosted my rank from 64th to top 30.\nCommons in top solutions I read through top 10 solutions and indeed they are roughtly the same. Unlike in Tweet Sentiment Extraction where every top solution has its own novel and fancy model structures, top solutions in this competition mostly share the following ideas:\n  Pseudo-Labeling (using soft labels).\n  Use monolingual Transformer models.\n  Two stage training.\n  Post-processing (each team have has different pp strategy).\n  Two stage training This is the last common ideas that I haven\u0026rsquo;t explained. The main idea is the following.\n  One stage training means that we always train the same data in training. This policy always get the lower scores.\n  Two stage training means that we will have two different data for training. e.g., firstly train on Toxic Comment Classification dataset (+ Unintended Bias in Toxicity dataset). After that train on 8k validation data. This policy always get the higher scores.\n  A checklist of contemporary NLP classification techniques 1st place author shared a checklist of contemporary NLP classification techniques. The followings are a selection of what didn\u0026rsquo;t work for them in this competition, but I still believe these ideas are worth trying in any competition or project.\n  Document-level embedders (e.g., LASER) Further MLM pretraining of Transformer models using task data Alternative ensembling mechanisms (rank-averaging, stochastic weight averaging) Alternative loss functions (e.g., focal loss, histogram loss) Alternative pooling mechanisms for the classification head (e.g., max-pool CNN across tokens, using multiple hidden layers etc.) Non-FastText pretrained embeddings (e.g., Flair, glove, bpem) Freeze-finetuning for the Transformer models Regularization (e.g., multisample dropout, input mixup, manifold mixup, sentencepiece-dropout) Backtranslation as data augmentation English translations as train/test-time augmentation Self-distilling to relabel the 2018 data Adversarial training by perturbing the embeddings layer using FGM Multi-task learning Temperature scaling on pseudo-labels Semi-supervised learning using the test data Composing two models into an encoder-decoder model Use of translation focused pretrained models (e.g., mBart)    Reference:\n 1st place solution: https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification/discussion/160862, https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification/discussion/160986 4th place solution: https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification/discussion/160980 6th place solution: https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification/discussion/161095 11st place solution: https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification/discussion/160961 Jigsaw Multilingual: Quick EDA \u0026amp; TPU Modeling: https://www.kaggle.com/ipythonx/jigsaw-multilingual-quick-eda-tpu-modeling Jigsaw Train Multilingual Coments (Google API) For multilingual models training: https://www.kaggle.com/miklgr500/jigsaw-train-multilingual-coments-google-api  The followings are monolingual language models in huggingface communities mentioned by the 6th place solution:\n es:  https://huggingface.co/dccuchile/bert-base-spanish-wwm-uncased# https://huggingface.co/dccuchile/bert-base-spanish-wwm-cased# https://github.com/dccuchile/beto   fr:  https://huggingface.co/camembert/camembert-large https://huggingface.co/camembert-base https://huggingface.co/flaubert/flaubert-large-cased https://huggingface.co/flaubert/flaubert-base-uncased https://huggingface.co/flaubert/flaubert-base-cased   tr:  (128k vocabulary) https://huggingface.co/dbmdz/bert-base-turkish-128k-uncased (128k vocabulary) https://huggingface.co/dbmdz/bert-base-turkish-128k-cased# (32k vocabulary) https://huggingface.co/dbmdz/bert-base-turkish-uncased# (32k vocabulary) https://huggingface.co/dbmdz/bert-base-turkish-cased#   it  https://huggingface.co/dbmdz/bert-base-italian-xxl-uncased# https://huggingface.co/dbmdz/bert-base-italian-xxl-cased# https://huggingface.co/dbmdz/bert-base-italian-uncased# https://huggingface.co/dbmdz/bert-base-italian-cased#   pt  https://huggingface.co/neuralmind/bert-large-portuguese-cased# https://huggingface.co/neuralmind/bert-base-portuguese-cased#   ru  https://huggingface.co/DeepPavlov/bert-base-bg-cs-pl-ru-cased https://huggingface.co/DeepPavlov/rubert-base-cased#    ","permalink":"https://tangliyan.com/blog/posts/kaggle_jigsaw/","summary":"Before we start Two of my previous post might be helpful in getting a general understanding of the top solutions of this competition. Please feel free to check them out.\n Knowledge Distillation clearly explained Common Multilingual Language Modeling methods (M-Bert, LASER, MultiFiT, XLM)  Jigsaw Multilingual Toxic Comment Classification  Use TPUs to identify toxicity comments across multiple languages.\n Overview of the competition Jigsaw Multilingual Toxic Comment Classification is the 3rd annual competition organized by the Jigsaw team.","title":"Kaggle: Jigsaw Multilingual Toxic Comment Classification - top solutions"},{"content":"Multilingual Models are a type of Machine Learning model that can understand different languages. In this post, I\u0026rsquo;m going to discuss four common multi-lingual language models Multilingual-Bert (M-Bert), Language-Agnostic SEntence Representations (LASER Embeddings), Efficient multi-lingual language model fine-tuning (MultiFiT) and Cross-lingual Language Model (XLM).\nWays of tokenization Word-based tokenization Word-based tokenization works well for the morphologically poor English, but results in very large and sparse vocabularies for morphologically rich languages, such as Polish and Turkish. Some languages such as Chinese don’t really even have the concept of a “word”, so require heuristic segmentation approaches, which tend to be complicated, slow, and inaccurate.\nCharacter-based tokenization Character-based models use individual characters as tokens. While in this case the vocabulary (and thus the number of parameters) can be small, such models require modelling longer-term dependencies and can thus be harder to train and less expressive than word-based models.\nSubword tokenization Subword tokenization strikes a balance between the two approaches above by using a mixture of character, subword and word tokens, depending on how common they are.\nsubword tokenization has two very desirable properties for multilingual language modelling:\n  Subwords more easily represent inflections (the change in the form of a word), including common prefixes and suffixes and are thus well-suited for morphologically rich languages.\n  Subword tokenization is a good fit for open-vocabulary problems and eliminates out-of-vocabulary tokens, as the coverage is close to 100% tokens.\n  Existing approaches for cross-lingual NLP   Parallel data across languages — that is, a corpus of documents with exactly the same contents, but written in different languages. This is very hard to acquire in a general setting.\n  A shared vocabulary — that is, a vocabulary that is common across multiple languages. This approach over-represents languages with a lot of data (e.g., Multi-lingual BERT, which I\u0026rsquo;ll discuss in this post).\n  Out-of-vocabulary (OOV) problem in mono/multi-lingual settings It has been shown that the performance on many NLP tasks drops dramatically on held-out data when a significant percentage of words do not appear in the training data, i.e., out-of-vocabulary (OOV) words. OOV problems have been addressed in previous works under monolingual settings, through replacing OOV words with their semantically similar in-vocabulary words or using character/word information or subword information like byte pair encoding (BPE).\nAll those monolingual pre-trained models (e.x. BERT, GPT) rely on language modeling, where a common trick is to tie the weights of softmax and word embeddings. However, in multilingual setting, due to the expensive computation of softmax and data imbalance across different languages, the vocabulary size for each language in a multilingual model is relatively small compared to the monolingual BERT models, especially for low-resource languages. Even for a high-resource language like Chinese, its vocabulary size 10k in the multilingual BERT is only half the size of that in the Chinese BERT. Just as in monolingual settings, the OOV problem also hinders the performance of a multilingual model on tasks that are sensitive to token-level or sentence-level information.\nM-BERT (Multi-lingual BERT) Multilingual BERT is pre-trained in the same way as monolingual BERT, but instead of being trained only on monolingual English data with an English-derived vocabulary, it is trained on the Wikipedia pages of 104 languages with a shared word piece vocabulary. The vocabulary is 119,547 WordPiece model, and the input is tokenized into word-pieces (also known as subwords) so that each word piece is an element of the dictionary. Non-word-initial units are prefixed with ## as a continuation symbol except for Chinese characters which are surrounded by spaces before any tokenization takes place.\nTo account for the differences in the size of Wikipedia, some languages are sub-sampled, and some are super-sampled using exponential smoothing (assigns exponentially decreasing weights as the observation get older).\nIt does not use any marker denoting the input language, and does not have any explicit mechanism to encourage translation equivalent pairs to have similar representations.\nWHY MULTILINGUAL BERT WORKS Definitions:\n  Word-piece overlap: the texts from different languages share some common word-piece vocabulary (like numbers, links, etc.. including actual words, if they have the same script).\n  structural similarity: They define the structure of a language as every property of an individual language that is invariant to the script of the language (e.g., morphology, word-ordering, word frequency are all parts of structure of a language).\n  In the paper Cross-lingual ability of multilingual bert, the authors provide a comprehensive study of the contribution of different components in M-BERT to its cross-lingual ability.\nThe most notable finding is that word-piece overlap on the one hand, and multi-head attention on the other, are both not significant, whereas structural similarity and the depth of the model are crucial for its cross-lingual ability.\nNote:\n  Previous work hypothesizes that M-BERT generalizes across languages because these shared word-pieces force the other word-pieces to be mapped to the same shared space. The paper shows that the contribution of word-piece overlap is very small, which is quite surprising and contradictory to prior hypotheses.\n  The authors' experiment results shows that the number of attention heads doesn’t have a significant effect on cross-lingual ability. B-BERT (bilingual-bert) is satisfactorily cross-lingual even with a single attention head, which is in agreement with the recent study on monolingual BERT.\n  A significant shortcoming of M-BERT The author observe a drastic drop in the entailment performance (NLI task) of B-BERT when the premise and hypothesis are in different languages. One of the possible explanations could be that BERT is learning to make textual entailment decisions by matching words or phrases in the premise to those in the hypothesis. In the following LASER model, it instead supports any combination of premises and hypotheses in different languages in the NLI task.\nLASER (Language-Agnostic SEntence Representations) Facebook open-sourced LASER (Language-Agnostic SEntence Representations) toolkit in Jan 2019. It is the first successful exploration of massively multilingual sentence representations to be shared publicly with the NLP community.\nThe toolkit now works with more than 90 languages and LASER achieves these results by embedding all languages jointly in a single shared space (rather than having a separate model for each).\nLASER also offers several additional benefits:\n  It delivers extremely fast performance, processing up to 2,000 sentences per second on GPU.\n  The sentence encoder is implemented in PyTorch with minimal external dependencies.\n  Languages with limited resources can benefit from joint training over many languages.\n  The model supports the use of multiple languages in one sentence.\n  Performance improves as new languages are added, as the system learns to recognize characteristics of language families.\n  Universal, language-agnostic sentence embeddings LASER’s vector representations maps a sentence in any language to a point in a high dimensional space with the goal that the same statement in any language will end up in the same neighborhood. This representation could be seen as a universal language in a semantic vector space. We have observed that the distance in that space correlates very well to the semantic closeness of the sentences.\nIn the following figure, the image on the left shows a monolingual embedding space. The one on the right illustrates LASER’s approach, which embeds all languages in a single, shared space.\nTheir approach builds on the same underlying technology as neural machine translation: an encoder/decoder approach. they use one shared encoder for all input languages and a shared decoder to generate the output language. The encoder is a five-layer bidirectional LSTM network. In contrast with neural machine translation, we do not use an attention mechanism but instead have a 1,024-dimension fixed-size vector to represent the input sentence. It is obtained by max-pooling over the last states of the BiLSTM. This enables us to compare sentence representations and feed them directly into a classifier.\nFor the detailed results, check out the original paper.\nZero-shot, cross-lingual natural language inference  natural language inference (NLI): the task of determining whether a “hypothesis” is true (entailment), false (contradiction), or undetermined (neutral) given a “premise”.  The table table shows LASER’s zero-shot transfer performance on the XNLI corpus.\nTheir proposed model achieves excellent results in cross-lingual natural language inference (NLI). Performance on this task is a strong indicator of how well the model represents the meaning of a sentence. They consider the zero-shot setting; in other words, they train the NLI classifier on English and then apply it to all target languages with no fine tuning or target-language resources. For 8 out of 14 languages, the zero-shot performance is within 5 percent of performance on English, including distant languages like Russian, Chinese, and Vietnamese. They also achieve strong results on low-resource languages like Swahili and Urdu. Finally, LASER outperforms all previous approaches to zero-shot transfer for 13 out of 14 languages.\nIn contrast to previous methods, which required one sentence to be in English, their system is fully multilingual and supports any combination of premises and hypotheses in different languages.\nUsage !pip install laserembeddings !python -m laserembeddings download-models \u0026gt;\u0026gt;\u0026gt; from laserembeddings import Laser \u0026gt;\u0026gt;\u0026gt; laser = Laser() \u0026gt;\u0026gt;\u0026gt; embedding = laser.embed_sentences(\u0026#34;I like natural language processing.\u0026#34;, lang=\u0026#34;en\u0026#34;) \u0026gt;\u0026gt;\u0026gt; embedding.shape (1, 1024) MultiFiT (Efficient multi-lingual language model fine-tuning) Main idea MultiFiT extends ULMFiT (Universal Language Model Fine-Tuning) to make it more efficient and more suitable for language modelling beyond English: It utilizes tokenization based on subwords rather than words and employs a QRNN rather than an LSTM. In addition, it leverages a number of other improvements.\nQuasi-Recurrent Neural Networks (QRNNs) Drawbacks of RNN Recurrent neural networks (RNNs) are a powerful tool for modeling sequential data, but the dependence of each timestep\u0026rsquo;s computation on the previous timestep’s output limits parallelism and makes RNNs unwieldy for very long sequences.\nQRNN The QRNN is a proposed new LM which strikes a balance between an CNN and an LSTM: It can be parallelized across time and minibatch dimensions like a CNN and inherits the LSTM’s sequential bias as the output depends on the overall order of elements in the sequence. Specifically, the QRNN alternates convolutional layers, which are parallel across timesteps and a recurrent pooling function, which is parallel across channels. Despite lacking trainable recurrent layers, stacked QRNNs have better predictive accuracy than stacked LSTMs of the same hidden size. Due to their increased parallelism, they are up to 16 times faster at train and test time.\nULMFiT ensembles the predictions of a forward and backward language model. Even though bidirectionality has been found to be important in contextual word vectors, they did not see big improvements for our downstream tasks (text classification) with ELMo-style joint training. As joint training is quite memory-intensive and they emphasize efficiency, they opted to just train forward language models for all languages.\nThe full model can be seen in the below figure. It consists of a subword embedding layer, four QRNN layers, an aggregation layer, and two linear layers. The dimensionality of each layer can be seen in each box at the top.\nFor the detailed results, check out the original paper.\nZero-shot Transfer with a Cross-lingual Teacher Definitions:\n  Source Language: the language being translated from.\n  Target Language: also called the receptor language, is the language being translated into.\n  Inductive bias: The inductive bias of a machine learning algorithm is the set of assumptions that the model makes in order to predict results given inputs it has not yet encountered (generalize to new inputs).\n  Language-agnostic representation: Sentences in different languages with the same meaning should be given similar embeddings.\n  Intuition:\nIf a powerful cross-lingual model and labeled data in a high-resource language are available, it would be nice to make use of them in some way. To this end, they propose to use the classifier that is learned on top of the cross-lingual model on the source language data as a teacher to obtain labels for training their model on the target language. This way, they can perform zero-shot transfer using the monolingual language model by bootstrapping from a cross-lingual one (uses the pre-trained model\u0026rsquo;s zero-shot predictions as pseudo labels to fine-tune the monolingual model on target language data).\nTo illustrate how this works, take a look at the following diagram:\nThe process consists of three main steps:\n  The monolingual language model is pretrained on Wikipedia data in the target language (a) and fine-tuned on in-domain data (target language documents) of the corresponding task (b).\n  Train a classifier on top of cross-lingual model such as LASER using labelled data in a high-resource source language and perform zero-shot inference as usual with this classifier to predict labels on target language documents.\n  In the final step (c), use these predicted labels to fine-tune a classifier on top of the fine-tuned monolingual language model.\n  This is similar to distillation, which has recently been used to train smaller language models or distill task-specific information into downstream models. In contrast, they do not just seek to distill the information of a big model into a small model but into one with a different inductive bias.\nIn addition to circumventing the need for labels in the target language, our approach thus brings another benefit: As the monolingual model is specialized to the target language, its inductive bias might be more suitable than the more language-agnostic representations learned by the cross-lingual model. It might thus be able to make better use of labels in the target language, even if they are noisy.\nThey\u0026rsquo;re saying that a monolingual model will be more easily fine-tunable for a particular target-language task than a cross-lingual model. Put another way: suppose you\u0026rsquo;re trying to train a POS tagger for Hindi. It\u0026rsquo;s better to have a monolingual Hindi pre-trained LM than a cross-lingual model, although that cross-lingual model could potentially do things like generalize an English tagger to work in Hindi.\nTheir results show that the monolingual language model fine-tuned on zero-shot predictions outperforms its teacher in all settings as you can see in the figure below.\nXLM (Cross-lingual Language Model) XLM uses a known pre-processing technique (BPE) and a dual-language training mechanism with BERT in order to learn relations between words in different languages. The model outperforms other models in a cross-lingual classification task (sentence entailment in 15 languages) and significantly improves machine translation when a pre-trained model is used for initialization of the translation model.\nShared sub-word vocabulary It uses Byte-Pair Encoding (BPE) that splits the input into the most common sub-words across all languages. This greatly improves the alignment of embedding spaces across languages that share either the same alphabet or anchor tokens such as digits or proper nouns. This is a common pre-processing algorithm.\nThe authors of the paper proposed three language modeling objectives CLM, MLM, and TLM. The first two only requires monolingual data, while the third one requires parallel sentences.\nCLM (Causal Language Modeling) Their causal language modeling (CLM) task consists of a Transformer language model trained to model the probability of a word given the previous words in a sentence $P(w_t|w_1, \u0026hellip; , w_{t−1}, \\theta)$. Given the previous hidden state to the current batch, the model predicts the next word.\nNote: this technique does not scale to the cross-lingual setting.\nModified MLM (Masked Language Modeling) There are two differences between the proposed MLM and the normal MLM\n  Include the use of text streams of an arbitrary number of sentences (truncated at 256 tokens) instead of pairs of sentences.\n  Subsample the frequent subword to counter the imbalance between rare and frequent tokens (e.g. punctuations or stop words).\n  TLM (Translation Language Modeling) Both the CLM and MLM objectives are unsupervised and only require monolingual data. However, these objectives cannot be used to leverage parallel data when it is available. The objective of TLM is an extension of MLM, where instead of considering monolingual text streams, we concatenate parallel sentences as illustrated in the figure below. They randomly mask words in both the source and target sentences. To predict a word masked in an English sentence, the model can either attend to surrounding English words or to the French translation, encouraging the model to align the English and French representations. In particular, the model can leverage the French context if the English one is not sufficient to infer the masked English words. To facilitate the alignment, they also reset the positions of target sentences.\nNote: BERT use segment embeddings (represent different sentence) while XLM use language embeddings (represent different language).\nThe paper also shows that training a cross-lingual language-model can be very beneficial for low-resource languages, as they can leverage data from other languages, especially similar ones mainly due to the BPE pre-processing.\nOn the XNLI benchmark, it achieves very good performance on Zero-shot. Even better performance if translated data is used during training.\nXLM-RoBERTa The biggest update that XLM-Roberta offers over the original is a significantly increased amount of training data. The cleaned CommonCrawl data that it is trained on takes up a whopping 2.5tb of storage! It is several orders of magnitude larger than the Wiki-100 corpus that was used to train its predecessor and the scale-up is particularly noticeable in the lower resourced languages. The \u0026ldquo;RoBERTa\u0026rdquo; part comes from the fact that its training routine is the same as the monolingual RoBERTa model, specifically, that the sole training objective is the Masked Language Model.\nXLM-R significantly outperforms Multilingual-BERT (M-BERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models.\nXLM-R seems to be the best solution to date. It is very possible that the TLM (Translation Language Model) approach to train multilingual transformers will be combined with other technologies.\n Website Reference:\n Judit Ács\u0026rsquo;s blog: http://juditacs.github.io/2019/02/19/bert-tokenization-stats.html https://huggingface.co/bert-base-multilingual-cased Inductive bias 1: https://blog.aylien.com/emnlp-2018-highlights-inductive-bias-cross-lingual-learning-and-more/ Inductive bias 2: https://stackoverflow.com/questions/35655267/what-is-inductive-bias-in-machine-learning fast.ai: http://nlp.fast.ai/classification/2019/09/10/multifit.html XLM 1: https://towardsdatascience.com/xlm-enhancing-bert-for-cross-lingual-language-model-5aeed9e6f14b XLM 2: https://medium.com/towards-artificial-intelligence/cross-lingual-language-model-56a65dba9358 http://nlpprogress.com/english/natural_language_inference.html https://medium.com/deepset-ai/xlm-roberta-the-multilingual-alternative-for-non-english-nlp-cf0b889ccbbf Multilingual Transformers: https://towardsdatascience.com/multilingual-transformers-ae917b36034d  Paper Reference:\n Facebook engineering: https://engineering.fb.com/ai-research/laser-multilingual-sentence-embeddings/ MultiFiT: Efficient Multi-lingual Language Model Fine-tuning: https://arxiv.org/pdf/1909.04761.pdf QUASI-RECURRENT NEURAL NETWORKS: https://arxiv.org/pdf/1611.01576.pdf Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond (LASER): https://arxiv.org/pdf/1812.10464.pdf Improving Pre-Trained Multilingual Models with Vocabulary Expansion: https://www.groundai.com/project/improving-pre-trained-multilingual-models-with-vocabulary-expansion/1 Cross-lingual ability of multilingual bert: an empirical study: https://openreview.net/pdf?id=HJeT3yrtDr Cross-lingual Language Model Pretraining: https://arxiv.org/pdf/1901.07291.pdf Unsupervised Cross-lingual Representation Learning at Scale: https://arxiv.org/pdf/1911.02116.pdf  ","permalink":"https://tangliyan.com/blog/posts/multilingual/","summary":"Multilingual Models are a type of Machine Learning model that can understand different languages. In this post, I\u0026rsquo;m going to discuss four common multi-lingual language models Multilingual-Bert (M-Bert), Language-Agnostic SEntence Representations (LASER Embeddings), Efficient multi-lingual language model fine-tuning (MultiFiT) and Cross-lingual Language Model (XLM).\nWays of tokenization Word-based tokenization Word-based tokenization works well for the morphologically poor English, but results in very large and sparse vocabularies for morphologically rich languages, such as Polish and Turkish.","title":"Multi-lingual: M-Bert, LASER, MultiFiT, XLM"},{"content":"Currently, especially in NLP, very large scale models are being trained. A large portion of those can’t even fit on an average person’s hardware. We can train a small network that can run on the limited computational resource of our mobile device. But small models can’t extract many complex features that can be handy in generating predictions unless you devise some elegant algorithm to do so. Plus, due to the Law of diminishing returns, a great increase in the size of model barely maps to a small increase in the accuracy.\nThere are currently two ways to solve this problem:\n Knowledge Distillation. Model Compression.  In this blog, I focus on talking about Knowledge Distillation. Using distillation, one could reduce the size of models like BERT by 87% and still retain 96% of its performance. Recent work even suggests that students can actually exceed teacher performance.\nShortcoming of normal neural networks Take an example of MNIST dataset. Let’s pick a sample picture of number 3.\nIn training data, the number 3 translates to a corresponding one-hot-vector: [0 0 0 1 0 0 0 0 0 0]. This vector simply tells that the number in that image in 3 but fails to explicitly mention anything about the shape of number 3. Like the shape of 3 is similar to 8. Hence, neural network is never explicitly being asked to learn the generalized understanding of the training data.\nGeneralization of Information The goal of a neural network is to predict the output for samples that the network had never seen during training by generalizing the knowledge within the training data. Taking the example of a discriminative neural network whose objective is to identify the number in a picture. Now the neural network returns distribution of probabilities across all classes 0, 1, 2, ..., 9 and this tells us a lot about the capability of the network to generalize over the concepts within the training data.\nFor a decently trained neural network on MNIST,\n  even though the probability for number 3 is significantly greater than the probability for the number 8 and number 0\n  Probability of 8 and 0 are comparable\n  still the probabilities of 8 and 0 are comparatively higher than other numbers.\n  So, the neural network is able to identify that the shape of the number in that image is 3 but the neural network also suggests that the shape of 3 is quite similar to the shape of numbers 8 and 0.\nIn the above example, we usually train a large and complex network or an ensemble model which can extract important features from the image data and can, therefore, produce better predictions.\nHowever, these models are mostly very cumbersome (aka cumbersome model/network, which means deep and complex) Its deepness gives the ability to extract complex features and its complexity gives it the power to remain accurate. But the model is heavy enough that one need a large amount of memory and a powerful GPU to perform large and complex calculations. So that’s why we need to transfer the knowledge learned by this model to a much smaller model which can easily be used in mobile.\nKnowledge Distillation A few Definitions   soft targets: network’s probability/weight distribution across all classes.\n  hard targets: one-hot vector representation within the original training data.\n  Transfer-Set: pass the data through the cumbersome model and use its output (probability distribution) as the respective truth values. It can consist of the dataset used to train the original model, new dataset or both.\n  General idea of knowledge distillation Knowledge distillation is a simple way to improve the performance of deep learning models on mobile devices. In this process, we train a large and complex network or an ensemble model which can extract important features from the given data and can, therefore, produce better predictions. Then we train a small network with the help of the cumbersome model. This small network will be able to produce comparable results, and in some cases, it can even be made capable of replicating the results of the cumbersome network.\nYou can distill the large and complex network in another much smaller network, and the smaller network does a reasonable job of approximating the original function learned by a deep network.\nTeacher and Student The distilled model (student), is trained to mimic the output of the larger network (teacher), instead of training it on the raw data directly.\nThe point is that the teacher is outputting class probabilities — soft labels rather than hard labels. A number classifier (classify 0,3,8) might say \u0026ldquo;0: 0.1, 3: 0.75, 8: 0.15\u0026rdquo; instead of \u0026ldquo;0: 0, 3: 1, 8: 0\u0026rdquo;. Why bother? Because these “soft labels” are more informative than the original ones — telling the student that 3 does very slightly resemble 0 or 8.\nStudent models can often come very close to teacher-level performance. Recent work even suggests that students can actually exceed teacher performance.\nTemperature \u0026amp; Entropy Temperature and Entropy are what we learned in physics and we know that Entropy increases with Temperature.\nWhen soft-targets have high entropy, they give much more information per-training sample than hard-targets. For example, the soft targets \u0026ldquo;0: 0.1, 3: 0.75, 8: 0.15\u0026rdquo;, contains information such as 0 and 8 are somehow similar. However, hard targets \u0026ldquo;0: 0, 3: 1, 8: 0\u0026rdquo; does not contain such relation between 0 and 8.\nHowever, the soft-targets would be less useful if the probability distribution of the output has low entropy (e.x. \u0026ldquo;0: 0.01, 3: 0.98, 8: 0.01\u0026rdquo;). If this is the case, we need to raise its entropy and make it more informative.\nSpecifically, we use a parameter Temperature (T) to adjust the level of entropy and the formula is\n$$q_{i}=\\frac{\\exp \\left(z_{i} / T\\right)}{\\sum_{j} \\exp \\left(z_{j} / T\\right)} \\tag 1$$\nNote:\n  $z_i$ is the logit of a class\n  $z_j$ are logits of all classes\n  $T$ is the temperature\n  $q_i$ is the resulting probability\n  For high temperatures $(T \\to \\infty)$, all actions have nearly the same probability.\n  For temperature $(T = 1)$, probabilities remain the same.\n  For low temperatures $(T \\to 0)$, the probability of the class with the highest logit tends to be $1$.\n  In distillation, we raise the temperature of the final softmax until the cumbersome model produces a suitably soft set of targets. We then use the same high temperature when training the small model to match these soft targets.\n  Here is an example of adjusting Temperature $T$:\n    0 3 8     truth 0 1 0   logit 0.1 0.7 0.2   Temp= 0.5 0.183 0.594 0.223   Temp= 1.0 0.254 0.464 0.282   Temp= 2.0 0.294 0.397 0.309   Temp= 5.0 0.318 0.358 0.324    Higher temperature results in a softer probability distribution over classes.\nSuitable soft targets leads to:\n  smaller loss, hence smaller correction gradient (backpropagation).\n  less variation between the gradients of different training examples.\n  As a result:\n  a greater learning rate can be used to train the model.\n  a smaller dataset can be used to train the model.\n  Training the Distil Model The simplest form of distillation is training a model on the soft targets from a cumbersome model with high temperature. But it works better to fit both the hard targets and the soft targets from the cumbersome model.\nOne of the most efficient methods of doing this is by using 2 objective functions (as shown in the figure above):\n  cross-entropy with soft targets using a high-temperature cumbersome model.\n  cross-entropy with hard targets using the same cumbersome model but with the temperature set to 1.\n  The following is a more detailed figure for training the Distil Model:\nWe calculate the total loss by\n$$L=\\lambda L^{s o f t}+(1-\\lambda) L^{h a r d} \\tag 2$$\nwhere the weight of the first term should be usually larger and the total entropy loss\n$$L^{s o f t}=-\\sum_{c=1}^{C} y_{c}^{s o f t} \\log \\frac{e^{\\frac{z_c}{T}}}{\\sum_{c=1}^{C} e^{\\frac{z_c}{T}}} \\tag 3$$\nwhere $C$ are all classes and $y_{c}^{s o f t}$ is the output soft target of class $c$ from the cumbersome model with high temperature setting.\nDuring inference, the temperature of the distilled model is set to 1 to do prediction normally.\n Reference:\n Knowledge distillation 1: https://medium.com/neuralmachine/knowledge-distillation-dc241d7c2322 Knowledge distillation 2: https://towardsdatascience.com/distillation-of-knowledge-in-neural-networks-cc02f79698b6 Knowledge distillation 3: https://blog.csdn.net/xbinworld/article/details/83063726?biz_id=102\u0026amp;utm_term=%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F\u0026amp;utm_medium=distribute.pc_search_result.none-task-blog-2~blog~sobaiduweb~default-0-83063726\u0026amp;spm=1018.2118.3001.4187 Knowledge distillation 4: https://www.zhihu.com/question/50519680 Hinton, Dark knowledge: https://www.ttic.edu/dl/dark14.pdf Teacher and Student: https://www.quora.com/What-is-a-teacher-student-model-in-a-Convolutional-neural-network  ","permalink":"https://tangliyan.com/blog/posts/distillation/","summary":"Currently, especially in NLP, very large scale models are being trained. A large portion of those can’t even fit on an average person’s hardware. We can train a small network that can run on the limited computational resource of our mobile device. But small models can’t extract many complex features that can be handy in generating predictions unless you devise some elegant algorithm to do so. Plus, due to the Law of diminishing returns, a great increase in the size of model barely maps to a small increase in the accuracy.","title":"Knowledge Distillation"},{"content":"Note This post is the second part of overall summarization of the competition. The first half is here.\nNoteworthy ideas in 1st place solution Idea First step:\nUse transformers to extract token level start and end probabilities.\nSecond step:\nFeed these probabilities to a character level model. This step gives the team a huge improve on the final score since it handled the \u0026ldquo;noise\u0026rdquo; in the data properly.\nLast step:\nEnsemble.\nSecond level models Architectures The following three Char-NN architectures uses character-level probabilities as input. The first level models output token-level probabilities and the following code convert token-level probabilities to character-level probabilities. The idea in the following cide is to assigning each character the probability of the corresponding token.\ndef token_level_to_char_level(text, offsets, preds): probas_char = np.zeros(len(text)) for i, offset in enumerate(offsets): if offset[0] or offset[1]: # remove padding and sentiment probas_char[offset[0]:offset[1]] = preds[i] return probas_char Things you need to know for nn.Embedding\nThe following architectures all train the embedding from scratch. Here we want to shortly discuss how nn.Embedding works.\nnn.Embedding holds a Tensor of dimension (vocab_size, vector_size), i.e., of (the size of the vocabulary, the dimension of each vector embedding), and a method that does the lookup. When you create an embedding layer, the Tensor is initialised randomly.\nYou can also add pretrained weights with the command nn.Embedding.from_pretrained(weight).\nArchitecture 1: RNN In the following, the parameter len_voc is calculated by\ntokenizer.fit_on_texts(df_train[\u0026#39;text\u0026#39;].values) len_voc = len(tokenizer.word_index) + 1 Compare the following code with the figure above.\nclass TweetCharModel(nn.Module): # check the config in the original code post def __init__(self, len_voc, use_msd=True, embed_dim=64, lstm_dim=64, char_embed_dim=32, sent_embed_dim=32, ft_lstm_dim=32, n_models=1): super().__init__() self.use_msd = use_msd self.char_embeddings = nn.Embedding(len_voc, char_embed_dim) self.sentiment_embeddings = nn.Embedding(3, sent_embed_dim) # 3 sentiments self.proba_lstm = nn.LSTM(n_models * 2, ft_lstm_dim, batch_first=True, bidirectional=True) self.lstm = nn.LSTM(char_embed_dim + ft_lstm_dim * 2 + sent_embed_dim, lstm_dim, batch_first=True, bidirectional=True) self.lstm2 = nn.LSTM(lstm_dim * 2, lstm_dim, batch_first=True, bidirectional=True) self.logits = nn.Sequential( nn.Linear(lstm_dim * 4, lstm_dim), nn.ReLU(), nn.Linear(lstm_dim, 2)) self.high_dropout = nn.Dropout(p=0.5) def forward(self, tokens, sentiment, start_probas, end_probas): bs, T = tokens.size() probas = torch.cat([start_probas, end_probas], -1) probas_fts, _ = self.proba_lstm(probas) char_fts = self.char_embeddings(tokens) sentiment_fts = self.sentiment_embeddings(sentiment).view(bs, 1, -1) sentiment_fts = sentiment_fts.repeat((1, T, 1)) features = torch.cat([char_fts, sentiment_fts, probas_fts], -1) features, _ = self.lstm(features) features2, _ = self.lstm2(features) features = torch.cat([features, features2], -1) # Multi-sample dropout (MSD) if self.use_msd and self.training: logits = torch.mean( torch.stack( [self.logits(self.high_dropout(features)) for _ in range(5)], dim=0), dim=0) else: logits = self.logits(features) start_logits, end_logits = logits[:, :, 0], logits[:, :, 1] return start_logits, end_logits Architecture 2: CNN class ConvBlock(nn.Module): # check the config in the original code post def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, dilation=1, padding=\u0026#34;same\u0026#34;, use_bn=True): super().__init__() if padding == \u0026#34;same\u0026#34;: padding = kernel_size // 2 * dilation if use_bn: self.conv = nn.Sequential( nn.Conv1d(in_channels, out_channels, kernel_size, padding=padding, stride=stride, dilation=dilation), nn.BatchNorm1d(out_channels), nn.ReLU()) else: self.conv = nn.Sequential( nn.Conv1d(in_channels, out_channels, kernel_size, padding=padding, stride=stride, dilation=dilation), nn.ReLU()) def forward(self, x): return self.conv(x) class TweetCharModel(nn.Module): def __init__(self, len_voc, use_msd=True, cnn_dim=64, char_embed_dim=32, sent_embed_dim=32, proba_cnn_dim=32, n_models=1, kernel_size=3, use_bn=False): super().__init__() self.use_msd = use_msd self.char_embeddings = nn.Embedding(len_voc, char_embed_dim) self.sentiment_embeddings = nn.Embedding(3, sent_embed_dim) self.probas_cnn = ConvBlock(n_models * 2, proba_cnn_dim, kernel_size=kernel_size, use_bn=use_bn) self.cnn = nn.Sequential( ConvBlock(char_embed_dim + sent_embed_dim + proba_cnn_dim, cnn_dim, kernel_size=kernel_size, use_bn=use_bn), ConvBlock(cnn_dim, cnn_dim * 2, kernel_size=kernel_size, use_bn=use_bn), ConvBlock(cnn_dim * 2 , cnn_dim * 4, kernel_size=kernel_size, use_bn=use_bn), ConvBlock(cnn_dim * 4, cnn_dim * 8, kernel_size=kernel_size, use_bn=use_bn)) self.logits = nn.Sequential( nn.Linear(cnn_dim * 8, cnn_dim), nn.ReLU(), nn.Linear(cnn_dim, 2)) self.high_dropout = nn.Dropout(p=0.5) def forward(self, tokens, sentiment, start_probas, end_probas): bs, T = tokens.size() probas = torch.cat([start_probas, end_probas], -1).permute(0, 2, 1) probas_fts = self.probas_cnn(probas).permute(0, 2, 1) char_fts = self.char_embeddings(tokens) sentiment_fts = self.sentiment_embeddings(sentiment).view(bs, 1, -1) sentiment_fts = sentiment_fts.repeat((1, T, 1)) x = torch.cat([char_fts, sentiment_fts, probas_fts], -1).permute(0, 2, 1) features = self.cnn(x).permute(0, 2, 1) # [Bs x T x nb_ft] if self.use_msd and self.training: logits = torch.mean( torch.stack( [self.logits(self.high_dropout(features)) for _ in range(5)], dim=0), dim=0) else: logits = self.logits(features) start_logits, end_logits = logits[:, :, 0], logits[:, :, 1] return start_logits, end_logits Architecture 3: WaveNet This is a model architecture from another competition, so I ignore the author\u0026rsquo;s detail here. I attached the source code in the reference.\nstacking ensemble Their solution has no post-processing and just modeling. The following is the idea how they did the final ensemble.\nNoteworthy ideas in 2nd place solution Ensemble  Using two different seeds (seed averaging):  RoBERTa Base 11th layer + RoBERTa Large 23th layer + RoBERTa Base MSD + RoBERTa Large MSD\n$4$ models $\\times$ $2$ seeds = Total $8$ models\nPost-processing on the extra space I attached the pp in the reference. I tried out this pp and it raised my rank to around $20$th.\nReranking-model training (Create multi candidates and choose best one) What is re-ranking Their model can predict not only a top-$1$ selected_text candidate but also top-$n$ candidates. So re-ranking means that they re-score these top-$n$ candidate.\nWhy re-ranking \u0026ldquo;I calculated the upper bound jaccard score of top-5 candidates in the validation set and that was 0.87-0.88. (If I choose only top-1, the score is only 0.71-0.72.)\nSo I realized that there is a huge room for improving score by re-ranking. In fact, in the field of question answering (similar to this task), the re-ranking approach is developed.\u0026rdquo;\nHow to re-rank To re-rank candidates, they used two score.\nFirst one is based on start \u0026amp; end value(after applying softmax) from base-model. Second one is a predicted jaccard score using re-ranking model.\nRe-ranking model Build a second level model on top of previous model.\n input: triple (sentiment, tweet, candidate) predict: jaccard score  The way they pass the tuple (sentiment, tweet, candidate) to the model is\ntweet_candidate = TOKENIZER.encode(str(tweet) + \u0026#34; \u0026#34; + str(candidate)) token_ids = [0] + [sentiment] + [2] + [2] + tweet_candidate.ids + [2] In concrete, they calculated top-$5$ candidates in training and validation set and memorize their jaccard score.\nThey use a simple roberta-base model to predict jaccard score using MSELoss().\nFinal re-ranking score Finally, candidates are re-ranked using this score.\n(start value + end value) * 0.5 + predicted jaccard score\nHere, start and end value means logits calculated by the base model (i.e. start and end position logits after softmax function). This part of the code is under the reference section.\nsequence bucketing (dynamic padding) Team:\u0026quot;inference time speed up x2 and surprisingly got a better result than not using.\u0026quot;\nIn RNNs, the input sequences are often all padded to the same length by doing something along the lines of this:\nx_train = pad_sequences(x_train, maxlen=MAX_LEN)\nThis is suboptimal because when iterating over the dataset in batches, there will be some batches where the length of all samples is smaller than MAX_LEN. So there will be tokens which are zero everywhere in the batch but are still processed by the RNN. Using sequence bucketing, we can speed this up by dynamically padding every batch to the maximum sequence length which occurs in that batch. Or to e.g. the $95$th percentile of lengths in that batch.\nclass RerankingCollate: def __init__(self): self.CONFIG = {} self.CONFIG[\u0026#39;BUCKET\u0026#39;] = True self.CONFIG[\u0026#39;MAX_LEN\u0026#39;] = MAX_LEN def __call__(self, batch): out = { \u0026#39;orig_tweet\u0026#39; : [], \u0026#39;sentiment\u0026#39; : [], \u0026#39;orig_selected\u0026#39; : [], \u0026#39;jaccard\u0026#39; : [], \u0026#39;score\u0026#39; : [], \u0026#39;ids\u0026#39; : [], \u0026#39;mask\u0026#39; : [], \u0026#39;token_type_ids\u0026#39; : [], } for i in range(len(batch)): for k, v in batch[i].items(): out[k].append(v) # Deciding the number of padding if self.CONFIG[\u0026#39;BUCKET\u0026#39;]: max_pad = 0 for p in out[\u0026#39;ids\u0026#39;]: if len(p)\u0026gt;max_pad: max_pad = len(p) else: max_pad = self.CONFIG[\u0026#39;MAX_LEN\u0026#39;] # Padding for i in range(len(batch)): tokenized_text = out[\u0026#39;ids\u0026#39;][i] token_type_ids = out[\u0026#39;token_type_ids\u0026#39;][i] mask = out[\u0026#39;mask\u0026#39;][i] text_len = len(tokenized_text) out[\u0026#39;ids\u0026#39;][i] = (tokenized_text + [1]*(max_pad - text_len))[:max_pad] out[\u0026#39;token_type_ids\u0026#39;][i] = (token_type_ids + [0]*(max_pad - text_len)[:max_pad] out[\u0026#39;mask\u0026#39;][i] = (mask + [0]*(max_pad - text_len))[:max_pad] # torch.float out[\u0026#39;jaccard\u0026#39;] = torch.tensor(out[\u0026#39;jaccard\u0026#39;], dtype=torch.float) out[\u0026#39;score\u0026#39;] = torch.tensor(out[\u0026#39;score\u0026#39;], dtype=torch.float) # torch.long out[\u0026#39;ids\u0026#39;] = torch.tensor(out[\u0026#39;ids\u0026#39;], dtype=torch.long) out[\u0026#39;mask\u0026#39;] = torch.tensor(out[\u0026#39;mask\u0026#39;], dtype=torch.long) out[\u0026#39;token_type_ids\u0026#39;] = torch.tensor(out[\u0026#39;token_type_ids\u0026#39;], dtype=torch.long) return out Here is how to use it:\nvalid_data_loader = torch.utils.data.DataLoader( valid_dataset, batch_size=val_nums, collate_fn=RerankingCollate(), num_workers=0, ) Noteworthy ideas in 3rd place solution Idea 1: Normal model with beamsearch-like decoder Copied XLNet\u0026rsquo;s decoder head for question answering to RoBERTa. Basically you predict the start index, get the $k$ hidden states at the top-$k$ indices. For each hidden state, concat it to the end index logits and predict the corresponding top-$k$ end indices. The best $k$ is 3, for whatever reasons, which resulted in a $3 \\times 3$ start-end pairs. I ranked them by taking the product of the two probs.\nGeneral explanation for idea 1 Training Step:\n Predict the start index normally. Take the hidden representation at the target index (ignoring the predicted) and concat it into the representations at every position. The new presentation is then fed to a MLP to predict the end index.  Inference:\n Predicting the start index normally. Take top-$k$ hidden states corresponding to top-$k$ start indices with highest probabilities. Each hidden state is then concatenated into the representations at every position. The new representation is fed to a MLP, similar to training. Then select top-$k$ end indices for each selected hidden state, resulting in $k \\times k$ top start-end pairs. The best k is $3$, which resulted in a $3 \\times 3$ start-end pairs. They ranked them by taking the product of the two probs.  Idea 2: Character level model with GRU head Address the noisy targets by adding a prediction head that enables character wise span-prediction and completely learns the noise. They trained all models using a 5-seed average. Their best submission consists of a total of 3x5x2 models (backbones x seeds x team mates).\nGeneral explanation for idea 2 They realized that the key to a decent performance is the capability to predict a character-wise span. A good example, which they are also using in the illustration below is the tweet text “is back home now gonna miss everyone” which weirdly had the label “onna”. They knew that if a model would be able to predict “onna”, that would put them in a top spot.\nIn fact a character-wise prediction would solves two issues:\n Definition of label: you now can just put the selected text as it as label. Predicting noise: you now are able to predict “half words” which was the key of the competition.  The key idea of this method is that \u0026ldquo;The model uses a standard transformer backbone which takes the word-level tokenized text and outputs hidden states with a certain dimension (in case of roberta-base 768). Instead of predicting the start and end word now, as done by “standard” models I replicate the hiddenstate of each word as often as the word has characters, so we get a hidden state for each character. In order to differentiate between characters I add a few 768 -\u0026gt; 768 RNN layers. Finally, two linear layers predict the start and end character of the selected text.\u0026rdquo;\nIn the following, I illustrate how the idea \u0026ldquo;replicate the hiddenstate of each word as often as the word has characters, so we get a hidden state for each character\u0026rdquo; is implemented. (Suppose we have $2$ words both having length $4$, with unique hidden states.)\n\u0026gt;\u0026gt;\u0026gt; x = torch.tensor([[[1,2,5,7], [5,6,7,9]]]) \u0026gt;\u0026gt;\u0026gt; x tensor([[[1, 2, 5, 7], [5, 6, 7, 9]]]) \u0026gt;\u0026gt;\u0026gt; x.size() torch.Size([1, 2, 4]) \u0026gt;\u0026gt;\u0026gt; x.unsqueeze(-1) tensor([[[[1], [2], [5], [7]], [[5], [6], [7], [9]]]]) \u0026gt;\u0026gt;\u0026gt; x.unsqueeze(-1).size() torch.Size([1, 2, 4, 1]) \u0026gt;\u0026gt;\u0026gt; x.unsqueeze(-1).expand(-1,-1,-1, 5) tensor([[[[1, 1, 1, 1, 1], [2, 2, 2, 2, 2], [5, 5, 5, 5, 5], [7, 7, 7, 7, 7]], [[5, 5, 5, 5, 5], [6, 6, 6, 6, 6], [7, 7, 7, 7, 7], [9, 9, 9, 9, 9]]]]) For the following ideas, we assume that we are writing a customized roberta model and here is the beginning of the customized roberta class:\nIdea 3: fastai style freeze-unfreeze scheme class CustomRoberta(nn.Module): def __init__(self, path=\u0026#39;path/to/roberta-base/pytorch_model.bin\u0026#39;): super(CustomRoberta, self).__init__() config = RobertaConfig.from_pretrained( \u0026#39;path/to/roberta-base/config.json\u0026#39;, output_hidden_states=True) self.roberta = RobertaModel.from_pretrained(path, config=config) self.weights_init_custom() # ignore the detail def forward(*args): pass fastai style freeze-unfreeze scheme is the following:\ndef freeze(self): for child in self.roberta.children(): for param in child.parameters(): param.requires_grad = False def unfreeze(self): for child in self.roberta.children(): for param in child.parameters(): param.requires_grad = True idea 4: Customized Layer Initialization The following code is an initialization of the last three layers of the model.\ndef weights_init_custom(self): init_layers = [9, 10, 11] dense_names = [\u0026#34;query\u0026#34;, \u0026#34;key\u0026#34;, \u0026#34;value\u0026#34;, \u0026#34;dense\u0026#34;] layernorm_names = [\u0026#34;LayerNorm\u0026#34;] for name, module in self.roberta.named_parameters(): if any(f\u0026#34;.{i}.\u0026#34; in name for i in init_layers): if any(n in name for n in dense_names): if \u0026#34;bias\u0026#34; in name: module.data.zero_() elif \u0026#34;weight\u0026#34; in name: module.data.normal_(mean=0.0, std=0.02) elif any(n in name for n in layernorm_names): if \u0026#34;bias\u0026#34; in name: module.data.zero_() elif \u0026#34;weight\u0026#34; in name: module.data.fill_(1.0) Let\u0026rsquo;s break it into parts. Let\u0026rsquo;s see an example of a pair of name and module in self.roberta.named_parameters():\n\u0026gt;\u0026gt;\u0026gt; name, module (\u0026#39;embeddings.word_embeddings.weight\u0026#39;, Parameter containing: tensor([[ 0.1476, -0.0365, 0.0753, ..., -0.0023, 0.0172, -0.0016], [ 0.0156, 0.0076, -0.0118, ..., -0.0022, 0.0081, -0.0156], [-0.0347, -0.0873, -0.0180, ..., 0.1174, -0.0098, -0.0355], ..., [ 0.0304, 0.0504, -0.0307, ..., 0.0377, 0.0096, 0.0084], [ 0.0623, -0.0596, 0.0307, ..., -0.0920, 0.1080, -0.0183], [ 0.1259, -0.0145, 0.0332, ..., 0.0121, 0.0342, 0.0168]], requires_grad=True)) The followings are some examples of weights in the last three layers that they want to initialize:\nencoder.layer.9.attention.self.query.weight\nencoder.layer.9.attention.self.query.bias\nencoder.layer.9.attention.self.key.weight\nencoder.layer.9.attention.self.key.bias\nencoder.layer.9.attention.self.value.weight\nencoder.layer.9.attention.self.value.bias\nencoder.layer.9.attention.output.dense.weight\nencoder.layer.9.attention.output.dense.bias\nencoder.layer.9.attention.output.LayerNorm.weight\nencoder.layer.9.attention.output.LayerNorm.bias\nNoteworthy ideas in 4th place solution They also use the idea of re-ranking like in 2nd place team, but their re-ranking method is quite different, so I would like to do a summary of their ideas also.\nThey add four heads to each of their transformer model and here is the detail:\nHead 1:\nTake hidden states from the last two layers. Add a linear layer without any dropout for predicting start and end tokens (with label smoothing). This is common and used by each team.\nHead 2:\nTake hidden states from the last layer. Add a linear layer to predict binary target for each token: if it should be in selected text or not. Takes hidden states from the last layer. The loss in binary cross-entropy.\nHead 3:\nTake hidden states from the last layer. Add a linear layer to predict a sentiment of each token. Predicts three classes – neutral, positive and negative. Tokens from selected text are labeled as having the same sentiment as the tweet, while all other tokens are assigned neutral class. The loss in binary cross-entropy for each token separately.\nHead 4:\nTake hidden states from the last two layers. Concatenates mean and max pooling over all tokens in a tweet skipping cls and sentiment tokens. Add two linear layers with ReLU in between to predict the sentiment of the whole tweet (with MSD).\nTraining phase During training, the total loss is calculated as the weighted sum of losses from all four heads. Training is performed on $8$ folds with AdamW optimizer and using (Stochastic Weight Averaging) SWA over a get_cosine_with_hard_restarts_schedule_with_warmup scheduler for 10 epochs.\nInference phase Score 1 (from Head 1):\nThe first head is used to create a set of (start, end) candidates. Softmax is applied across all pairs to obtain probabilities for candidates and top $3$ of them are selected to be used for the further processing. Call the probability of a candidate from this head qa_prob.\nScore 2 (from Head 2):\nThe output of the second head is the set of logits: one for each token. To obtain a score for each of the selected (start, end) candidates they took the sigmoid from the tokens and calculated the average log of the resultant token probabilities across candidate tokens. Call the output number as score_per_token.\nScore 3 (from Head 3):\nThe output of the third head is used in a very similar way to the previous. The only difference is to take the softmax over each token logits instead of sigmoid since there are three classes of sentiments. Then the probability corresponding to the sentiment of the tweet is selected. Then the same averaging operation as for previous head is applied to obtain a score for candidates. Call it sentiment_per_token.\nFrom the above, at inference time they now have three (start, end) candidates with three scores assigned to each of them.\nSecond level model Similar to $2$nd team\u0026rsquo;s solution, they build a second level model on top of previous models.\nArchitecture Used ELECTRA with the following input:\n[CLS] ([POSITIVE]|[NEUTRAL]|[NEGATIVE]) tweet [SEP] selected_text_candidate [SEP]\nSingle head (linear-\u0026gt;tanh-\u0026gt;dropout-\u0026gt;linear) on top of the transformer is fed with the concatenation of the cls token and the hidden states from the last two layers to predict if the current candidate for selected text is correct or not. Loss is computed with cross-entropy.\nTraining phase Dataset for training is built with all tweets each having three candidates from the previous model and also tweet with true selected_text is added if it is not present among candidates. Trained it for 3 epochs with AdamW and SWA.\nInference phase Three candidates for each tweet are scored with this model. It outputs two logits which are softmaxed and then the log of class 1 proba is taken as the score for the candidate. Will call it external_score in the following.\nSo after this step they have three candidates and each of them has four scores.\nThe final score for each candidate is the weighted sum of qa_prob, score_per_token, sentiment_per_token and external_score inside the model type (BERT, RoBERTa or ELECTRA) and then the weighted sum of these sums. The final prediction is the candidate with the largest score, which then goes through post-processing.\n Reference:\n 1st place solution: https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/159254 1st place solution code: https://www.kaggle.com/theoviel/character-level-model-magic 2nd place solution: https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/159310 2nd place solution code: https://www.kaggle.com/hiromoon166/inference-8models-seed100101-bucketing-2-ver2/input?select=pre_processed.txt#Inference-of-Reranking-model 2nd place post-processing: https://www.kaggle.com/futureboykid/2nd-place-post-processing 3rd place solution: https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/159910 3rd place solution code: https://github.com/suicao/tweet-extraction 4th place solution: https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/159499 5th place solution: https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/159268 Label Smoothing code: https://www.kaggle.com/shonenkov/tpu-training-super-fast-xlmroberta, https://github.com/pytorch/pytorch/issues/7455 Label Smoothing: https://www.flixstock.com/label-smoothing-an-ingredient-of-higher-model-accuracy, https://www.kaggle.com/shahules/tackle-with-label-smoothing-proved Multi-Sample Dropout for Accelerated Training and Better Generalization: https://arxiv.org/pdf/1905.09788.pdf https://stackoverflow.com/questions/50747947/embedding-in-pytorch sequence-bucketing: https://www.kaggle.com/bminixhofer/speed-up-your-rnn-with-sequence-bucketing#Implementation-\u0026amp;-comparing-static-padding-with-sequence-bucketing Re-ranking in QA paper: https://arxiv.org/pdf/1906.03008.pdf Common model structure: https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/143281 SWA: https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/  ","permalink":"https://tangliyan.com/blog/posts/kaggle_tweet_sent2/","summary":"Note This post is the second part of overall summarization of the competition. The first half is here.\nNoteworthy ideas in 1st place solution Idea First step:\nUse transformers to extract token level start and end probabilities.\nSecond step:\nFeed these probabilities to a character level model. This step gives the team a huge improve on the final score since it handled the \u0026ldquo;noise\u0026rdquo; in the data properly.\nLast step:","title":"Kaggle: Tweet Sentiment Extraction - top solutions"},{"content":"Note This post is the first part of overall summarization of the competition. The second half is here.\nBefore we start I attended two NLP competition in June, Tweet Sentiment Extraction and Jigsaw Multilingual Toxic Comment Classification, and I\u0026rsquo;m happy to be a Kaggle Expert from now on :)\nTweet Sentiment Extraction Goal:\nThe objective in this competition is to \u0026ldquo;Extract support phrases for sentiment labels\u0026rdquo;. More precisely, this competition asks kagglers to construct a model that can figure out what word or phrase best supports the given tweet from the labeled sentiment. In other word, kagglers\u0026rsquo;re attempting to predict the word or phrase from the tweet that exemplifies the provided sentiment. The word or phrase should include all characters within that span (i.e. including commas, spaces, etc).\nFor example:\ntext : \u0026quot;Sooo SAD I will miss you here in San Diego!!!\u0026quot; sentiment : negative selected_text: \u0026quot;Sooo SAD\u0026quot; In this competition, the state-of-the-art (SOTA) transformer models were not so bad in extracting the selected_text. The main problem was to capture the \u0026ldquo;noise\u0026rdquo; in the dataset.\nThe organizer of this competition did not introduce the \u0026ldquo;noise\u0026rdquo; (magic of the competition) on purpose but probably by some regex mistake (I\u0026rsquo;ll talk about the \u0026ldquo;noise\u0026rdquo; in the next section). When I analyzed the data, I found some weird selected_text like most other teams did. For example,\ntext : \u0026quot; ROFLMAO for the funny web portal =D\u0026quot; sentiment : positive selected_text: \u0026quot;e funny\u0026quot; --------------------------------------------------------------- text : \u0026quot; yea i just got outta one too....i want him back tho but i feel the same way...i`m cool on dudes for a lil while\u0026quot; sentiment : positive selected_text: \u0026quot;m cool\u0026quot; However, most teams (including my team) did not strive to figure out how such weird selected_text are selected or just treated it as a mistake such that they chose to ignore it in the consideration of overfitting if trying to correct it.\nThis turns out to be a watershed of this competition. Teams solved this problem were still among the top ranked positions in the private leaderboard but those who did not fix this problem had shakes on their ranks to some degree. I found that the scores of top $30$ teams are mostly stable but other than those, the private LB had a huge shake that was out of my expectation. The fun part is: I know there would be a shake in the competition, so I gave my team the name Hope no shake, but it didn\u0026rsquo;t help at all :( . My team was in the silver medal range in the public LB but our rank dropped to almost $800$th in the private LB! What a shame! There are teams even more unfortunate than us and dropped their ranks from top 50 to bottom 200\u0026hellip;\nAnyway, there are still some fancy and interesting solution among top ranked teams and their solutions can be divided into three categories:\n Solution with character-level model only (First place solution! Awesome!). solution with well-designed post-processing. Bolution with both character-level model and well-designed post-processing.  After the competition, I spend one week trying to understand their solutions and unique ideas and really learned a lot. So here I would like to share their ideas to those who are interested.\nIn the rest of the post, I made a summary of the top solutions and also add some of my understanding. The reference are at the bottom. So let\u0026rsquo;s get started!\nWhat is the MAGIC? This is just a bug introduced when the competition holder created this task. Here shows a representative example and we call this the \u0026ldquo;noise\u0026rdquo; in the labels.\nThe given original annotation is “onna” but it is too weird. The true annotation should be “miss” (this is a negative sentence). We think that the host applied a wrong slice obtained on the normalized text without consequence spaces for the original text with plenty of spaces, emojis, or emoticons.\nHere is how to solve it theoretically:\n Recover true annotation from the buggy annotation (pre-processing). Train model with true annotation. Predict the right annotation. Project back the right annotation to the buggy annotation (post-processing).  Here is the visualization:\nCommon Methods Most Kagglers use the following model structure (from public notebook) as a baseline and here is the illustration (I copied it from the author, the link in at the bottom of the post). This is the tensorflow version:\nWe are given text, selected_text, and sentiment. For roBERTa model, we prepare question answer as \u0026lt;s\u0026gt; text \u0026lt;/s\u0026gt;\u0026lt;/s\u0026gt; sentiment \u0026lt;/s\u0026gt;. Note that roBERTa tokenizer sometimes creates more than 1 token for 1 word. Let\u0026rsquo;s take the example \u0026ldquo;Kaggle is a fun webplace!\u0026rdquo;. The word \u0026ldquo;webplace\u0026rdquo; will be split into two tokens \u0026ldquo;[web][place]\u0026rdquo; by roBERTa tokenizer.\nAfter converting text and selected_text into tokens, we can then determine the start index and end index of selected_text within text. We will one hot encode these indices. Below are the required inputs and targets for roBERTa. In this example, we have chosen roBERTa with max_len=16, so our input_ids have 2 \u0026lt;pad\u0026gt; tokens.\nWe begin with vanilla TFRobertaModel. This model takes our 16 (max_len) input_ids and outputs $16$ vectors each of length 768. Each vector is a representation of one input token.\nWe then apply tf.keras.layers.Conv1D(filters=1, kernel_size=1) which transforms this matrix of size ($768, 16$) into a vector of size (1, 16). Next we apply softmax to this length $16$ vector and get a one hot encoding of our start_index. We build another head for our end_index.\nMost top ranked kagglers implemented the following two methods: Label Smoothing and Multi-sample Dropout. SO I would like to talk about this methods first before I go forward.\nLabel Smoothing When we apply the cross-entropy loss to a classification task, we’re expecting true labels to have 1, while the others 0. In other words, we have no doubts that the true labels are true, and the others are not. Is that always true in our case? As a result, the ground truth labels we have had perfect beliefs on are possibly wrong.\nOne possible solution to this is to relax our confidence on the labels. For instance, we can slightly lower the loss target values from 1 to, say, 0.9. And naturally, we increase the target value of 0 for the others slightly as such. This idea is called label smoothing.\nThe smoothed labels are calculated by\nnew_onehot_labels = onehot_labels * (1 – label_smoothing) + label_smoothing / num_classes\nFoe example, suppose we are training a model for binary classification, and our labels are $0$ for Non-toxic, $1$ for toxic. Now, say you set label_smoothing = 0.2, then using the equation above, we get:\nnew_labels = [0, 1] * (1 — 0.2) + 0.2 / 2 = [0, 1]*(0.8) + 0.1 = [0.1 ,0.9]\nImplementation of Label Smoothing In tensorflow tf.keras.losses.binary_crossentropy( y_true, y_pred, from_logits=False, label_smoothing=0 ) In pytorch There are multiple ways to achieve this, I list two here.\n way 1  class LabelSmoothing(nn.Module): def __init__(self, smoothing = 0.1): super(LabelSmoothing, self).__init__() self.confidence = 1.0 - smoothing self.smoothing = smoothing def forward(self, x, target): if self.training: x = x.float() target = target.float() logprobs = torch.nn.functional.log_softmax(x, dim = -1) nll_loss = -logprobs * target nll_loss = nll_loss.sum(-1) smooth_loss = -logprobs.mean(dim=-1) loss = self.confidence * nll_loss + self.smoothing * smooth_loss return loss.mean() else: return torch.nn.functional.cross_entropy(x, target) Somehow in the training step, you would use label smoothing like this:\ncriterion = LabelSmoothing() loss = criterion(outputs, targets)  way 2  class LabelSmoothing(nn.Module): def __init__(self, classes, smoothing=0.0, dim=-1): super(LabelSmoothing, self).__init__() self.confidence = 1.0 - smoothing self.smoothing = smoothing self.cls = classes self.dim = dim def forward(self, pred, target): pred = pred.log_softmax(dim=self.dim) with torch.no_grad(): # true_dist = pred.data.clone() true_dist = torch.zeros_like(pred) true_dist.fill_(self.smoothing / (self.cls - 1)) true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence) return torch.mean(torch.sum(-true_dist * pred, dim=self.dim)) Multi-sample dropout This is also an idea from the 1st place solution of the last competition I attended (Google QUEST Q\u0026amp;A Labeling). The idea came from a paper called Multi-Sample Dropout for Accelerated Training and Better Generalization.\nThe original dropout creates a randomly selected subset (called a dropout sample) from the input in each training iteration while the multi-sample dropout creates multiple dropout samples. The loss is calculated for each sample, and then the sample losses are averaged to obtain the final loss. Experimental results showed that multi-sample dropout significantly accelerates training by reducing the number of iterations until convergence Experiments also showed that networks trained using multi-sample dropout achieved lower error rates and losses for both the training set and validation set.\nImplementation The implementation is not that hard and the following is part of my code used in the competition.\ndef forward(self, input_ids, attention_mask): # `hs` is the 12 hidden layers. Later we uses hs[-1], hs[-2] # which are the last 2 hidden layers. batch_size=16. _, _, hs = self.roberta(input_ids, attention_mask) # x = torch.stack([hs[-1], hs[-2]]) # torch.Size([2, 16, 96, 768]) # x = x[0] * 0.9 + x[1] * 0.1 # torch.Size([16, 96, 768]) stacked = torch.stack([hs[-1], hs[-2]]) # torch.Size([2, 16, 96, 768]) apool= torch.mean(stacked, 0) # torch.Size([16, 96, 768]) mpool, _ = torch.max(stacked, 0) # torch.Size([16, 96, 768]) x = torch.cat((apool, mpool), -1) # torch.Size([16, 96, 768 * 2])  # Multisample Dropout: https://arxiv.org/abs/1905.09788 logits = torch.mean( torch.stack( [self.fc(self.high_dropout(x)) for _ in range(5)], dim=0, ), dim=0, ) start_logits, end_logits = logits.split(1, dim=-1) # torch.Size([16, 96, 1]) start_logits = start_logits.squeeze(-1) # torch.Size([16, 96]) end_logits = end_logits.squeeze(-1) # torch.Size([16, 96]) return start_logits, end_logits Stochastic Weight Averaging (SWA) Author: by Pavel Izmailov and Andrew Gordon Wilson\nStochastic Weight Averaging (SWA) is a simple procedure that improves generalization in deep learning over Stochastic Gradient Descent (SGD) at no additional cost, and can be used as a drop-in replacement for any other optimizer in PyTorch.\nIn short, SWA performs an equal average of the weights traversed by SGD with a modified learning rate schedule. SWA solutions end up in the center of a wide flat region of loss, while SGD tends to converge to the boundary of the low-loss region, making it susceptible to the shift between train and test error surfaces (see the middle and right panels in the figure below):\nWith the implementation in torchcontrib, using SWA is as easy as using any other optimizer in PyTorch:\nfrom torchcontrib.optim import SWA # training loop base_opt = torch.optim.SGD(model.parameters(), lr=0.1) opt = torchcontrib.optim.SWA(base_opt, swa_start=10, swa_freq=5, swa_lr=0.05) for _ in range(100): opt.zero_grad() loss_fn(model(input), target).backward() opt.step() opt.swap_swa_sgd() You can wrap any optimizer from torch.optim using the SWA class, and then train your model as usual. When training is complete you simply call swap_swa_sgd() to set the weights of your model to their SWA averages. Below we explain the SWA procedure and the parameters of the SWA class in detail. We emphasize that SWA can be combined with any optimization procedure, such as Adam, in the same way that it can be combined with SGD.\nFor the following methods, we assume that we are writing a customized roberta model and here is the beginning of the customized roberta class\nclass CustomRoberta(nn.Module): def __init__(self, path=\u0026#39;path/to/roberta-base/pytorch_model.bin\u0026#39;): super(CustomRoberta, self).__init__() config = RobertaConfig.from_pretrained( \u0026#39;path/to/roberta-base/config.json\u0026#39;, output_hidden_states=True) self.roberta = RobertaModel.from_pretrained(path, config=config) self.weights_init_custom() # ignore the detail def forward(*args): pass Different learning rate settings for encoder and head This is an idea from the last competition I attended called Google QUEST Q\u0026amp;A Labeling, and this idea is mentioned in the 1st place solution.\nparam_optimizer = list(model.named_parameters()) no_decay = [\u0026#39;bias\u0026#39;, \u0026#39;LayerNorm.bias\u0026#39;, \u0026#39;LayerNorm.weight\u0026#39;] optimizer_grouped_parameters = [ {\u0026#39;params\u0026#39;: [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \u0026#39;weight_decay\u0026#39;: 0.01}, {\u0026#39;params\u0026#39;: [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \u0026#39;weight_decay\u0026#39;: 0.0} ] num_train_optimization_steps = int(EPOCHS*len(train_df)/batch_size/accumulation_steps) optimizer = AdamW(optimizer_grouped_parameters, lr=lr, correct_bias=False) scheduler1 = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0.1*num_train_optimization_steps, num_training_steps=num_train_optimization_steps) scheduler2 = get_constant_schedule(optimizer) Customized Layer Initialization The following code is an initialization of the last three layers of the model.\ndef weights_init_custom(self): init_layers = [9, 10, 11] dense_names = [\u0026#34;query\u0026#34;, \u0026#34;key\u0026#34;, \u0026#34;value\u0026#34;, \u0026#34;dense\u0026#34;] layernorm_names = [\u0026#34;LayerNorm\u0026#34;] for name, module in self.roberta.named_parameters(): if any(f\u0026#34;.{i}.\u0026#34; in name for i in init_layers): if any(n in name for n in dense_names): if \u0026#34;bias\u0026#34; in name: module.data.zero_() elif \u0026#34;weight\u0026#34; in name: module.data.normal_(mean=0.0, std=0.02) elif any(n in name for n in layernorm_names): if \u0026#34;bias\u0026#34; in name: module.data.zero_() elif \u0026#34;weight\u0026#34; in name: module.data.fill_(1.0) Let\u0026rsquo;s break it into parts. Let\u0026rsquo;s see an example of a pair of name and module in self.roberta.named_parameters():\n\u0026gt;\u0026gt;\u0026gt; name, module (\u0026#39;embeddings.word_embeddings.weight\u0026#39;, Parameter containing: tensor([[ 0.1476, -0.0365, 0.0753, ..., -0.0023, 0.0172, -0.0016], [ 0.0156, 0.0076, -0.0118, ..., -0.0022, 0.0081, -0.0156], [-0.0347, -0.0873, -0.0180, ..., 0.1174, -0.0098, -0.0355], ..., [ 0.0304, 0.0504, -0.0307, ..., 0.0377, 0.0096, 0.0084], [ 0.0623, -0.0596, 0.0307, ..., -0.0920, 0.1080, -0.0183], [ 0.1259, -0.0145, 0.0332, ..., 0.0121, 0.0342, 0.0168]], requires_grad=True)) The followings are some examples of weights in the last three layers that they want to initialize:\nencoder.layer.9.attention.self.query.weight\nencoder.layer.9.attention.self.query.bias\nencoder.layer.9.attention.self.key.weight\nencoder.layer.9.attention.self.key.bias\nencoder.layer.9.attention.self.value.weight\nencoder.layer.9.attention.self.value.bias\nencoder.layer.9.attention.output.dense.weight\nencoder.layer.9.attention.output.dense.bias\nencoder.layer.9.attention.output.LayerNorm.weight\nencoder.layer.9.attention.output.LayerNorm.bias\n Reference:\n 1st place solution: https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/159254 1st place solution code: https://www.kaggle.com/theoviel/character-level-model-magic 2nd place solution: https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/159310 2nd place solution code: https://www.kaggle.com/hiromoon166/inference-8models-seed100101-bucketing-2-ver2/input?select=pre_processed.txt#Inference-of-Reranking-model 2nd place post-processing: https://www.kaggle.com/futureboykid/2nd-place-post-processing 3rd place solution: https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/159910 3rd place solution code: https://github.com/suicao/tweet-extraction 4th place solution: https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/159499 5th place solution: https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/159268 Label Smoothing code: https://www.kaggle.com/shonenkov/tpu-training-super-fast-xlmroberta, https://github.com/pytorch/pytorch/issues/7455 Label Smoothing: https://www.flixstock.com/label-smoothing-an-ingredient-of-higher-model-accuracy, https://www.kaggle.com/shahules/tackle-with-label-smoothing-proved Multi-Sample Dropout for Accelerated Training and Better Generalization: https://arxiv.org/pdf/1905.09788.pdf https://stackoverflow.com/questions/50747947/embedding-in-pytorch sequence-bucketing: https://www.kaggle.com/bminixhofer/speed-up-your-rnn-with-sequence-bucketing#Implementation-\u0026amp;-comparing-static-padding-with-sequence-bucketing Re-ranking in QA paper: https://arxiv.org/pdf/1906.03008.pdf Common model structure: https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/143281 SWA: https://pytorch.org/blog/stochastic-weight-averaging-in-pytorch/  ","permalink":"https://tangliyan.com/blog/posts/kaggle_tweet_sent1/","summary":"Note This post is the first part of overall summarization of the competition. The second half is here.\nBefore we start I attended two NLP competition in June, Tweet Sentiment Extraction and Jigsaw Multilingual Toxic Comment Classification, and I\u0026rsquo;m happy to be a Kaggle Expert from now on :)\nTweet Sentiment Extraction Goal:\nThe objective in this competition is to \u0026ldquo;Extract support phrases for sentiment labels\u0026rdquo;. More precisely, this competition asks kagglers to construct a model that can figure out what word or phrase best supports the given tweet from the labeled sentiment.","title":"Kaggle: Tweet Sentiment Extraction - common methods"},{"content":"Sequence Data There are many sequence data in applications. Here are some examples\n  Machine translation\n from text sequence to text sequence.    Text Summarization\n from text sequence to text sequence.    Sentiment classification\n from text sequence to categories.    Music Generation\n from nothing or some simple stuff (character, integer, etc) to wave sequence.    Name entity recognition (NER)\n From text sequence to label sequence.    Why not use a standard neural network for sequence tasks   Inputs, outputs can be different lengths in different examples. This can be solved by standard neural network by paddings with the maximum lengths but it\u0026rsquo;s not a good solution since there would be too many parameters.\n  Doesn\u0026rsquo;t share features learned across different positions of text/sequence. Note that Convolutional Neural Network (CNN) is a good example of parameter sharing, so we should have a similar model for sequence data.\n  RNN A Recurrent Neural Network (RNN) can be thought of as multiple copies of the same network, each passing a message to a successor. This chain-like nature reveals that recurrent neural networks are intimately related to sequences. Therefore, they’re the natural architecture of neural network to use for sequence data. Note that it also allows previous outputs to be used as inputs.\nFor each time step $t$, the activation \\(a^{\u0026lt;t\u0026gt;}\\) and the output $y^{\u0026lt;t\u0026gt;}$ are expressed as follows:\n  $a^{\u0026lt;t\u0026gt;}=g_{1}\\left(W\\_{a a} a^{\u0026lt;t-1\u0026gt;}+W_{a x} x^{\u0026lt;t\u0026gt;}+b_{a}\\right)$\n  $y^{\u0026lt;t\u0026gt;}=g_{2}\\left(W\\{y a} a^{\u0026lt;t\u0026gt;}+b\\{y}\\right)$\n  These calculation could be visualized in the following figure\nNote:\n  dimension of $W_{a a}$: (number of hidden neurons, number of hidden neurons)\n  dimension of $W_{a x}$: (number of hidden neurons, length of $x$)\n  dimension of $W_{y a}$: (length of $y$, number of hidden neurons)\n  The weight matrix $W_{a a}$ is the memory the RNN is trying to maintain from the previous layers.\n  dimension of $b_a$: (number of hidden neurons, 1)\n  dimension of $b_y$: (length of $y$, 1)\n  We can simplify the notations further;\n  $a^{\u0026lt;t\u0026gt;}=g_{1}\\left(W_{a} , [a^{\u0026lt;t-1\u0026gt;}, x^{\u0026lt;t\u0026gt;}] + b_{a}\\right)$\n  $y^{\u0026lt;t\u0026gt;}=g_{2}\\left(W_{y}, a^{\u0026lt;t\u0026gt;}+b_{y}\\right)$\n  Note:\n  $w_a$ is $w_{aa}$ and $w_{ax}$ stacked horizontally.\n  $[a^{\u0026lt;t-1\u0026gt;}, x^{\u0026lt;t\u0026gt;}]$ is $a^{\u0026lt;t-1\u0026gt;}$ and $x^{\u0026lt;t\u0026gt;}$ stacked vertically.\n  dimension of $w_a$: (number of hidden neurons, number of hidden neurons $+$ length of $x$)\n  dimension of $[a^{\u0026lt;t-1\u0026gt;}, x^{\u0026lt;t\u0026gt;}]$: (number of hidden neurons $+$ length of $x$, 1)\n  Different types of RNNs Loss function of RNN The loss function $\\mathcal{L}$ of all time steps is defined based on the loss at every time step as follows:\n$$\\mathcal{L}(\\hat{y}, y)=\\sum_{t=1}^{T_{y}} \\mathcal{L}\\left(\\hat{y}^{\u0026lt;t\u0026gt;}, y^{\u0026lt;t\u0026gt;}\\right)$$\nBackpropagation through time Backpropagation is done at each point in time. At timestep $t$, the derivative of the loss $\\mathcal{L}$ with respect to weight matrix $W$ is expressed as follows:\n$$ \\begin{aligned} \\frac{\\partial \\mathcal{L}^{(t)}}{\\partial W} \u0026amp;= \\left. \\sum_{k=0}^{t} \\frac{\\partial \\mathcal{L}^{(t)}}{\\partial W}\\right|_{(k)} \\ \u0026amp;= \\sum_{k=0}^{t} \\frac{\\partial \\mathcal{L}^{(t)}}{\\partial y^{\u0026lt;t\u0026gt;}} \\frac{\\partial y^{\u0026lt;t\u0026gt;}}{\\partial a^{\u0026lt;t\u0026gt;}} \\frac{\\partial a^{\u0026lt;t\u0026gt;}}{\\partial a^{}} \\frac{\\partial a^{}}{\\partial W} \u0026amp;\u0026amp; (1)\\ \u0026amp;= \\sum_{k=0}^{t} \\frac{\\partial \\mathcal{L}^{(t)}}{\\partial y^{\u0026lt;t\u0026gt;}} \\frac{\\partial y^{\u0026lt;t\u0026gt;}}{\\partial a^{\u0026lt;t\u0026gt;}} \\left(\\prod_{j=k+1}^t \\frac{\\partial a^{}}{\\partial a^{}} \\right) \\frac{\\partial a^{}}{\\partial W} \u0026amp;\u0026amp; (2)\\ \\end{aligned} $$\nNote that from $(1)$ to $(2)$, we used the chain rule on $\\frac{\\partial a^{\u0026lt;t\u0026gt;}}{\\partial a^{}}$. From the derivative formula, we see that RNN could suffer from vanishing gradient descent problem easily.\nVanishing gradients with RNNs Suppose we are working with language modeling problem and there are two sequences that model tries to learn:\n \u0026ldquo;The cat, which already ate \u0026hellip;, was full\u0026rdquo; \u0026ldquo;The cats, which already ate \u0026hellip;, were full\u0026rdquo;  The naive RNN is not very good at capturing very long-term dependencies like this. The reason is clear from the above section (Backpropagation through time).\nAdvantages and Drawbacks of RNN Advantages:\n  Possibility of processing input of any length.\n  Model size not increasing with size of input.\n  Computation takes into account historical information.\n  Weights are shared across time.\n  Drawbacks:\n  Computation being slow.\n  Difficulty of accessing information from a long time ago.\n  Cannot consider any future input for the current state.\n  LSTM Long Short Term Memory (LSTM) networks are a special kind of RNN, capable of learning long-term dependencies. LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn.\nTypes of gates In order to remedy the vanishing gradient problem, specific gates are used in some types of RNNs and usually have a well-defined purpose. They are usually noted $\\Gamma$ and are equal to:\n$$ \\Gamma=\\sigma\\left(W_1 a^{\u0026lt;t-1\u0026gt;}+ W_2 x^{\u0026lt;t\u0026gt;}+b\\right) $$\nwhere $W_1, W_2, b$ are coefficients specific to the gate and $\\sigma$ is the sigmoid function. We can also simplify it to\n$$ \\Gamma=\\sigma\\left(W \\left[a^{\u0026lt;t-1\u0026gt;}, x^{\u0026lt;t\u0026gt;}\\right]+b\\right) $$\n  Relevance gate $\\Gamma_{r}$: Drop previous information?\n  Forget gate $\\Gamma_{f}$: Erase a cell or not?\n  Output gate $\\Gamma_{o}$: How much to reveal of a cell?\n  formulas and illustration of formulas Variants of RNNs Bi-directional RNN   Part of the forward propagation goes from left to right, and part from right to left. Note that this is just a combination of two uni-directional RNN. It can\u0026rsquo;t strictly learn from \u0026ldquo;both sides\u0026rdquo;.\n  To make predictions we use $\\hat{y}^{\u0026lt;t\u0026gt;}$ by using the two activations that come from left and right.\n  The blocks here can be any RNN block including the basic RNNs, LSTMs, or GRUs.\n  The disadvantage of Bi-RNNs that you need the entire sequence before you can process it. For example, in live speech recognition if you use BiRNNs you will need to wait for the person who speaks to stop to take the entire sequence and then make your predictions.\n  Deep RNN Note: In feed-forward deep nets, there could be $100$ or even $200$ layers. In deep RNNs stacking $3$ layers is already considered deep and expensive to train.\n Reference:\n http://colah.github.io/posts/2015-08-Understanding-LSTMs/ https://github.com/mbadry1/DeepLearning.ai-Summary/tree/master/5-%20Sequence%20Models#recurrent-neural-networks https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks  ","permalink":"https://tangliyan.com/blog/posts/lstm/","summary":"Sequence Data There are many sequence data in applications. Here are some examples\n  Machine translation\n from text sequence to text sequence.    Text Summarization\n from text sequence to text sequence.    Sentiment classification\n from text sequence to categories.    Music Generation\n from nothing or some simple stuff (character, integer, etc) to wave sequence.    Name entity recognition (NER)","title":"Recurrent Neural Network (RNN) and Long Short Term Memory (LSTM)"},{"content":"Deep Learning v.s. Machine Learning The major difference between Deep Learning and Machine Learning technique is the problem solving approach. Deep Learning techniques tend to solve the problem end to end, where as Machine learning techniques need the problem statements to break down to different parts to be solved first and then their results to be combine at final stage.\nForward Propagation The general procedure is the following:\n$$ \\begin{aligned} a^{(1)}(x) \u0026amp;= w^{(1)^T} \\cdot x + b^{(1)} \\\\ h^{(1)}(x) \u0026amp;= g_1(a^{(1)}(x)) \\\\ a^{(2)}(x) \u0026amp;= w^{(2)^T} \\cdot h^{(1)}(x) + b^{(2)} \\\\ h^{(2)}(x) \u0026amp;= g_2(a^{(2)}(x)) \\\\ \u0026amp;\u0026hellip;\u0026hellip; \\\\ a^{(L+1)}(x) \u0026amp;= w^{(L+1)^T} \\cdot h^{(L)}(x) + b^{(L+1)} \\\\ h^{(L+1)}(x) \u0026amp;= g_{L+1}(a^{(L+1)}(x)) \\end{aligned} $$\nNote:\n  $w^{(i)}$ has dimension: (# of (hidden) units in layer $i$) $\\times$ (# of (hidden) units in layer $i-1$).\n  $b^{(i)}, a^{(i)}, h^{(i)}$ have the same dimension: (# of (hidden) units in layer $i$, $1$).\n  $g_{i}$ is an activation function. Sigmoid, tanh, relu are common activation functions. In the last layer, the choose of activation function $g_{(L+1)}$ depends on problems, usually sigmoid, and softmax.\n  Loss functions of neural network Common loss functions are cross-entropy loss, hinge loss, triple loss, etc. In fact, depending on specifice problems, we can define arbitrarily loss functions. We can also use AUC as a loss.\nIn this post, we focus on the cross-entropy loss.\nFor example, let the output of the neural network be $(0.6, 0.2, 0.1, 0.1)$, and the true label $(0, 1, 0, 0)$, then we can write the cross-entropy loss as\n$$ \\ell \\left(\\left[\\begin{array}{l} 0.6 \\\\ 0.2 \\\\ 0.1 \\\\ 0.1 \\end{array}\\right],\\left[\\begin{array}{l} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{array}\\right]\\right) = -(0 \\cdot \\log 0.6 + 1 \\cdot \\log 0.2 + 0 \\cdot \\log 0.1 + 0 \\cdot \\log 0.1) = -\\log 0.2 $$\nFrom now on, we call the output of the neural network $f(x)$, and the true label w.r.t $x$ is y, then the corss-entropy loss is written by\n$$\\ell \\left( f(x), y\\right) = - \\log f(x)_y$$\nwhere $f(x)_y$ is the $y$-th entry of $f(x)$.\nBack-propagation In fitting a neural network, backpropagation computes the gradient of the loss function with respect to the weights of the network. The backpropagation algorithm works by computing the gradient of the loss function with respect to each weight by the chain rule, computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule.\nIn the following, we would try to compute $\\frac{\\partial \\ell}{\\partial f(x)}, \\frac{\\partial \\ell}{\\partial a^{(L+1)}(x)}, \\frac{\\partial \\ell}{\\partial h^{(k)}(x)}, \\frac{\\partial \\ell}{\\partial a^{(k)}(x)}, \\frac{\\partial \\ell}{\\partial w^{(k)}}, \\frac{\\partial \\ell}{\\partial b^{(k)}}$, and then make a summary of the back-propagation process.\ncompute $\\frac{\\partial \\ell}{\\partial f(x)}$ First consider single element: $$ \\begin{aligned} \\frac{\\partial \\ell}{\\partial f(x)_j} \u0026amp;= \\frac{\\partial -\\log f(x)_y}{\\partial f(x)_j} \\\\ \u0026amp;= \\frac{-1}{f(x)_y} \\cdot \\frac{\\partial f(x)_y}{\\partial f(x)_j} \\\\ \u0026amp;= \\frac{-1\\cdot I(y=j)}{f(x)_y} \\end{aligned} $$\nPut it into vector form:\n$$ \\frac{\\partial \\ell}{\\partial f(x)} = \\frac{\\partial -\\log f(x)_y}{\\partial f(x)} = \\frac{-1}{f(x)_y} \\cdot\\left(\\begin{array}{l} I(y=1) \\\\ I(y=2) \\\\ \\quad \u0026hellip; \\\\ I(y=n) \\ \\end{array}\\right) = \\frac{-e(y)}{f(x)_y} \\tag 1 $$\nwhere $e(y)$ is the one-hot encoding of label $y$.\ncompute $\\frac{\\partial \\ell}{\\partial a^{(L+1)}(x)}$ First consider single element:\n$$ \\begin{aligned} \\frac{\\partial \\ell}{\\partial a^{(L+1)}(x)_j} \u0026amp;= \\frac{\\partial -\\log f(x)_y}{\\partial a^{(L+1)}(x)_j} \\\\ \u0026amp;= \\frac{-1}{f(x)_y} \\cdot \\frac{\\partial f(x)_y}{\\partial a^{(L+1)}(x)_j}\\\\ \u0026amp;= \\frac{-1}{f(x)_y} \\cdot \\frac{\\partial}{\\partial a^{(L+1)}(x)_j} \\cdot \\frac{\\exp (a^{(L+1)}(x)_y)}{\\sum_{j'} \\exp (a^{(L+1)}(x)_{j'})} \\\\ \u0026amp;= f(x)_j - I(y=j) \\ \\end{aligned} $$\nPut it into vector form:\n$$\\frac{\\partial \\ell}{\\partial a^{(L+1)}(x)} = \\left(\\begin{array}{l} f(x)_1 - I(y=1) \\\\ f(x)_2 - I(y=2) \\\\ \\quad \u0026hellip; \\\\ f(x)_n - I(y=n) \\ \\end{array}\\right) = f(x) - e(y) \\tag 2 $$\nNote that here, we assume the last activation function is softmax.\ncompute $\\frac{\\partial \\ell}{\\partial h^{(k)}(x)}$ $$ \\begin{aligned} \\frac{\\partial \\ell}{\\partial h^{(k)}(x)_j} \u0026amp;= \\sum_i \\frac{\\partial \\ell}{\\partial a^{(k+1)}(x)_i} \\cdot \\frac{\\partial a^{(k+1)}(x)_i}{\\partial h^{(k)}(x)_j} \\\\ \u0026amp;= \\sum_i \\frac{\\partial \\ell}{\\partial a^{(k+1)}(x)_i} \\cdot \\frac{\\partial \\sum_j w_{ij}^{(k+1)}h^{(k)}(x)_j + b^{(k+1)}_i}{\\partial h^{(k)}(x)_j} \\\\ \u0026amp;= \\sum_i \\frac{\\partial \\ell}{\\partial a^{(k+1)}(x)_i} \\cdot w_{ij}^{(k+1)} \\\\ \u0026amp;= w_{ij}^{(k+1)^T} \\cdot \\frac{\\partial -\\log f(x)_y}{\\partial a^{(k+1)}(x)} \\end{aligned} $$\nwhere $a^{(k+1)}(x) = w^{(k+1)}h^{(k)}(x) + b^{(k+1)}$.\nPut it into vector form:\n$$ \\frac{\\partial \\ell}{\\partial h^{(k)}(x)} = w^{(k+1)^T} \\cdot \\frac{\\partial -\\log f(x)_y}{\\partial a^{(k+1)}(x)} \\tag 3$$\ncompute $\\frac{\\partial \\ell}{\\partial a^{(k)}(x)}$ $$ \\begin{aligned} \\frac{\\partial \\ell}{\\partial a^{(k)}(x)_j} \u0026amp;= \\frac{\\partial \\ell}{\\partial h^{(k)}(x)_j} \\cdot \\frac{\\partial h^{(k)}(x)_j}{\\partial a^{(k)}(x)_j} \\\\ \u0026amp;= \\frac{\\partial \\ell}{\\partial h^{(k)}(x)_j} \\cdot g'_k(a^{(k)}(x)_j) \\end{aligned} $$\nwhere $ h^{(k)}(x) = g_k(a^{(k)}(x))$. Now put it into vector form:\n$$ \\begin{aligned} \\frac{\\partial \\ell}{\\partial a^{(k)}(x)} \u0026amp;= \\left( \\frac{\\partial \\ell}{\\partial h^{(k)}(x)_1} \\cdot g'_k(a^{(k)}(x)_1), \\ \u0026hellip; \\ ,\\ \\frac{\\partial \\ell}{\\partial h^{(k)}(x)_n} \\cdot g'_k(a^{(k)}(x)_n) \\right) \\\\ \u0026amp;= \\left(\\begin{array}{l} \\frac{\\partial \\ell}{\\partial h^{(k)}(x)_1} \\\\ \\quad \u0026hellip; \\\\ \\frac{\\partial \\ell}{\\partial h^{(k)}(x)_n} \\\\ \\end{array}\\right) \\odot \\left(\\begin{array}{l} g'_k(a^{(k)}(x)_1) \\\\ \\quad \u0026hellip; \\\\ g'_k(a^{(k)}(x)_n) \\ \\end{array}\\right) \\\\ \u0026amp;= \\frac{\\partial \\ell}{\\partial h^{(k)}(x)} \\odot g'(a^{(k)}(x)) \\end{aligned} \\tag 4 $$\nwhere \u0026ldquo;$\\odot$\u0026rdquo; means the element-wise product.\ncompute $\\frac{\\partial \\ell}{\\partial w^{(k)}}$ $$ \\begin{aligned} \\frac{\\partial \\ell}{\\partial w_{ij}^{(k)}} \u0026amp;= \\frac{\\partial \\ell}{\\partial a^{(k)}(x)_i} \\cdot \\frac{\\partial a^{(k)}(x)_i}{\\partial w_{ij}^{(k)}} \\\\ \u0026amp;= \\frac{\\partial \\ell}{\\partial a^{(k)}(x)_i} \\cdot \\frac{\\sum_j w_{ij}^{(k)}h^{(k-1)}(x)_j + b^{(k)}_i}{\\partial w_{ij}^{(k)}} \\\\ \u0026amp;= \\frac{\\partial \\ell}{\\partial a^{(k)}(x)_i} \\cdot h^{(k-1)}(x)_j \\end{aligned} $$\nput it into vector form:\n$$ \\begin{aligned} \\frac{\\partial \\ell}{\\partial w^{(k)}} \u0026amp;= \\frac{\\partial -\\log f(x)_y}{\\partial a^{(k)}(x)} \\cdot( h^{(k-1)}(x))^T \\\\ \u0026amp;= \\left(\\begin{array}{l} \\frac{\\partial -\\log f(x)_y}{\\partial a^{(k)}(x)_1} \\\\ \\quad \u0026hellip; \\\\ \\frac{\\partial -\\log f(x)_y}{\\partial a^{(k)}(x)_m} \\ \\end{array}\\right) \\cdot \\left( h^{(k-1)}(x)_1, ,\u0026hellip; , , , h^{(k-1)}(x)_n\\right) \\end{aligned} \\tag 5 $$\ncompute $\\frac{\\partial \\ell}{\\partial b^{(k)}}$ $$ \\begin{aligned} \\frac{\\partial \\ell}{\\partial b_{i}^{(k)}} \u0026amp;= \\frac{\\partial \\ell}{\\partial a^{(k)}(x)_i} \\cdot \\frac{\\partial a^{(k)}(x)_i}{\\partial b_i^{(k)}} \\\\ \u0026amp;= \\frac{\\partial \\ell}{\\partial a^{(k)}(x)_i} \\cdot 1 \\\\ \u0026amp;= \\frac{\\partial \\ell}{\\partial a^{(k)}(x)_i} \\end{aligned} $$\nput it into vector form:\n$$ \\frac{\\partial \\ell}{\\partial b^{(k)}} = \\frac{\\partial \\ell}{\\partial a^{(k)}(x)} = \\frac{\\partial -\\log f(x)_y}{\\partial a^{(k)}(x)} \\tag 6 $$\nBack-propagation Precedure (using SGD) Summary For each data (x,y):\n  Forward progagation: Compute $a^{(k)}(x), h^{(k)}(x)$, loss.\n  Back Propagation:\n Compute output gradient: $$ \\nabla_{a^{(L+1)}(x)} -\\log f(x)_y = f(x) - e(y)$$ for $k = L+1$ to $1$:  Compute the gradients of parameters: $$ \\begin{aligned} \\nabla_{w^{(k)}(x)} -\\log f(x)y \u0026amp;= (\\nabla{a^{(k)}(x)} -\\log f(x)y) \\cdot (h^{(k-1)}(x))^T \\\\ \\nabla{b^{(k)}(x)} -\\log f(x)y \u0026amp;= \\nabla{a^{(k)}(x)} -\\log f(x)_y \\end{aligned} $$ Compute the gradients of hidden layers: $$ \\begin{aligned} \\nabla_{h^{(k-1)}(x)} -\\log f(x)y \u0026amp;= (w^{(k)}(x))^T \\cdot (\\nabla{a^{(k)}(x)} -\\log f(x)y) \\\\ \\nabla{a^{(k-1)}(x)} -\\log f(x)y \u0026amp;= (\\nabla{h^{(k-1)}(x)} -\\log f(x)_y) \\odot g'(a^{(k-1)}(x)) \\end{aligned} $$      Debugging: Gradient Checking Since gradient computation can be notoriously difficult to debug and get right, even with a buggy implementation, it may not at all be apparent that anything is amiss. Gradient checking is a method for numerically checking the derivatives computed by your code to make sure that your implementation is correct.\nRecall the mathematical definition of the derivative as:\n$$ \\frac{d}{d\\theta}J(\\theta) = \\lim_{\\epsilon \\rightarrow 0} \\frac{J(\\theta+ \\epsilon) - J(\\theta-\\epsilon)}{2 \\epsilon}$$\nSuppose we have a function $g_i(\\theta)$ that computes $\\textstyle \\frac{\\partial}{\\partial \\theta_i} J(\\theta)$; we’d like to check if $g_i$ is outputting correct derivative values. We would choose a very small $\\epsilon$ and choose a threshold $\\delta$ and check if the following is true:\n$$ | \\frac{J(\\theta+ \\epsilon) - J(\\theta-\\epsilon)}{2 \\epsilon} - g_i(\\theta)| \u0026lt; \\delta$$\nDropout Dropout refers to ignoring units during the training phase of certain set of neurons which is chosen at random. “ignoring” means these units are not considered during a particular forward or backward pass. More technically, at each training stage, individual nodes are either dropped out of the net with probability $1-p$ or kept with probability $p$, so that a reduced network is left; incoming and outgoing edges to a dropped-out node are also removed.\nDropout is an approach of regularization in neural networks which helps reducing interdependent learning amongst the neurons.\nTraining Phase \u0026amp; Testing Phase: For each hidden layer, for each training sample, for each iteration, ignore (zero out) a random fraction, $p$, of nodes (and corresponding activations).\nIn testing phase, we won\u0026rsquo;t use dropout.\nNote: Dropout roughly doubles the number of iterations required to converge. However, training time for each epoch is less.\nEarly Stopping Early stopping is a form of regularization used to avoid overfitting when training a learner with an iterative method, such as gradient descent. Such methods update the learner so as to make it better fit the training data with each iteration. Up to a point, this improves the learner\u0026rsquo;s performance on data outside of the training set. Past that point, however, improving the learner\u0026rsquo;s fit to the training data comes at the expense of increased generalization error. Early stopping rules provide guidance as to how many iterations can be run before the learner begins to over-fit.\nSGD with Converge Theoretically. SGD would converge if it satisfies\n$$ \\sum_{t=1}^{+\\infty} \\eta_t = +\\infty \\tag 7$$ $$\\sum_{t=1}^{+\\infty} \\eta_t^2 \u0026lt; \\infty \\tag 8$$\nwhere $\\eta$ is the learning rate.\n Reference:\n https://en.wikipedia.org/wiki/Backpropagation http://deeplearning.stanford.edu/tutorial/supervised/DebuggingGradientChecking/ https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5 https://en.wikipedia.org/wiki/Early_stopping https://www.researchgate.net/figure/Cross-validation-Early-stopping-Principe-2000_fig4_228469923 https://towardsdatascience.com/why-deep-learning-is-needed-over-traditional-machine-learning-1b6a99177063  ","permalink":"https://tangliyan.com/blog/posts/dl/","summary":"Deep Learning v.s. Machine Learning The major difference between Deep Learning and Machine Learning technique is the problem solving approach. Deep Learning techniques tend to solve the problem end to end, where as Machine learning techniques need the problem statements to break down to different parts to be solved first and then their results to be combine at final stage.\nForward Propagation The general procedure is the following:\n$$ \\begin{aligned} a^{(1)}(x) \u0026amp;= w^{(1)^T} \\cdot x + b^{(1)} \\\\ h^{(1)}(x) \u0026amp;= g_1(a^{(1)}(x)) \\\\ a^{(2)}(x) \u0026amp;= w^{(2)^T} \\cdot h^{(1)}(x) + b^{(2)} \\\\ h^{(2)}(x) \u0026amp;= g_2(a^{(2)}(x)) \\\\ \u0026amp;\u0026hellip;\u0026hellip; \\\\ a^{(L+1)}(x) \u0026amp;= w^{(L+1)^T} \\cdot h^{(L)}(x) + b^{(L+1)} \\\\ h^{(L+1)}(x) \u0026amp;= g_{L+1}(a^{(L+1)}(x)) \\end{aligned} $$","title":"Intro to Deep Learning and Backpropagation"},{"content":"Log-Linear model Let $x$ be an example, and let $y$ be a possible label for it. A log-linear model assumes that\n$$ p(y | x ; w)=\\frac{\\exp [\\sum_{j=1}^J w_{j} F_{j}(x, y)]}{Z(x, w)} $$\nwhere the partition function\n$$ Z(x, w)=\\sum_{y^{\\prime}} \\exp [\\sum_{j=1}^J w_{j} F_{j}\\left(x, y^{\\prime}\\right)] $$\nNote that in $\\sum_{y^{\\prime}}$, we make a summation over all possible $y$. Therefore, given $x$, the label predicted by the model is\n$$ \\hat{y}=\\underset{y}{\\operatorname{argmax}} p(y | x ; w)=\\underset{y}{\\operatorname{argmax}} \\sum_{j=1}^J w_{j} F_{j}(x, y) $$\nEach expression $F_j(x, y)$ is called a feature-function. You can think of it as the $j$-th feature extracted from $(x,y)$.\nRemark of the log-linear model:\n  a linear combination $\\sum_{j=1}^J w_{j} F_{j}(x, y)$ can take any positive or negative real value; the exponential makes it positive.\n  The division makes the result $p(y | x ; w)$ between 0 and 1, i.e. makes them be valid probabilities.\n  Conditional Random Fields (CRF) Last time, we talked about Markov Random Fields. In this post, we are going to discuss Conditional Random Fields, which is an important special case of Markov Random Fields arises when they are applied to model a conditional probability distribution $p(y|x)$, where $x$ and $y$ are vactor-valued variables.\nFormal definition of CRF Formally, a CRF is a Markov network which specifies a conditional distribution\n$$P(y\\mid x) = \\frac{1}{Z(x)} \\prod_{c \\in C} \\phi_c(x_c,y_c)$$\nwith partition function\n$$Z = \\sum_{y \\in \\mathcal{Y}} \\prod_{c \\in C} \\phi_c(x_c,y_c)$$\nwe further assume that the factors $\\phi_c(x_c,y_c)$ (maximal cliques) are of the form\n$$\\phi_c(x_c,y_c) = \\exp[w_c^T f_c(x_c, y_c)]$$\nSince we require our potential function $\\phi$ to be non-negative, it\u0026rsquo;s natural to use the exponential function. $f_c(x_c, y_c)$ can be an arbitrary set of features describing the compatibility between $x_c$ and $y_c$. Note that these feature functions could be designed by manually doing feature engineering or using deep learning, LSTM, etc.\nLog-linear model to linear-CRF As a remainder, let $x$ be an example, and let $y$ be a possible label for it. Then a log-linear model assumes that\n$$ p(y | x ; w)=\\frac{\\exp [\\sum_{j=1}^J w_{j} F_{j}(x, y)]}{Z(x, w)} $$\nFrom now on, we use the bar notation for sequences. Then to linear-CRF, we write the above equation as\n$$ \\begin{aligned} p(\\bar y | \\bar x; w) \u0026amp;= \\frac{\\exp [\\sum_{j=1}^J w_{j} F_{j}(\\bar x, \\bar y)]}{Z(\\bar x, w)}\\\\ \u0026amp;= \\frac{\\exp [\\sum_{j=1}^J w_{j} \\sum_{i=2}^{T} f_j (y_{i-1}, y_i, \\bar x)]}{Z(\\bar x, w)} \u0026amp;\u0026amp;\\quad(1) \\end{aligned} $$\nwhere $y$ can take values from ${1,2,\u0026hellip;,m}$. Here is an example:\nAssume we have a sequence $\\bar x = (x_1, x_2, x_3, x_4)$ and the corresponding hidden sequence $\\bar y = (y_1, y_2, y_3, y_4)$.\nWe can divide each feature-function $F_j(\\bar x, \\bar y)$ into fuctions for each maximal clique. That is,\n$$ F_j(\\bar x, \\bar y) = \\sum_{i=2}^{T} f_j (y_{i-1}, y_i, \\bar x) \\tag {1.1}$$\nPerticularly, from the above figure, since we have $3$ maximal cliques, so\n$$ F_j(\\bar x, \\bar y) = f_j(y_1, y_2, \\bar x) + f_j(y_2, y_3, \\bar x) + f_j(y_3, y_4, \\bar x)$$\nIf we extract $J$ feature functions from the $(\\bar x, \\bar y)$ pair, then it becomes\n$$\\sum_{j=1}^J w_{j} F_{j}(x, y) = \\sum_{j=1}^J w_{j} \\sum_{i=2}^{T} f_j (y_{i-1}, y_i, \\bar x)$$\nInference problem for CRF  Goal: given a sequence $\\bar x$, and parameter $w$, find the best hidden sequence $\\bar y$. The condition probability of $\\bar y$ is  $$ p(\\bar y | \\bar x; w) = \\frac{\\exp [\\sum_{j=1}^J w_{j} \\sum_{i=2}^{T} f_j (y_{i-1}, y_i, \\bar x)]}{Z(\\bar x, w)} $$\nOur objective is（check that the objective of CRF is the objective of Log-Linear model described above）:\n$$ \\begin{aligned} \\hat{y} \u0026amp;= \\underset{\\bar y}{\\operatorname{argmax}} p(\\bar y | \\bar x ; w) \u0026amp;\u0026amp;(2)\\\\ \u0026amp;= \\underset{\\bar y}{\\operatorname{argmax}} \\sum_{j=1}^J w_{j} \\sum_{i=2}^{T} f_j (y_{i-1}, y_i, \\bar x) \u0026amp;\u0026amp;(3) \\\\ \u0026amp;= \\underset{\\bar y}{\\operatorname{argmax}} \\sum_{i=2}^{T} g_i(y_{i-1}, y_i) \u0026amp;\u0026amp; (4) \\end{aligned} $$\nNote:\n  $(2) \\to (3)$: we can ignore the denominator since it stays the same for all possible $\\bar y$. Exponential function won\u0026rsquo;t affect our objective.\n  We set $$g_i(y_{i-1}, y_i) = \\sum_{j=1}^J w_{j} \\cdot f_j (y_{i-1}, y_i, \\bar x) \\tag 5$$\n  Based on our objective in $(5)$, we want to find the best path from $y_1$ to $y_T$ such that the objective function is maximized. Clearly, we can use Dynamic Programming (DP) here.\nLet $u(k,v)$ denote the score of the best path from $t=1$ to $t=k$, where the tag of time $k$ is $v$. Then the recursion formula can be easily visualized from the above figure and we can write it as\n$$ u(k,v) = \\underset{s}{\\operatorname{max}} [u(k-1, s) + g_k(s,v)]$$\nwhere $s$ takes values from states ${1,2,\u0026hellip;,m}$. The maximum of the objective is $\\operatorname{max} {u(T,1), u(T,2), \u0026hellip;, u(T,m)}$.\n  Time complexity: $O(mT) \\cdot O(m) = O(m^2 T)$.\n  Space complexity: $O(mT)$ since we need to track the path of the best sequence $\\bar y$.\n  Learning problem for CRF Goal: Given the data set $D = { ({x^{(1)}}, {y^{(1)}}), \u0026hellip;, ({x^{(n)}}, {y^{(n)}})}$, we want to find parameter $w$ to maximize $p(D|w)$. That is,\n$$ \\begin{aligned} \\hat{w}_{MLE} \u0026amp;= \\underset{w}{\\operatorname{max}} p(D|w) \\\\ \u0026amp;= \\underset{w}{\\operatorname{max}} \\prod_{i=1}^{n} p( {y^{(i)}} | {x^{(i)}}; w) \\end{aligned} $$\nThat is, we need to take derivatives and then use the gradient descent method.\nLearning problem for general Log-Linear model $$p(\\bar y, \\bar x; w) = \\frac{\\exp [\\sum_{j=1}^J w_{j} F_{j}(x, y)]}{Z(x, w)}$$\nTake the derivative with respect to $w_j$:\n$$ \\begin{aligned} \\frac{\\partial}{\\partial w_j} [\\log p(y| x; w)] \u0026amp;= \\frac{\\partial}{\\partial w_j} [\\sum_{j=1}^J w_{j} F_j(x, y) - \\log Z(x,w)] \\\\ \u0026amp;= F_j(x, y) - \\frac{1}{Z(x,w)} \\cdot \\frac{\\partial}{\\partial w_j} Z(x,w) \u0026amp;\u0026amp;(6) \\end{aligned} $$\nwhere\n$$ \\begin{aligned} \\frac{\\partial}{\\partial w_j} Z(x,w) \u0026amp;= \\frac{\\partial}{\\partial w_j} \\sum_{y^{\\prime}} \\exp [\\sum_{j=1}^J w_{j} F_{j}\\left(x, y^{\\prime}\\right)] \\\\ \u0026amp;= \\sum_{y^{\\prime}} \\frac{\\partial}{\\partial w_j} [\\exp \\sum_{j=1}^J w_{j} F_{j}\\left(x, y^{\\prime}\\right)] \\\\ \u0026amp;= \\sum_{y^{\\prime}} [\\exp \\sum_{j=1}^J w_{j} F_{j}\\left(x, y^{\\prime}\\right)] \\cdot F_{j}\\left(x, y^{\\prime}\\right) \u0026amp;\u0026amp;(7) \\end{aligned} $$\nCombining $(6)$ and $(7)$, we have\n$$ \\begin{aligned} \\frac{\\partial}{\\partial w_j} [\\log p(y| x; w)] \u0026amp;= F_{j}\\left(x, y\\right) - \\frac{1}{Z(x,w)} \\sum_{y^{\\prime}} F_{j}\\left(x, y^{\\prime}\\right) [\\exp \\sum_{j=1}^J w_{j} F_{j}\\left(x, y^{\\prime}\\right)] \\\\ \u0026amp;= F_{j}\\left(x, y\\right) - \\sum_{y^{\\prime}} F_{j}\\left(x, y^{\\prime}\\right) \\frac{\\exp \\sum_{j=1}^J w_{j} F_{j}\\left(x, y^{\\prime}\\right)}{Z(x,w)} \\\\ \u0026amp;= F_{j}\\left(x, y\\right) - \\sum_{y^{\\prime}} F_{j}\\left(x, y^{\\prime}\\right) \\cdot p(y^{\\prime}|x;w) \\\\ \u0026amp;= F_{j}\\left(x, y\\right) - E_{y^{\\prime} \\sim p(y^{\\prime}|x;w)}[F_{j}\\left(x, y^{\\prime}\\right)] \u0026amp;\u0026amp;(8) \\end{aligned} $$\nLearning problem for CRF We can edit $(8)$ to get the partial derivative for CRF:\n$$ \\begin{aligned} \\frac{\\partial}{\\partial w_j} [\\log p(\\bar y| \\bar x; w)] \u0026amp;= F_{j}\\left(\\bar x, \\bar y\\right) - E_{\\bar y^{\\prime} \\sim p(\\bar y^{\\prime}|x;w)}[F_{j}\\left(x,\\bar y^{\\prime}\\right)] \u0026amp;\u0026amp;(9)\\\\ \u0026amp;= F_{j}\\left(\\bar x, \\bar y\\right) - E_{\\bar y^{\\prime}}[\\sum_{i=2}^T f_j(y_{i-1}, y_i, \\bar x)] \u0026amp;\u0026amp;(10)\\\\ \u0026amp;= F_{j}\\left(\\bar x, \\bar y\\right) - \\sum_{i=2}^T E_{\\bar y^{\\prime} }[f_j(y_{i-1}, y_i, \\bar x)] \u0026amp;\u0026amp;(11)\\\\ \u0026amp;= F_{j}\\left(\\bar x, \\bar y\\right) - \\sum_{i=2}^T E_{y_{i-1}, y_i}[f_j(y_{i-1}, y_i, \\bar x)] \u0026amp;\u0026amp;(12)\\\\ \u0026amp;= F_{j}\\left(\\bar x, \\bar y\\right) - \\sum_{i=2}^T \\sum_{y_{i-1}} \\sum_{y_{i}} f_j(y_{i-1}, y_i, \\bar x) \\cdot p(y_{i-1}, y_i| \\bar x; w) \u0026amp;\u0026amp;(13) \\end{aligned} $$\nNote:\n  $(9) \\to (10)$: Use equation $(1)$.\n  $(11) \\to (12)$: each term $f_j(y_{i-1}, y_i, \\bar x)$ is only related to $y_{i-1}$ and $y_i$.\n  In the equation $(13)$, the only unknown term is $p(y_{i-1}, y_i| \\bar x; w)$. Let\u0026rsquo;s now see how to compute it.\n  Compute $Z(\\bar x, w)$ $$ \\begin{aligned} Z(\\bar x, w) \u0026amp;= \\sum_{\\bar y} \\exp \\left[\\sum_{j=1}^J w_{j} F_{j}\\left(\\bar x, \\bar y \\right)\\right] \\\\ \u0026amp;= \\sum_{\\bar y} \\exp \\left[\\sum_{j=1}^J w_{j} \\sum_{i=2}^T f_j(y_{i-1}, y_i, \\bar x)\\right] \\\\ \u0026amp;= \\sum_{\\bar y} \\left[\\exp \\sum_{i=2}^T g_i(y_{i-1}, y_i)\\right] \u0026amp;\u0026amp; (14) \\end{aligned} $$\nWe see the term $g_i(y_{i-1}, y_i)$ in the equation $(14)$ again, and $(14)$ is the sum of $\\left[\\exp \\sum_{i=2}^T g_i(y_{i-1}, y_i)\\right]$ over all $y$. If we list all the possibilities, the time complexity is $O(m^T)$, which is not acceptable. So we should solve it in a similar way like what we did in the inference section (Dynamic Programming). There are two ways to solve it: forward algorithm and backward algorithm. Note that this is very similar to HMM we discussed before.\nForward Algorithm Let $\\alpha(k,v)$ denote the sum of all possible paths from $t=1$ to $t=k$, where the tag of time $k$ is $v$. Then the recursion formula can be easily visualized from the above figure and we can write it as\n$$ \\alpha(k,v) = \\underset{s}{\\operatorname{max}} \\left[\\alpha(k-1, s) \\cdot \\text{exp}, g_k(s,v)\\right]$$\nwhere $s \\in {1,2,\u0026hellip;,m}$. Then, we can write $Z(\\bar x, w)$ as\n$$ Z(\\bar x, w) = \\sum_{s=1}^m \\alpha(T, s)$$\nBackward Algorithm Let $\\beta(k,v)$ denote the sum of all possible paths from $t=k$ to $t=T$, where the tag of time $t$ is $v$. Then the recursion formula can be easily visualized from the above figure and we can write it as\n$$ \\beta(k,v) = \\underset{s}{\\operatorname{max}} \\left[\\beta(k+1, s) \\cdot \\text{exp}, g_{k+1}(v,s)\\right]$$\nwhere $s \\in {1,2,\u0026hellip;,m}$. Then, we can write $Z(\\bar x, w)$ as\n$$ Z(\\bar x, w) = \\sum_{s=1}^m \\beta(1, s)$$\nCompute $p(y_k=u|\\bar x; w)$ From the figure above, we can divide it into the product of a forward term and a backward term:\n$$ p(y_k=u|\\bar x; w) = \\frac{\\alpha(k,u)\\cdot \\beta(k,u)}{Z(\\bar x, w)}$$\nwhere\n$$ Z(\\bar x, w) = \\sum_u \\alpha(k,u)\\cdot \\beta(k,u)$$\n Note that we can also compute $Z(\\bar x, w)$ by write it as a product of an $\\alpha$ term and a $\\beta$ term.  Compute $p(y_k=u, y_{k+1}=v|\\bar x; w)$ From the figure above, we can divide it into the product of a forward term, a backward term, and an term that represent the path going from $y_k=u$ to $y_{k+1}= v$:\n$$ p(y_k=u, y_{k+1}=v|\\bar x; w) = \\frac{\\alpha(k,u)\\cdot [\\text{exp} , g_{k+1} (u,v)] \\cdot\\beta(k+1,v)}{Z(\\bar x, w)}$$\nwhere\n$$ Z(\\bar x, w) = \\sum_u \\sum_v \\alpha(k,u)\\cdot [\\text{exp} , g_{k+1} (u,v)] \\cdot\\beta(k+1,v)$$\nNow go back to where we stopped (equation $(13)$), and use what we just derived above, we have\n$$ \\begin{aligned} \\frac{\\partial}{\\partial w_j} [\\log p(\\bar y| \\bar x; w)] \u0026amp;= F_{j}\\left(\\bar x, \\bar y\\right) - \\sum_{i=2}^T \\sum_{y_{i-1}} \\sum_{y_{i}} f_j(y_{i-1}, y_i, \\bar x) \\cdot p(y_{i-1}, y_i| \\bar x; w) \\\\ \u0026amp;= F_{j}\\left(\\bar x, \\bar y\\right) - \\sum_{i=2}^T \\sum_{y_{i-1}} \\sum_{y_{i}} f_j(y_{i-1}, y_i, \\bar x) \\cdot \\frac{\\alpha(i-1,y_{i-1})\\cdot [\\text{exp} , g_{i} (y_{i-1},y_i)] \\cdot\\beta(i,y_1)}{Z(\\bar x, w)} \u0026amp;\u0026amp;(15) \\end{aligned} $$\nNow every term in the equation $(15)$ is known. So we can use SGD to update the parameter $w$.\n Reference:\n https://ermongroup.github.io/cs228-notes/representation/undirected/ http://cseweb.ucsd.edu/~elkan/250B/CRFs.pdf http://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf http://cseweb.ucsd.edu/~elkan/250Bfall2007/loglinear.pdf  ","permalink":"https://tangliyan.com/blog/posts/crf/","summary":"Log-Linear model Let $x$ be an example, and let $y$ be a possible label for it. A log-linear model assumes that\n$$ p(y | x ; w)=\\frac{\\exp [\\sum_{j=1}^J w_{j} F_{j}(x, y)]}{Z(x, w)} $$\nwhere the partition function\n$$ Z(x, w)=\\sum_{y^{\\prime}} \\exp [\\sum_{j=1}^J w_{j} F_{j}\\left(x, y^{\\prime}\\right)] $$\nNote that in $\\sum_{y^{\\prime}}$, we make a summation over all possible $y$. Therefore, given $x$, the label predicted by the model is\n$$ \\hat{y}=\\underset{y}{\\operatorname{argmax}} p(y | x ; w)=\\underset{y}{\\operatorname{argmax}} \\sum_{j=1}^J w_{j} F_{j}(x, y) $$","title":"Log-Linear Model, Conditional Random Field(CRF)"},{"content":"Gaussian mixture model (GMM) A Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters.\nInterpretation from geometry $p(x)$ is a weighted sum of multiple Gaussian distribution.\n$$p(x)=\\sum_{k=1}^{K} \\alpha_{k} \\cdot \\mathcal{N}\\left(x | \\mu_{k}, \\Sigma_{k}\\right) $$\nInterpretation from mixture model setup:\n  The total number of Gaussian distribution $K$.\n  $x$, a sample (observed variable).\n  $z$, the distribution of the sample $x$ (a latent variable), where\n  $z \\in {c_1, c_2, \u0026hellip;, c_K}$.\n  $\\sum_{k=1}^K p(z=c_k)= 1$. We denote $p(z=c_k)$ by $p_k$.\n    Mixture models are usually generative models, which means new data can be drawn from the distribution of models. Specifically, in the Gaussian Mixture Model (GMM), a new data is generated by first select a class $c_k$ based on the probability distribution of all classes $c$, and then draw a value from the Gaussian distribution of that class. Therefore, we could write $p(x)$ as the following\n$$ \\begin{aligned} p(x) \u0026amp;= \\sum_z p(x,z) \\\\ \u0026amp;= \\sum_{k=1}^{K} p(x, z=c_k) \\\\ \u0026amp;= \\sum_{k=1}^{K} p(z=c_k) \\cdot p(x|z=c_k) \\\\ \u0026amp;= \\sum_{k=1}^{K} p_k \\cdot \\mathcal{N}(x | \\mu_{k}, \\Sigma_{k}) \\end{aligned} $$\nWe see that two ways of interpretation reach to the same result.\nGMM Derivation set up   X: observed data, where $X = (x_1, x_2, \u0026hellip;, x_N)$\n  $\\theta$: parameter of the model, where $\\theta={p_{1}, p_{2}, \\cdots, p_{K}, \\mu_{1}, \\mu_{2}, \\cdots, \\mu_{K}, \\Sigma_{1}, \\Sigma_{2}, \\cdots, \\Sigma_{K}}$\n  $p(x) = \\sum_{k=1}^{K} p_k \\cdot \\mathcal{N}(x | \\mu_{k}, \\Sigma_{k})$.\n  $p(x,z) = p(z) \\cdot p(x|z) = p_z \\cdot \\mathcal{N}(x | \\mu_{z}, \\Sigma_{z})$\n  $p(z|x) = \\frac{p(x,z)}{p(x)} = \\frac{p_z \\cdot \\mathcal{N}(x | \\mu_{z}, \\Sigma_{z})}{\\sum_{k=1}^K p_z \\cdot \\mathcal{N}(x | \\mu_{z}, \\Sigma_{z})}$\n  Solve by MLE $$ \\begin{aligned} \\hat{\\theta}_{MLE} \u0026amp;= \\underset{\\theta}{\\operatorname{argmax}} p(X) \\\\ \u0026amp;=\\underset{\\theta}{\\operatorname{argmax}} \\log p(X) \\\\ \u0026amp;=\\underset{\\theta}{\\operatorname{argmax}} \\sum_{i=1}^{N} \\log p\\left(x_{i}\\right) \\\\ \u0026amp;=\\underset{\\theta}{\\operatorname{argmax}} \\sum_{i=1}^{N} \\log , [\\sum_{i=1}^{K} p_{k} \\cdot \\mathcal{N}\\left(x_{i} | \\mu_{k}, \\Sigma_{k}\\right)] \\end{aligned} $$\nI mentioned in the previous posts multiple times that the log of summation is very hard to solve. Therefore, we need somehow use approximation methods to solve for optimal $\\theta$. Since $Z$ is a hidden variable, it\u0026rsquo;s natural to use EM Algorithm.\nSolve by EM Algorithm Check my previous post to see how EM Algorithm is derived, Here is a briefly review of the Algorithm:\n Initialize peremeters $\\theta_0$.  Iterate between steps 2 and 3 until convergence:\nExpectation (E) step:  $$ \\begin{aligned} Q(\\theta, \\theta^{(t)}) \u0026amp;= \\sum_Z P(Z|X,\\theta^{(t)}) \\cdot \\log p(X,Z|\\theta) \\\\ \u0026amp;= E_{Z \\sim P(Z|X,\\theta^{(t)})}[\\log p(X,Z|\\theta)] \\end{aligned} $$\nMaximization (M) step:  Compute parameters maximizing $Q(\\theta, \\theta^{(t)})$ found on the $E$ step and then update parameters to $\\theta^{(t+1)}$. That is\n$$ \\theta^{(t+1)} = \\underset{\\theta}{\\operatorname{argmax}} Q(\\theta, \\theta^{(t)})$$\nThe derivation is the following:\nE step:\n$$ \\begin{aligned} Q(\\theta, \\theta^{(t)}) \u0026amp;= E_{Z \\sim P(Z|X,\\theta^{(t)})}[\\log p(X,Z|\\theta)] \\\\ \u0026amp;= \\sum_Z \\log p(X,Z|\\theta) \\cdot P(Z|X,\\theta^{(t)})\\\\ \u0026amp;= \\sum_{Z}\\left[\\log \\prod_{i=1}^{N} p\\left(x_{i}, z_{i} | \\theta\\right)\\right] \\prod_{i=1}^{N} p\\left(z_{i} | x_{i}, \\theta^{(t)}\\right) \\\\ \u0026amp;= \\sum_{Z}\\left[\\sum_{i=1}^{N} \\log p\\left(x_{i}, z_{i} | \\theta\\right)\\right] \\prod_{i=1}^{N} p\\left(z_{i} | x_{i}, \\theta^{(t)}\\right) \u0026amp;\u0026amp;(1)\\ \\end{aligned} $$\nWe can expand equation $(1)$ and try to simplify the first term first:\n$$ \\begin{aligned} \u0026amp; \\quad \\sum_{Z} \\log p\\left(x_{1}, z_{1} | \\theta\\right) \\cdot \\prod_{i=1}^{N} p\\left(z_{i} | x_{i}, \\theta^{(t)}\\right) \\\\ \u0026amp;= \\sum_{z_1}\\sum_{z_2}\u0026hellip;\\sum_{z_N} \\log p\\left(x_{1}, z_{1} | \\theta\\right) \\cdot \\prod_{i=1}^{N} p\\left(z_{i} | x_{i}, \\theta^{(t)}\\right) \\\\ \u0026amp;= \\sum_{z_1}\\sum_{z_2}\u0026hellip;\\sum_{z_N} \\log p\\left(x_{1}, z_{1} | \\theta\\right) \\cdot p\\left(z_{1} | x_{1}, \\theta^{(t)}\\right) \\cdot \\prod_{i=2}^{N} p\\left(z_{i} | x_{i}, \\theta^{(t)}\\right) \u0026amp;\u0026amp; (2)\\\\ \u0026amp;= \\sum_{z_1} \\log p\\left(x_{1}, z_{1} | \\theta\\right) \\cdot p\\left(z_{1} | x_{1}, \\theta^{(t)}\\right) \\sum_{z_2}\u0026hellip;\\sum_{z_N} , \\prod_{i=2}^{N} p\\left(z_{i} | x_{i}, \\theta^{(t)}\\right) \u0026amp;\u0026amp; (3)\\\\ \u0026amp;= \\sum_{z_1} \\log p\\left(x_{1}, z_{1} | \\theta\\right) \\cdot p\\left(z_{1} | x_{1}, \\theta^{(t)}\\right) \\sum_{z_2}\u0026hellip;\\sum_{z_N} , p\\left(z_{2} | x_{2}, \\theta^{(t)}\\right) \u0026hellip; p\\left(z_{N} | x_{N}, \\theta^{(t)}\\right) \u0026amp;\u0026amp; (4)\\\\ \u0026amp;= \\sum_{z_1} \\log p\\left(x_{1}, z_{1} | \\theta\\right) \\cdot p\\left(z_{1} | x_{1}, \\theta^{(t)}\\right) \\sum_{z_2} p\\left(z_{2} | x_{2}, \\theta^{(t)}\\right) \u0026hellip;\\sum_{z_N} p\\left(z_{N} | x_{N}, \\theta^{(t)}\\right) \u0026amp;\u0026amp; (5)\\\\ \u0026amp;= \\sum_{z_1} \\log p\\left(x_{1}, z_{1} | \\theta\\right) \\cdot p\\left(z_{1} | x_{1}, \\theta^{(t)}\\right) \u0026amp;\u0026amp; (6) \\ \\end{aligned} $$\nRemark:\n  $(2) \\to (3)$: since the term $\\log p\\left(x_{1}, z_{1} | \\theta\\right) \\cdot p\\left(z_{1} | x_{1}, \\theta^{(t)}\\right)$ is only related to $\\sum_{z_1}$, we can pull it to the front.\n  $(4) \\to (5)$: we use the same trick as in $(2) \\to (3)$.\n  $(5) \\to (6)$: Clearly, every summation of products is one except the first summation.\n  Therefore, the rest of terms can be simplified the same way. So we have\n$$ \\begin{aligned} Q(\\theta, \\theta^{(t)}) \u0026amp;= \\sum_{Z}\\left[\\sum_{i=1}^{N} \\log p\\left(x_{i}, z_{i} | \\theta\\right)\\right] \\prod_{i=1}^{N} p\\left(z_{i} | x_{i}, \\theta^{(t)}\\right)\\\\ \u0026amp;= \\sum_{z_1} \\log p\\left(x_{1}, z_{1} | \\theta\\right) \\cdot p\\left(z_{1} | x_{1}, \\theta^{(t)}\\right) + \u0026hellip; + \\sum_{z_N} \\log p\\left(x_{N}, z_{N} | \\theta\\right) \\cdot p\\left(z_{N} | x_{N}, \\theta^{(t)}\\right) \\\\ \u0026amp;= \\sum_{i=1}^N \\sum_{z_i} \\log p\\left(x_{i}, z_{i} | \\theta\\right) \\cdot p\\left(z_{i} | x_{i}, \\theta^{(t)}\\right) \\\\ \u0026amp;= \\sum_{k=1}^K \\sum_{i=1}^N \\log p\\left(x_{i}, z_{k} | \\theta\\right) \\cdot p\\left(z_{i} = c_k | x_{i}, \\theta^{(t)}\\right) \\\\ \u0026amp;= \\sum_{k=1}^K \\sum_{i=1}^N [\\log p_k + \\log \\mathcal{N}(x_i | \\mu_{k}, \\Sigma_{k})] \\cdot p\\left(z_{i} = c_k | x_{i}, \\theta^{(t)}\\right) \\ \\end{aligned} $$\nwhere\n$$ p\\left(z_{i} = c_k | x_{i}, \\theta^{(t)}\\right) = \\frac{p_{k}^{(t)} \\mathcal{N}\\left(x_{i} | \\mu_{z_{i}}^{(t)}, \\Sigma_{z_{i}}^{(t)}\\right)}{\\sum_{k} p_{k}^{(t)} \\mathcal{N}\\left(x_{i} | \\mu_{k}^{(t)}, \\Sigma_{k}^{(t)}\\right)} $$\nM step:\nWe can update $p^{(t+1)}, \\mu^{(t+1)}, \\Sigma^{(t+1)}$ separately. We first find $p^{(t+1)}$, where $p^{(t+1)} = (p_1^{(t+1)}, p_2^{(t+1)}, \u0026hellip;, p_k^{(t+1)})$:\n$$ p^{(t+1)} = \\underset{p}{\\operatorname{argmax}} \\sum_{k=1}^K \\sum_{i=1}^N \\log p_k \\cdot p (z_{i} = c_k | x_{i}, \\theta^{(t)}) $$\nwith the constraint $ \\sum_{k=1}^K p_k = 1$. Clearly, this is a constrained optimization problem. So we are going to solve it by introducing a Lagrange multiplier.\n$$ \\begin{aligned} L\\left(p, \\lambda \\right) \u0026amp;=\\sum_{k=1}^{K} \\sum_{i=1}^{N} \\log p_{k} p\\left(z_{i}=c_k | x_{i}, \\theta^{(t)}\\right) + \\lambda\\left(\\sum_{k=1}^{K} p_{k} - 1\\right) \u0026amp;\u0026amp; (7)\\ \u0026amp;\\frac{\\partial L}{\\partial p_{k}} \u0026amp;= \\sum_{i=1}^{N} \\frac{1}{p_{k}} p\\left(z_{i}=k | x_{i}, \\theta^{(t)}\\right)+\\lambda =0 \\ \\end{aligned} $$\nThen multiply on both side by $p_k$ and combine all $p_k$, we have\n$$\\sum_{i=1}^{N} p\\left(z_{i}=c_k | x_{i}, \\theta^{(t)}\\right) + p_k \\cdot \\lambda = 0 \\ \\sum_{i=1}^{N} \\sum_{k=1}^{K} p\\left(z_{i}=c_k | x_{i}, \\theta^{(t)}\\right) + \\sum_{k=1}^{K} p_k \\cdot \\lambda = 0 $$\nSince $\\sum_{k=1}^{K} p\\left(z_{i}=c_k | x_{i}, \\theta^{(t)}\\right) = 1$, and $\\sum_{k=1}^{K} p_k = 1$, we have\n$$\\lambda = -N$$\nPut $\\lambda = -N$ back to (7), we have\n$$ p_k^{(t+1)} = \\frac{1}{N} \\sum_{i=1}^{N} p\\left(z_{i}=c_k | x_{i}, \\theta^{(t)}\\right)$$\nNote that solving $\\mu^{(t+1)}$ and $\\Sigma^{(t+1)}$ is basically the same as solving $p^{(t+1)}$ except that they don\u0026rsquo;t have any constraint.\nK-means K-Means is one of the most popular \u0026ldquo;clustering\u0026rdquo; algorithms. K-means stores $k$ centroids that it uses to define clusters. A point is considered to be in a particular cluster if it is closer to that cluster\u0026rsquo;s centroid than any other centroid.\nThe Algorithm:\n  Initialize $K$ centroids.\n  Iterate until convergence:\na. Hard assign each data-point to it’s closest centroid.\nb. Move each centroid to the center of data-points assigned to it.\n  Notice that this process is very similar to the way we update parameters of GMM. And we call it an EM-style method to approximate optimal parameters.\nThe objective function is to minimize\n$$ L = \\sum_{i=1}^{N} \\sum_{k=1}^K \\gamma_{ik} \\cdot || x_i - \\mu_k||^2_2$$\nwhere $\\gamma_{ik}$ = 1 if $x_i \\in c_k$, $0$ otherwise. Note that $\\gamma_{ik}$ here is a hard label, which can only be $0$ or $1$. So GMM is a more generalized model than K-means. In K-means, we can think $\\gamma_{ik}$ as the latent variable and $\\mu$ as the parameter we want to optimize.\n Reference:\n http://people.csail.mit.edu/dsontag/courses/ml12/slides/lecture21.pdf https://www.cs.toronto.edu/~jlucas/teaching/csc411/lectures/tut8_handout.pdf https://space.bilibili.com/97068901 https://stanford.edu/~cpiech/cs221/handouts/kmeans.html Kevin P. Murphy. Machine Learning: A Probabilistic Perspective.  ","permalink":"https://tangliyan.com/blog/posts/gmm/","summary":"Gaussian mixture model (GMM) A Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters.\nInterpretation from geometry $p(x)$ is a weighted sum of multiple Gaussian distribution.\n$$p(x)=\\sum_{k=1}^{K} \\alpha_{k} \\cdot \\mathcal{N}\\left(x | \\mu_{k}, \\Sigma_{k}\\right) $$\nInterpretation from mixture model setup:\n  The total number of Gaussian distribution $K$.\n  $x$, a sample (observed variable).","title":"Gaussian mixture model (GMM), k-means"},{"content":"Probabilistic Graphical Model (PGM) Definition: A probabilistic graphical model is a probabilistic model for which a graph expresses the conditional dependence structure between random variables.\nIn general, PGM obeys following rules: $$ \\begin{aligned} \u0026amp;\\text {Sum Rule : } p\\left(x_{1}\\right)=\\int p\\left(x_{1}, x_{2}\\right) d x_{2}\\\\ \u0026amp;\\text {Product Rule : } p\\left(x_{1}, x_{2}\\right)=p\\left(x_{1} | x_{2}\\right) p\\left(x_{2}\\right)\\\\ \u0026amp;\\text {Chain Rule: } p\\left(x_{1}, x_{2}, \\cdots, x_{p}\\right)=\\prod_{i=1}^{p} p\\left(x_{i} | x_{i+1, x_{i+2}} \\ldots x_{p}\\right)\\\\ \u0026amp;\\text {Bayesian Rule: } p\\left(x_{1} | x_{2}\\right)=\\frac{p\\left(x_{2} | x_{1}\\right) p\\left(x_{1}\\right)}{p\\left(x_{2}\\right)} \\end{aligned} $$\nAs the dimension of the data increases, the chain rule is harder to compute. In fact, many models try to simplify it in some ways.\nWhy we need probabilistic graphical models Reasons:\n  They provide a simple way to visualize the structure of a probabilistic model and can be used to design and motivate new models.\n  Insights into the properties of the model, including conditional independence properties, can be obtained by inspection of the graph.\n  Complex computations, required to perform inference and learning in sophisticated models, can be expressed in terms of graphical manipulations, in which underlying mathematical expressions are carried along implicitly.\n  Three major parts of PGM   Representation: Express a probability distribution that models some real-world phenomenon.\n  Inference: Obtain answers to relevant questions from our models.\n  Learning: Fit a model to real-world data.\n  We are going to mainly focus on Representation in this post.\nRepresentation Representation: Express a probability distribution that models some real-world phenomenon.\nDirected graphical models (Bayesian networks) Directed graphical models is also known as Bayesian networks.\nIntuition:\nIn a directed graph, vertices correspond to variables $x_i$ and edges indicate dependency relationships. Once the graphical representation of a directed graph is given (directed acyclic graphs), we can easily calculate the joint probability. For example, from the figure above, we can calculate the joint probability $p(a,b,c,d,e)$ by\n$$p(a,b,c,d,e) = p(a) \\cdot p(b|a) \\cdot p(c|b,d) \\cdot p(d) \\cdot p(e|c)$$\nFormal Definition:\nA Bayesian network is a directed graph $G= (V,E)$ together with\n  A random variable $x_i$ for each node $i \\in V$.\n  One conditional probability distribution (CPD) $p(x_i \\mid x_{A_i})$ per node, specifying the probability of $x_i$ conditioned on its parents’ values.\n  Note:\n  Bayesian networks represent probability distributions that can be formed via products of smaller, local conditional probability distributions (one for each variable). Another way to say it is that each factor in the factorization of $p(a,b,c,d,e)$ is locally normalized (every factor can sum up to one).\n  Directed models are often used as generative models.\n  Undirected graphical models (Markov random fields) Undirected graphical models is also known as Markov random fields (MRFs).\nUnlike in the directed case, we cannot say anything about how one variable is generated from another set of variables (as a conditional probability distribution would do).\nIntuition:\nSuppose we have five students doing a project and we want to evaluate how well they would cooperate together. Since five people are too many to be evaluated as a whole, we devide it into small subgroups and evaluate these subgroups respectively. In fact, these small subgroups are called clique and we would introduce it later in this section.\nHere, we introduce the concept of potential function $\\phi$ to evaluete how well they would cooperate together. You can think of it as a score that measures how well a clique cooperate. Higher scores indicate better cooperation. In fact, we requie scores to be non-negative, and depending on how we define the potential functions, we would get different models. As the figure shown above, we could write $p(a,b,c,d,e)$ as\n$$p(a,b,c,d,e) = \\phi_1(a,b,c) \\cdot \\phi_2(b,d) \\cdot \\phi_3(d,e)$$\nNote that the left hand side of the queation is a probability but the right hand side is a product of potentials/ scores. To make the right hand side a valid probability, we need to introduce a normalization term $1/Z$. Hence it becomes\n$$p(a,b,c,d,e) = \\frac{1}{Z} \\cdot \\phi_1(a,b,c) \\cdot \\phi_2(b,d) \\cdot \\phi_3(d,e)$$\nHere we say $p(a,b,c,d,e)$ is globally normalized. Also, we call $Z$ a partition function, which is\n$$ Z = \\sum_{a,b,c,d,e} \\phi_1(a,b,c) \\cdot \\phi_2(b,d) \\cdot \\phi_3(d,e) \\tag 1$$\nNotice that the summation in $(1)$ is over the exponentially many possible assignments to $a,b,c,d$ and $e$. For this reason, computing $Z$ is intractable in general, but much work exists on how to approximate it.\nFormal Definition:\n  cliques: fully connected subgraphs.\n  maximal clique: A clique is a maximal clique if it is not contained in any larger clique.\n  A Markov Random Field (MRF) is a probability distribution $p$ over variables $x_{1}, \\ldots, x_{n}$ defined by an undirected graph $G$ in which nodes correspond to variables $x_{i} .$ The probability $p$ has the form\n$$p\\left(x_{1}, \\ldots, x_{n}\\right)=\\frac{1}{Z} \\prod_{c \\in C} \\phi_{c}\\left(x_{c}\\right) \\tag 2$$\nwhere $C$ denotes the set of cliques of $G,$ and each factor $\\phi_{c}$ is a non-negative function over the variables in a clique. The partition function\n$$ Z=\\sum_{x_{1}, \\ldots, x_{n}} \\prod_{c \\in C} \\phi_{c}\\left(x_{c}\\right) $$\nis a normalizing constant that ensures that the distribution sums to one.\nMarkov Properties of undirected graph   Global Markov Property: $p$ satisfies the global Markov property with respect to a graph $G$ if for any disjoint vertex subsets $A$, $B$, and $C$, such that $C$ separates $A$ and $B$, the random variables $X_A$ are conditionally independent of $X_B$ given $X_C$. Here,we say $C$ separates $A$ and $B$ if every path from a node in $A$ to a node in B passes through a node in $C$ (d-seperation).\n  Local Markov Property: $p$ satisfies the local Markov property with respect to $G$ if the conditional distribution of a variable given its neighbors is independent of the remaining nodes.\n  Pairwise Markov Property: $p$ satisfies the pairwise markov property with respect to $G$ if for any pair of non-adjacent nodes, $s,t \\in V$, we have $X_{s} \\perp X_{t} | X_{V \\backslash{s, t}}$.\n  Note:\n  A distribution $p$ that satisfies the global Markov property is said to be a Markov random field or Markov network with respect to the graph.\n  Global Markov Property $\\Rightarrow$ Local Markov Property $\\Rightarrow$Pairwise Markov Property.\n  A Markov random field reflects conditional independency since it satisfies the Local Markov Property.\n  To see whether a distribution is a Markov random field or Markov network, we have the following theorem:\nHammersley-Clifford Theorem: Suppose $p$ is a strictly positive distribution, and $G$ is an undirected graph that indexes the domain of $p$. Then $p$ is Markov with respect to G if and only if $p$ factorizes over the cliques of the graph $G$.\n  Comparison between Bayesian networks and Markov random fields   Bayesian networks effectively show causality, whereas MRFs cannot. Thus, MRFs are preferable for problems where there is no clear causality between random variables.\n  It is much easier to generate data from a Bayesian network, which is important in some applications.\n  In Markov random fields, computing the normalization constant $Z$ requires a summation over the exponentially many possible assignments. For this reason, computing $Z$ is intractable in general, but much work exists on how to approximate it.\n  Moral graph A moral graph is used to find the equivalent undirected form of a directed acyclic graph.\nThe moralized counterpart of a directed acyclic graph is formed by\n  Add edges between all pairs of non-adjacent nodes that have a common child.\n  Make all edges in the graph undirected.\n  Here is an example:\nNote that a Bayesian network can always be converted into an undirected network.\nTherefore, MRFs have more power than Bayesian networks, but are more difficult to deal with computationally. A general rule of thumb is to use Bayesian networks whenever possible, and only switch to MRFs if there is no natural way to model the problem with a directed graph\nFactor Graph A Markov network has an undesirable ambiguity from the factorization perspective. Consider the three-node Markov network in the figure (left). Any distribution that factorizes as\n$$p(x_1, x_2, x_3) \\propto \\phi(x_1,x_2,x_3) \\tag 3$$\nfor some positive function $\\phi$ is Markov with respect to this graph (check Hammersley-Clifford Theorem mentioned earlier). However, we may wish to use a more restricted parameterization, where\n$$p(x1, x2, x3) \\propto \\phi_1(x_1, x_2)\\phi_1(x_2, x_3)\\phi_1(x_1, x_3) \\tag 4$$\nThe model family in $(4)$ is smaller, and therefore may be more amenable to parameter estimation. But the Markov network formalism cannot distinguish between these two parameterizations. In order to state models more precisely, the factorization in $(2)$ can be represented directly by means of a factor graph.\nDefinition (factor graph): A factor graph is a bipartite graph $G = (V, F, E)$ in which a variable node $x_i \\in V$ is connected to a factor node $\\phi_a \\in F$ if $x_i$ is an argument to $\\phi_a$.\nAn example of a factor graph is shown on the right side of the figure above. In the figure, the circles are variable nodes, and the shaded boxes are factor nodes. Notice that, unlike the undirected graph, the factor graph depicts the factorization of the model unambiguously.\nRemark: Directed models can be thought of as a kind of factor graph, in which the individual factors are locally normalized in a special fashion so that globally $Z = 1$.\nInference Inference: Obtain answers to relevant questions from our models.\n Marginal inference: what is the probability of a given variable in our model after we sum everything else out?  $$ p(y=1) = \\sum_{x_1} \\sum_{x_2} \\cdots \\sum_{x_n} p(y=1, x_1, x_2, \\dotsc, x_n)$$\n Maximum a posteriori (MAP) inference: what is the most likely assignment to the variables in the model?  $$ \\max_{x_1, \\dotsc, x_n} p(y=1, x_1, \\dotsc, x_n)$$\nLearning Learning: Fit a model to real-world data.\n  Parameter learning: the graph structure is known and we want to estimate the parameters.\n  complete case:\n We use Maximum Likelihood Estimation to estimate parameters.    incomplete case:\n  We use EM Algorithm to approximate parameters.\n  Example: Guassian Mixture Model (GMM), Hidden Markov Model (HMM).\n      Structure learning: we want to estimate the graph, i.e., determine from data how the variables depend on each other.\n   Reference:\n Bishop, Christopher M., \u0026ldquo;Pattern Recognition and Machine Learning,\u0026rdquo; Springer, 2006. https://ermongroup.github.io/cs228-notes/ https://en.wikipedia.org/wiki/Moral_graph https://space.bilibili.com/97068901 https://zhenkewu.com/assets/pdfs/slides/teaching/2016/biostat830/lecture_notes/Lecture4.pdf https://skggm.github.io/skggm/tour https://homepages.inf.ed.ac.uk/csutton/publications/crftutv2.pdf  ","permalink":"https://tangliyan.com/blog/posts/pgm/","summary":"Probabilistic Graphical Model (PGM) Definition: A probabilistic graphical model is a probabilistic model for which a graph expresses the conditional dependence structure between random variables.\nIn general, PGM obeys following rules: $$ \\begin{aligned} \u0026amp;\\text {Sum Rule : } p\\left(x_{1}\\right)=\\int p\\left(x_{1}, x_{2}\\right) d x_{2}\\\\ \u0026amp;\\text {Product Rule : } p\\left(x_{1}, x_{2}\\right)=p\\left(x_{1} | x_{2}\\right) p\\left(x_{2}\\right)\\\\ \u0026amp;\\text {Chain Rule: } p\\left(x_{1}, x_{2}, \\cdots, x_{p}\\right)=\\prod_{i=1}^{p} p\\left(x_{i} | x_{i+1, x_{i+2}} \\ldots x_{p}\\right)\\\\ \u0026amp;\\text {Bayesian Rule: } p\\left(x_{1} | x_{2}\\right)=\\frac{p\\left(x_{2} | x_{1}\\right) p\\left(x_{1}\\right)}{p\\left(x_{2}\\right)} \\end{aligned} $$","title":"Probabilistic Graphical Model (PGM)"},{"content":"Before reading this post, make sure you are familiar with the EM Algorithm and decent among of knowledge of convex optimization. If not, please check out my previous post\n  EM Algorithm\n  convex optimization primal and dual problem\n  Let\u0026rsquo;s get started!\nConditional independence $A$ and $B$ are conditionally independent given $C$ if and only if, given knowledge that $C$ occurs, knowledge of whether $A$ occurs provides no information on the likelihood of $B$ occurring, and knowledge of whether $B$ occurs provides no information on the likelihood of $A$ occurring.\nFormally, if we denote conditional independence of $A$ and $B$ given $C$ by $(A\\perp B)\\mid C$, then by definition, we have\n$$(A\\perp B)\\mid C\\quad \\iff \\quad P(A, B\\mid C)= P(A\\mid C) \\cdot P(B\\mid C)$$\nGiven the knowledge that $C$ occurs, to show the knowledge of whether $B$ occurs provides no information on the likelihood of $A$ occurring, we have\n$$ \\begin{aligned} P(A | B ,C) \u0026amp;=\\frac{P(A , B , C)}{P(B , C)} \\\\ \u0026amp;=\\frac{P(A , B | C) \\cdot P(C)}{P(B , C)} \\\\ \u0026amp;=\\frac{P(A | C) \\cdot P(B | C) \\cdot P(C)}{P(B | C) \\cdot P(C)} \\\\ \u0026amp;=P(A | C) \\end{aligned} $$\nTwo classical cases where $X$ and $Z$ are conditionally independent Case 1 :\nFrom the above directed graph, we have $P(X,Y,Z) = P(X)\\cdot P(Y|X)\\cdot P(Z|Y)$. Hence we have\n$$ \\begin{aligned} P(Z|X,Y) \u0026amp;= \\frac{P(X,Y,Z)}{P(X,Y)}\\\\ \u0026amp;= \\frac{P(X)\\cdot P(Y|X)\\cdot P(Z|Y)}{P(X)\\cdot P(Y|X)}\\\\ \u0026amp;= P(Z|Y) \\end{aligned} $$\nTherefore, $X$ and $Z$ are conditionally independent.\nCase 2 :\nFrom the above directed graph, we have $P(X,Y,Z) = P(Y)\\cdot P(X|Y) \\cdot P(Z|Y)$. Hence we have\n$$ \\begin{aligned} P(Z|X,Y) \u0026amp;= \\frac{P(X,Y,Z)}{P(X,Y)}\\\\ \u0026amp;= \\frac{P(Y)\\cdot P(X|Y) \\cdot P(Z|Y)}{P(Y)\\cdot P(X|Y)}\\\\ \u0026amp;= P(Z|Y) \\end{aligned} $$\nTherefore, $X$ and $Z$ are conditionally independent.\nSettings of the Hidden Markov Model (HMM) The HMM is based on augmenting the Markov chain. A Markov chain is a model that tells us something about the probabilities of sequences of random variables, states, each of which can take on values from some set. A Markov chain makes a very strong assumption that if we want to predict the future in the sequence, all that matters is the current state.\nTo put it formally, suppose we have a sequence of state variables $z_1, z_2, \u0026hellip;, z_n$. Then the Markov assumption is\n$$ p(z_n | z_1z_2\u0026hellip;z_{n-1}) = p(z_n | z_{n-1}) $$\nA Markov chain is useful when we need to compute a probability for a sequence of observable events. However, in many cases the events we are interested in are hidden. For example we don’t normally observe part-of-speech (POS) tags in a text. Rather, we see words, and must infer the tags from the word sequence. We call the tags hidden because they are not observed.\nA hidden Markov model (HMM) allows us to talk about both observed events (like words that we see in the input) and hidden events (like part-of-speech tags) that we think of as causal factors in our probabilistic model. An HMM is specified by the following components:\n  A sequence of hidden states $z$, where $z_k$ takes values from all possible hidden states $Z = {1,2,..,m}$.\n  A sequence of observations $x$, where $x = (x_1, x_2, \u0026hellip;, x_n)$. Each one is drawn from a vocabulary $V$.\n  A transition probability matrix $A$, where $A$ is an $m \\times m$ matrix. $A_{ij}$ represents the probability of moving from state $i$ to state $j$: $A_{ij} = p(z_{t+1}=j| z_t=i)$, and $\\sum_{j=1}^{m} A_{ij} = 1$ for all $i$.\n  An emission probability matrix $B$, where $B$ is an $m \\times |V|$ matrix. $B_{ij}$ represents the probability of an observation $x_j$ being generated from a state $i$: $B_{ij} = P(x_t = V_j|z_t = i)$\n  An initial probability distribution $\\pi$ over states, where $\\pi = (\\pi_1, \\pi_2, \u0026hellip;, \\pi_m)$. $\\pi_i$ is the probability that the Markov chain will start in state $i$. $\\sum_{i=1}^{m} \\pi_i = 1$.\n  Given a sequence $x$ and the corresponding hidden states $z$ (like one in the picture above), we have\n$$ P(x, z|\\theta) = p(z_1) \\cdot [p(z_2|z_1)\\cdot p(z_3|z_2)\\cdot \u0026hellip; \\cdotp(z_n|z_{n-1})] \\cdot [p(x_1|z_1)\\cdot p(x_2|z_2)\\cdot \u0026hellip; \\cdot p(x_n|z_n)] \\tag 0$$\nWe get $p(z_1)$ from $\\pi$, $p(z_{k+1}|z_k)$ from $A$, and $p(x_k|z_k)$ from $B$.\nUseful probabilities $p(z_k | x)$ and $p(z_{k+1}, z_k | x)$ $p(z_k | x)$ and $p(z_{k+1}, z_k | x)$ are useful probabilities and we are going to use them later.\nIntuition: Once we have a sequence $x$, we might be interested in find the probability of any hidden state $z_k$, i.e., find probabilities $p(z_k =1| x), p(z_k =2| x), \u0026hellip;, p(z_k =m| x)$. we have the following\n$$ \\begin{aligned} p(z_k | x) \u0026amp;= \\frac{p(z_k, x)}{p(x)} \u0026amp; \u0026amp; (1)\\\\ \u0026amp;\\propto p(z_k, x) \u0026amp; \u0026amp; (2)\\ \\end{aligned} $$\nNote that from $(1)$ to (2), since $p(x)$ doesn\u0026rsquo;t change for all values of $z_k$, $p(z_k | x)$ is proportional to $p(z_k, x)$.\n$$ \\begin{aligned} p(z_k=i, x) \u0026amp;= p(z_k=i, x_{1:k}, x_{k+1:n}) \\\\ \u0026amp;= p(z_k=i, x_{1:k}) \\cdot p(x_{k+1:n}|z_k=i, x_{1:k}) \u0026amp; \u0026amp; (3)\\\\ \u0026amp;= p(z_k=i, x_{1:k}) \\cdot p(x_{k+1:n}|z_k=i) \u0026amp; \u0026amp; (4.1) \\\\ \u0026amp;= \\alpha_k(z_k=i) \\cdot \\beta_k(z_k=i) \u0026amp;\u0026amp;(4.11)\\ \\end{aligned} $$\nFrom the above graph, we see that the second term $(3)$ is the 2nd classical cases. So $x_{k+1:n}$ and $x_{1:k}$ are conditionally independent. This is why we can go from $(3)$ to $(4.1)$. We are going to use the Forward Algorithm to compute $p(z_k, x_{1:k})$, and Backward Algorithm to compute $p(x_{k+1:n}|z_k)$ later.\nWe denote $p(z_k, x_{1:k})$ by $\\alpha_k(z_k)$ and $p(x_{k+1:n}|z_k)$ by $\\beta_k(z_k)$.\nAfter we know how to calculate these two terms separately, we can calculate $p(z_k | x)$ easily by introducing a normalization term. That is,\n$$ \\begin{aligned} p(z_k = i | x) \u0026amp;= \\frac{p(z_k = i, x)}{\\sum_{j=1}^m p(z_k = j, x)} \\\\ \u0026amp;= \\frac{\\alpha_k(z_k=i)\\beta_k(z_k=i)}{\\sum_{j=1}^{m} \\alpha_k(z_k=j)\\beta_k(z_k=j)} \u0026amp;\u0026amp; (4.2) \\end{aligned} $$\nwhere $\\sum_j^m p(z_k = j, x)$ is the normalization term which makes $p(z_k = 1, x)$ take values between $0$ and $1$ for all $z_k$.\nSimilarly, we are also interested in finding $p(z_{k+1}, z_k | x)$, where\n$$ p(z_{k+1}, z_k | x) \\propto p(z_{k+1}, z_k, x)$$\nBy using the property of conditional independence, we have\n$$ \\begin{aligned} p(z_{k+1}=j, z_k=i, x) \u0026amp;= p(z_k=i, z_{k+1}=j, x_{1:k}, x_{k+1}, x_{k+2:n}) \\\\ \u0026amp;= p(z_k=i, x_{1:k}) \\cdot p(x_{k+2:n)|z_{k+1}=j}) \\cdot p(z_{k+1}=j|z_{k}=i) \\cdot p(x_{k+1}| z_{k+1}=j) \u0026amp;\u0026amp; (4.3)\\\\ \u0026amp;= \\alpha_k(z_k=i) \\cdot \\beta_{k+1}(z_{k+1}=j) \\cdot p(z_{k+1}=j|z_{k}=i) \\cdot p(x_{k+1}| z_{k+1}=j) \u0026amp;\u0026amp; (4.4) \\ \\end{aligned} $$\nNote that we can find the third and the forth term from the transition probability matrix and the emission probability matrix. Again, we can calculate $p(z_{k+1}, z_k | x)$ simply by introducing a normalization term. That is,\n$$ \\begin{aligned} p(z_{k+1}=s, z_k=r | x) \u0026amp;= \\frac{p(z_{k+1}=s, z_k=r, x)}{\\sum_{i=1}^{m} \\sum_{j=1}^{m} p(z_{k+1}=j, z_k=i, x)} \\\\ \u0026amp;= \\frac{\\alpha_k(z_k=r) \\cdot \\beta_{k+1}(z_{k+1}=s) \\cdot p(z_{k+1}=s|z_{k}=r) \\cdot p(x_{k+1}| z_{k+1}=s)}{\\sum_{i=1}^{m} \\alpha_k(z_k=i) \\cdot \\beta_{k+1}(z_{k+1}=j) \\cdot p(z_{k+1}=j|z_{k}=i) \\cdot p(x_{k+1}| z_{k+1}=j)} \u0026amp;\u0026amp; (4.42) \\end{aligned} $$\nRemark\n  We denote\n$$ \\gamma_k(i) = p(z_k = i | x) = \\frac{p(z_k=i, x)}{p(x)} \\tag{4.43}$$\n  We denote\n$$ \\xi_k(i,j) = p(z_{k+1}=j, z_k=i | x) = \\frac{p(z_{k+1}=j, z_k=i, x)}{p(x)} \\tag{4.44}$$\n  Three fundamental problems of HMM Problem 1 (Likelihood): Given an observation sequence $x$ and parameters $\\theta = (A, B, \\pi)$, determine the likelihood $p(x|\\theta)$.\nProblem 2 (Learning): Given an observation sequence $x$, learn the parameters $\\theta = (A, B, \\pi)$.\nProblem 3 (Inference): Given an observation sequence $x$ and parameters $\\theta = (A, B, \\pi)$, discover the best hidden state sequence $z$.\nProblem 1 (Likelihood) Goal: Given an observation sequence $x$ and parameters $\\theta = (A, B, \\pi)$, determine the likelihood $p(x|\\theta)$.\nNaive Way:\nFrom $(0)$, we have already know how to compute $P(x, z|\\theta)$, so we can compute $p(x|\\theta)$ by summing all possible sequence $z$:\n$$ p(x|\\theta) = \\sum_z P(x, z|\\theta) \\cdot p(z|\\theta)$$\nThis method is not applicable since there are $m^n$ ways of combinations of sequence $z$. So we introduce the following two algorithm: Forward Algorithm and Backward Algorithm.\nForward Algorithm  Goal: Compute $p(z_k, x_{1:k})$, given $\\theta = (A, B, \\pi)$.  From the picture above, it\u0026rsquo;s natural to compute $p(z_k, x_{1:k})$ by dynamic programming (DP). That is, to calculate it in terms of $p(z_{k-1}, x_{1:k-1})$:\n$$ \\begin{aligned} p(z_k , x_{1:k}) \u0026amp;= \\sum_{z_{k-1}} p(z_k, z_{k-1}, x_{1:k-1}, x_k) \u0026amp; \u0026amp; (5)\\\\ \u0026amp;= \\sum_{z_{k-1}} p(z_{k-1}, x_{1:k-1}) \\cdot p(z_k, x_k|z_{k-1}, x_{1:k-1}) \u0026amp; \u0026amp; (6)\\\\ \u0026amp;= \\sum_{z_{k-1}} p(z_{k-1}, x_{1:k-1}) \\cdot p(z_k|z_{k-1}, x_{1:k-1}) \\cdot p(x_k|z_k, z_{k-1}, x_{1:k-1}) \u0026amp; \u0026amp; (7)\\\\ \u0026amp;= \\sum_{z_{k-1}} p(z_{k-1}, x_{1:k-1}) \\cdot p(z_k|z_{k-1}) \\cdot p(x_k|z_k) \u0026amp; \u0026amp; (8)\\ \\end{aligned} $$\nRamark:\n  From $(6)$ to $(7)$, we use the fact that $p(b,c|a) = p(b|a) \\cdot p(c|a,b)$.\n  From $(7)$ to $(8)$, we use the conditional independence, which is visualized in the picture above.\n  We denote $p(z_k , x_{1:k})$ by $\\alpha_k(z_k)$, so\n$$ \\alpha_k(z_k) = p(z_k , x_{1:k}) = \\sum_{z_{k-1}} \\alpha_{k-1}(z_{k-1}) \\cdot p(z_k|z_{k-1}) \\cdot p(x_k|z_k) \\tag 9$$\n  In equation $(9)$, the term $p(z_k|z_{k-1})$ is the transition probability from state $z_{k-1}$ to state $z_{k}$; the term $p(x_k|z_k)$ is the emission probability of observing $x_k$ given state $z_k$.\n  $\\alpha_1(z_1=q) = p(z_1=q, x_1) = \\pi_q \\cdot p(x_1 | z_1 = q)$, where $p(x_1 | z_1 = q)$ is an emmission probability.\n  Knowing how to compute $p(z_k , x_{1:k})$ recurssively, we have\n$$p(x|\\theta) = p(x_{1:n}|\\theta) = \\sum_{z_n} p(z_n, x_{1:n}) = \\sum_{z_n} \\alpha_n(z_n) = \\sum_{q=1}^{m} \\alpha_n(z_n=q)$$\nBackward Algorithm  Goal: Compute $p(x_{k+1:n} | z_k)$, given $\\theta = (A, B, \\pi)$.  Again, we are going to use DP to compute $p(x_{k+1:n} | z_k)$ in terms of $p(x_{k+2:n} | z_{k+1})$:\n$$ \\begin{aligned} p(x_{k+1:n} | z_k) \u0026amp;= \\sum_{z_{k+1}} p(x_{k+1}, x_{k+2:n}, z_{k+1} | z_k) \\\\ \u0026amp;= \\sum_{z_{k+1}} p(x_{k+2:n}, z_{k+1}| z_k) \\cdot p(x_{k+1}| z_k, x_{k+2:n}, z_{k+1}) \\\\ \u0026amp;= \\sum_{z_{k+1}} p(z_{k+1}|z_k) \\cdot p(x_{k+2:n}|z_{k+1}, z_k) \\cdot p(x_{k+1} | z_k, x_{k+2:n}, z_{k+1}) \u0026amp;\u0026amp; (10)\\\\ \u0026amp;= \\sum_{z_{k+1}} p(x_{k+2:n}|z_{k+1}) \\cdot p(z_{k+1}|z_k) \\cdot p(x_{k+1}|z_{k+1}) \u0026amp;\u0026amp; (11)\\ \\end{aligned} $$\nRamark:\n  From $(10)$ to $(11)$, we use the conditional independece similar to the one in forward algorithm.\n  We denote $p(x_{k+1:n} | z_k)$ by $\\beta_k(z_k)$, so\n$$ \\beta_k(z_k) = p(x_{k+1:n} | z_k) = \\sum_{z_{k+1}} p(x_{k+2:n}|z_{k+1}) \\cdot p(z_{k+1}|z_k) \\cdot p(x_{k+1}|z_{k+1}) \\tag {12}$$\n  In equation $(12)$, the term $p(z_{k+1}|z_k)$ is the transition probability from state $z_{k}$ to state $z_{k+1}$; the term $p(x_{k+1}|z_{k+1})$ is the emission probability of observing $x_{k+1}$ given state $z_{k+1}$.\n  $\\beta_n(z_n) = 1$.\n  Knowing how to compute $p(x_{k+1:n} | z_k)$ recursively, we have\n$$ \\begin{aligned} p(x|\\theta) \u0026amp;= \\sum_{z_1} p(x, z_1) = \\sum_{z_1} p(x | z_1) \\cdot p(z_1) \\\\ \u0026amp;= \\sum_{z_1} p(x_1, x_{1+1:n} | z_1) \\cdot p(z_1) \\\\ \u0026amp;= \\sum_{z_1} p(x_1 | z_1) \\cdot p(x_{1+1:n}|z_1, x_1) \\cdot p(z_1) \u0026amp;\u0026amp;(13)\\\\ \u0026amp;= \\sum_{z_1} p(x_1 | z_1) \\cdot p(x_{1+1:n}|z_1) \\cdot p(z_1) \u0026amp;\u0026amp;(14)\\\\ \u0026amp;= \\sum_{z_1} p(x_1 | z_1) \\cdot \\beta_1(z_1) \\cdot p(z_1) \\\\ \u0026amp;= \\sum_{q=1}^m \\beta_1(z_1=q) \\cdot p(x_1 | z_1=q) \\cdot \\pi_q \\end{aligned} $$\nFrom $(13)$ to $(14)$, we use the conditional independence. To make it clean, I didn\u0026rsquo;t include $\\theta$ in the above derivation, but keep in mind $x$ is conditioned on $\\theta$.\nProblem 2 (Learning) Goal: Given an observation sequence $x$, learn the parameters $\\theta = (A, B, \\pi)$.\nGiven that the hidden states are unknown, it\u0026rsquo;s natural to use the EM Algorithm to solve parameters. Remind that the EM Algorithm consists of two steps:\n An expectation (E) step, which creates a function $Q(\\theta, \\theta_i)$ for the expectation of the log-likelihood $\\log p(x,z|\\theta)$ evaluated using the current conditional distribution of $z$ given $x$ and the current estimate of the parameters $\\theta_i$, where  $$ \\begin{aligned} Q(\\theta, \\theta_i) \u0026amp;= E_{z \\sim P(z|x,\\theta_i)}[\\log p(x,z|\\theta)] \\\\ \u0026amp;= \\sum_z P(z|x,\\theta_i) \\cdot \\log p(x,z|\\theta) \\ \\end{aligned} $$\n A maximization (M) step, which computes parameters maximizing the expected log-likelihood $Q(\\theta, \\theta_i)$ found on the $E$ step and then update parameters to $\\theta_{i+1}$.  We fist initialize parameters $\\theta_0 = (A_0, B_0, \\pi_0)$\nE Step:\nWe are going to construct $Q(\\theta, \\theta_i)$.\n$$ \\begin{aligned} Q(\\theta, \\theta_i) \u0026amp;= E_{z \\sim P(z|x,\\theta_i)}[\\log p(x,z|\\theta)] \\\\ \u0026amp;= \\sum_z P(z|x,\\theta_i) \\cdot \\log p(x,z|\\theta) \\\\ \u0026amp;= \\log p(x,z|\\theta) \\cdot \\frac{p(x,z|\\theta_i)}{p(x|\\theta_i)} \u0026amp;\u0026amp;(15)\\\\ \u0026amp;\\propto \\log p(x,z|\\theta) \\cdot p(x,z|\\theta_i) \u0026amp;\u0026amp;(16)\\ \\end{aligned} $$\nSince we know $x$ and $\\theta_i$, $p(x|\\theta_i)$ is a constant and therefore we can write from $(15)$ to $(16)$. In the earlier section \u0026ldquo;Settings of the Hidden Markov Model\u0026rdquo; of the post, we deduce that\n$$P(x, z|\\theta) = p(z_1) \\cdot [p(z_2|z_1)\\cdot p(z_3|z_2)\\cdot \u0026hellip; \\cdotp(z_n|z_{n-1})] \\cdot [p(x_1|z_1)\\cdot p(x_2|z_2)\\cdot \u0026hellip; \\cdot p(x_n|z_n)]$$\nSo we can formulate $Q(\\theta, \\theta_i)$ as\n$$ \\begin{aligned} Q(\\theta, \\theta_i) \u0026amp;= \\sum_z \\left( \\log \\pi_{z_i} + \\sum_{t=1}^{n-1} \\log p(z_{t+1}|z_t) + \\sum_{t=1}^{n} \\log p(x_n|z_n)\\right) \\cdot p(x,z|\\theta_i) \\\\ \u0026amp;= \\sum_z \\log \\pi_{z_i} \\cdot p(x,z|\\theta_i) + \\sum_z \\sum_{t=1}^{n-1} \\log p(z_{t+1}|z_t) \\cdot p(x,z|\\theta_i) + \\sum_z \\sum_{t=1}^{n} \\log p(x_t|z_t) \\cdot p(x,z|\\theta_i) \\ \\end{aligned} $$\nM Step:\nWe are going to maximize $Q(\\theta, \\theta_i)$ and update $\\theta_{i+1}$.\nNote that we write $Q(\\theta, \\theta_i)$ as the sum of three terms. The first therm is related to $\\pi$, the second term is related to $A$, and the third term is related to $B$. Therefore we can maximize each term separately.\nWe can write the first term as\n$$\\sum_z \\log \\pi_{z_i} \\cdot p(x,z|\\theta_i) = \\sum_{j=1}^m \\log \\pi_j \\cdot p(x, z_1 = j|\\theta_i)$$\nunder the constraint $\\sum_{j=1}^m \\pi_j = 1$. Clearly, this is a convex optimization problem:\nThe Lagrangian $L$ associated with the problem is\n$$ L(\\pi, v) = \\sum_{j=1}^m \\log \\pi_j \\cdot p(x, z_1 = j|\\theta_i) + v \\cdot (\\sum_{j=1}^m \\pi_j - 1)$$\nNote that any pair of primal and dual optimal points must satisfy the KKT conditions. So we use one KKT property that the gradient must vanish at the optimal point to find $\\pi$. This might not be the optimal $\\pi$ since \u0026ldquo;any pair of primal and dual optimal points must satisfy the KKT conditions\u0026rdquo; doesn\u0026rsquo;t imply that a point satisfying the KKT conditions is the optimal.\n$$ \\begin{aligned} \\frac{\\partial L}{\\partial \\pi_j} = p(x, z_1=j|\\theta_i) \\cdot \\frac{1}{\\pi_j} + v \u0026amp; = 0 \\\\ p(x, z_1=j|\\theta_i) + v \\cdot \\pi_j \u0026amp; = 0 \\\\ \\pi_j \u0026amp; = \\frac{-p(x, z_1=j|\\theta_i)}{v} \u0026amp; \u0026amp; (17)\\ \\end{aligned} $$\nBy setting $\\frac{\\partial L}{\\partial \\pi_j} = 0$ for all $j$, we have\n$$ \\begin{aligned} \\sum_{j=1}^m p(x, z_1=j|\\theta_i) + v \\cdot \\sum_{j=1}^m \\pi_j \u0026amp;= 0 \\\\ p(x|\\theta_i) + v \u0026amp;= 0\\\\ v \u0026amp;= - p(x|\\theta_i) \u0026amp;\u0026amp; (18)\\ \\end{aligned} $$\nBy plugging $(18)$ into $(17)$, we have\n$$ \\pi_j = \\frac{-p(x, z_1=j|\\theta_i)}{v} = \\frac{p(x, z_1=j|\\theta_i)}{p(x|\\theta_i)} = \\gamma_1(j)$$.\nIn the similar way, we can write the second term as\n$$ \\sum_z \\sum_{t=1}^{n-1} \\log p(z_{t+1}|z_t) \\cdot p(x,z|\\theta_i) = \\sum_{j=1}^m \\sum_{k=1}^m \\sum_{t=1}^{n-1} \\log p(z_{t+1}=k|z_t=j) \\cdot p(x,z_t=j, z_{t+1}=k|\\theta_i) \\ \\sum_z \\sum_{t=1}^{n} \\log p(x_t|z_t) \\cdot p(x,z|\\theta_i) = \\sum_{j=1}^m \\sum_{t=1}^n \\log p(x_t|z_t=j) \\cdot p(x,z_t=j|\\theta_i) $$\nwith seperate constraints\n$$\\sum_{k=1}^m p(z_{t+1}=k|z_t=j) = 1, \\ \\sum_{j=1}^m p(x_t|z_t=j) = 1 $$\nWe can solve for optimal parameters similar to solving for $\\pi$. After we set the gradient of corresponding Lagrangian to $0$, we have\n$$ \\begin{aligned} A_{jk} \u0026amp;= p(z_{t+1}=k|z_t=j) \\\\ \u0026amp;= \\frac{\\sum_{t=1}^{n-1} p(x, z_t=j, z_{t+1}=k|\\theta_i)}{\\sum_{t=1}^{n-1} p(x, z_t=j|\\theta_i)} \\\\ \u0026amp;= \\frac{\\sum_{t=1}^{n-1} p(x, z_t=j, z_{t+1}=k|\\theta_i)/ p(x|\\theta_i)}{\\sum_{t=1}^{n-1} p(x, z_t=j|\\theta_i)/p(x|\\theta_i)} \\\\ \u0026amp;= \\frac{\\sum_{t=1}^{n-1} \\xi_t(jk)}{\\sum_{t=1}^{n-1} \\gamma_t(j)} \u0026amp;\u0026amp;(19)\\\\ B_{jk} \u0026amp;= p(x_t= V_k|z_t=j) \\\\ \u0026amp;= \\frac{\\sum_{t=1}^{n-1} p(x, z_t=j|\\theta_i)\\cdot I(x_t= V_k)}{\\sum_{t=1}^{n-1} p(x, z_t=j | \\theta_i)} \\\\ \u0026amp;= \\frac{\\sum_{t=1}^{n-1} p(x, z_t=j|\\theta_i)/ p(x|\\theta_i) \\cdot I(x_t= V_k)}{\\sum_{t=1}^{n-1} p(x, z_t=j | \\theta_i)/ p(x|\\theta_i)} \\\\ \u0026amp;= \\frac{\\sum_{t=1}^{n-1} \\gamma_t(j) \\cdot I(x_t=V_k)}{\\sum_{t=1}^{n-1} \\gamma_t(j)} \u0026amp;\u0026amp;(20)\\ \\end{aligned} $$\nRemark: $I(x_t= V_k)$ is an indicator function. If $x_t= V_k$, then $I(x_t= V_k) = 1$, and $0$ otherwise. We get the results $(19)$ and $(20)$ from $(4.43)$ and $(4.44)$.\nWe also call this algorithm Baum-Welch algorithm.\nProblem 3 (Inference) Goal: Given an observation sequence $x$ and parameters $\\theta = (A, B, \\pi)$, discover the best hidden state sequence $z$.\nMethod 1:Brute force.\nThis is not applicable. Every hidden state has $m$ choices and the sequence has length $n$. So there are $m^n$ possible combinations.\nMethod 2: Use Forward/ Backward Algorithm. Given a sequence $x$, we know how to compute $p(z_k | x)$ from the top of the post. Therefore, at each time $k$, we can compute $p(z_k=i| x)$ for all $i \\in {1,2,\u0026hellip;,m}$ and choose the one with the highest probability. In the way, at every time, we chose the most possible hidden state. However, there is still a problem. It only finds the most possible hidden state locally and doesn\u0026rsquo;t take the whole sequence into account. Even if we chose the most possible hidden state at each time $k$. The combination of them might not be the best one and even doesn\u0026rsquo;t make sense. Foe example, if $A_{ij} = 0$, then if $z_k=i$, $z_{k+1}$ cannot take state $j$. But the Forward/ Backward Algorithm doesn\u0026rsquo;t take it into account. We can think of it as a greedy approach to approximate the best result.\nMethod 3: Viterbi algorithm:\nThe Viterbi algorithm is a dynamic programming algorithm for finding the most likely sequence of hidden states that results in a sequence of observed events in HMM.\nWe define $\\delta_k(i)$ to be the maximum probability among all paths which are at state $i$ at time $k$. That is,\n$$\\delta_k(i) = \\mathop{\\rm max}\\limits_{i_1:i_{k-1}} p(i_k=i, i_1:i_{k-1}, x_{1:k}|\\theta), \\quad i= 1,2,\u0026hellip;,m$$\nWe can construct a recurssion formula $\\delta_k(i)$. That is,\n$$ \\begin{aligned} \\delta_{k+1}(i) \u0026amp;= \\mathop{\\rm max}\\limits_{i_1:i_{k}} p(i_{k+1}=i, i_1:i_{k}, x_{1:k+1}|\\theta) \\\\ \u0026amp;= \\mathop{\\rm max}\\limits_{1\\leq j \\leq m} \\delta_k(j) \\cdot A_{ji} \\cdot p(x_{k+1}|z_{k+1}=i) \u0026amp;\u0026amp; i= 1,2,\u0026hellip;,m; , k=1,2,\u0026hellip;,n-1\\ \\end{aligned} $$\nAnd the base case is $\\delta_{1}(i) = \\pi_i \\cdot p(x_1| z_1=i)$.\nTherefore, we compute the optimal probability $P^{*}$\n$$P^{*} = \\mathop{\\rm max}\\limits_{1 \\leq i \\leq m} \\delta_n(i)$$\nby recursion. During the process of finding the highest probability of a path, we keep recording the hidden states associate with the path. So after we find the the highest probability, we also record the path associated with it and therefore the best sequence $z$.\n Reference:\n https://towardsdatascience.com/conditional-independence-the-backbone-of-bayesian-networks-85710f1b35b https://courses.cs.washington.edu/courses/cse473/16au/slides-16au/25-bn.pdf https://en.wikipedia.org/wiki/Conditional_independence https://web.stanford.edu/~jurafsky/slp3/A.pdf https://www.cs.cmu.edu/~epxing/Class/10701-08s/recitation/em-hmm.pdf https://people.eecs.berkeley.edu/~stephentu/writeups/hmm-baum-welch-derivation.pdf  ","permalink":"https://tangliyan.com/blog/posts/hmm/","summary":"Before reading this post, make sure you are familiar with the EM Algorithm and decent among of knowledge of convex optimization. If not, please check out my previous post\n  EM Algorithm\n  convex optimization primal and dual problem\n  Let\u0026rsquo;s get started!\nConditional independence $A$ and $B$ are conditionally independent given $C$ if and only if, given knowledge that $C$ occurs, knowledge of whether $A$ occurs provides no information on the likelihood of $B$ occurring, and knowledge of whether $B$ occurs provides no information on the likelihood of $A$ occurring.","title":"Hidden Markov Model (HMM)"},{"content":"Jensen’s inequality  Theorem: Let $f$ be a convex function, and let $X$ be a random variable. Then:  $$E[f(X)] \\geq f(E[X])$$\n$\\quad$ Moreover, if $f$ is strictly convex, then $E[f(X)] = f(E[X])$ holds true if and only if $X$ is a constant.\n Later in the post we are going to use the following fact from the Jensen\u0026rsquo;s inequality: Suppose $\\lambda_j \\geq 0$ for all $j$ and $\\sum_j \\lambda_j = 1$, then  $$ \\log \\sum_j \\lambda_j y_j \\geq \\sum_j \\lambda_j , log , y_j$$\n$\\quad$ where the $\\log$ function is concave.\nOverview of Expectation–Maximization (EM) algorithm In this post, let $Y$ be a set of observed data, $Z$ a set of unobserved latent data, and $\\theta$ the unknown parameters.\n(After this post, you will be comfortable with the following description about the EM algorithm.)\nExpectation–Maximization (EM) algorithm is an iterative method to find (local) maximum likelihood estimation (MLE) of $L(\\theta) = p(Y|\\theta)$, where the model depends on unobserved latent variables $Z$.\nAlgorithm:\n Initialize peremeters $\\theta_0$.  Iterate between steps 2 and 3 until convergence:\nan expectation (E) step, which creates a function $Q(\\theta, \\theta_i)$ for the expectation of the log-likelihood $\\log p(Y,Z|\\theta)$ evaluated using the current conditional distribution of $Z$ given $Y$ and the current estimate of the parameters $\\theta_i$, where  $$ \\begin{aligned} Q(\\theta, \\theta_i) \u0026amp;= \\sum_Z P(Z|Y,\\theta_i) \\cdot \\log p(Y,Z|\\theta) \\\\ \u0026amp;= E_{Z \\sim P(Z|Y,\\theta_i)}[\\log p(Y,Z|\\theta)] \\end{aligned} $$\nA maximization (M) step, which computes parameters maximizing the expected log-likelihood $Q(\\theta, \\theta_i)$ found on the $E$ step and then update parameters to $\\theta_{i+1}$.  These parameter-estimates are then used to determine the distribution of the latent variables in the next $E$ step. We say it converges if the increase in successive iterations is smaller than some tolerance parameter.\nIn general, multiple maxima may occur, with no guarantee that the global maximum will be found.\nIntuition: Why we need EM algorithm Sometimes maximizing the likelihood $\\ell(\\theta)$ explicitly might be difficult since there are some unknown latent variables. In such a setting, the EM algorithm gives an efficient method for maximum likelihood estimation.\nComplete Case v.s. Incomplete Case Complete case\n$(Y, Z)$ is observable, and the log likelihood can be written as\n$$ \\begin{aligned} \\ell(\\theta) \u0026amp;= \\log p(Y, Z | \\theta) \\\\ \u0026amp;= \\log p(Z|\\theta) \\cdot p(Y|Z, \\theta) \\\\ \u0026amp;= \\log p(Z|\\theta) + \\log p(Y|Z, \\theta) \\ \\end{aligned} $$\nWe subdivide our task of maximizing $\\ell(\\theta)$ into two sub-tasks. Note that in both $\\log p(Z|\\theta)$ and $\\log p(Y|Z, \\theta)$, the only unknown parameter is $\\theta$. They are just two standard MLE problems which could be easily solved by methods such as gradient descent.\nIncomplete case\n$(Y)$ is observable, but $(Z)$ is unknown. We need to introduce the marginal distribution of variable $Z$:\n$$ \\begin{aligned} \\ell(\\theta) \u0026amp;= \\log p(Y | \\theta) \\\\ \u0026amp;= \\log \\sum_Z p(Y, Z | \\theta) \\\\ \u0026amp;= \\log \\sum_Z p(Z|\\theta) \\cdot p(Y|Z, \\theta) \\ \\end{aligned} $$\nHere we have a summation inside the log, so it\u0026rsquo;s hard to use optimization methods or take derivatives. This is the case where EM algorithm comes into aid.\nEM Algorithm Derivation (Using MLE) Given the observed data $Y$, we want to maximize the likelihood $\\ell(\\theta) = p(Y|\\theta)$, and it\u0026rsquo;s the same as maximizing the log-likelihood $\\log p(Y|\\theta)$. Therefore, from now on we will try to maximize the likelihood\n$$\\ell(\\theta) = \\log p(Y|\\theta)$$\nby taking the unknown variable $Z$ into account, we rewrite the objective function as\n$$ \\begin{aligned} \\ell(\\theta) \u0026amp;= \\log p(Y|\\theta) \\\\ \u0026amp;= \\log \\sum_Z p(Y, Z | \\theta) \\\\ \u0026amp;= \\log \\sum_Z p(Y|Z,\\theta) \\cdot p(Z|\\theta) \\ \\end{aligned} $$\nNote that in the last step, the $\\log$ is outside of the $\\sum$, which is hard to compute and optimize. Check out my previous post to know why we prefer to have $\\log$ inside of $\\sum$, instead of outside. So later we would find a way to approximate it (Jensen’s inequality).\nSuppose we follow the iteration step 2 (E) and 3 (M) repeatedly, and have updated parameters to $\\theta_i$, then the difference between $\\ell(\\theta)$ and our estimate $\\ell(\\theta_i)$ is $\\ell(\\theta) - \\ell(\\theta_i)$. You can think of this difference as the improvement that later estimate of $\\theta$ tries to achieve. So our next step is to find $\\theta_{i+1}$ such that it improves the difference the most. That is, to make the difference $\\ell(\\theta) - \\ell(\\theta_i)$ as large as possible. So we want our next estimate $\\theta_{i+1}$ to be\n$$\\theta_{i+1} = \\mathop{\\rm arg,max}\\limits_{\\theta} ,, \\ell(\\theta) - \\ell(\\theta_i)$$\nNote that we know the value of $\\theta_i$, so as $\\ell(\\theta_i)$. And\n$$ \\begin{aligned} \\ell(\\theta) - \\ell(\\theta_i)\u0026amp;= \\log p(Y|\\theta) \\\\ \u0026amp;= \\log \\sum_Z p(Y|Z,\\theta) \\cdot p(Z|\\theta) - \\log p(Y|\\theta_i) \\\\ \u0026amp;= \\log \\sum_Z P(Z|Y,\\theta_i) \\frac{p(Y|Z,\\theta) \\cdot p(Z|\\theta)}{P(Z|Y,\\theta_i)} - \\log p(Y|\\theta_i) \u0026amp; \u0026amp; \u0026amp;(1)\\ \\end{aligned} $$\nSince $P(Z|Y,\\theta_i) \\geq 0$ for all $z\\in Z$ and $\\sum_Z P(Z|Y,\\theta_i) = 1$, we can use the Jensen’s inequality and then re-write $(1)$ as\n$$ \\begin{aligned} \\ell(\\theta) - \\ell(\\theta_i) \u0026amp;\\geq \\sum_Z P(Z|Y,\\theta_i) \\log \\frac{p(Y|Z,\\theta) \\cdot p(Z|\\theta)}{P(Z|Y,\\theta_i)} - \\sum_Z P(Z|Y,\\theta_i)) \\cdot \\log p(Y|\\theta_i)\\\\ \u0026amp;= \\sum_Z P(Z|Y,\\theta_i) \\left( \\log \\frac{p(Y|Z,\\theta) \\cdot p(Z|\\theta)}{P(Z|Y,\\theta_i)} - \\log p(Y|\\theta_i) \\right) \\\\ \\ell(\\theta) \u0026amp;\\geq \\ell(\\theta_i) + \\sum_Z P(Z|Y,\\theta_i) \\log \\frac{p(Y|Z,\\theta) \\cdot p(Z|\\theta)}{P(Z|Y,\\theta_i) \\cdot p(Y|\\theta_i)} \\end{aligned} $$\nNow we define\n$$ B(\\theta, \\theta_i) \\triangleq \\ell(\\theta_i) + \\sum_Z P(Z|Y,\\theta_i) \\log \\frac{p(Y|Z,\\theta) \\cdot p(Z|\\theta)}{P(Z|Y,\\theta_i) \\cdot p(Y|\\theta_i)} $$\nSo we see that\n$$ \\ell(\\theta) \\geq B(\\theta, \\theta_i)$$\nwhich implies that $B(\\theta, \\theta_i)$ is a lower bound of $\\ell(\\theta)$ for all $i$. Therefore our next step is to maximize the lower bound $B(\\theta, \\theta_i)$ and make it as tight as possible. In the $M$ step, we define\n$$ \\begin{aligned} \\theta_{i+1} \u0026amp;= \\mathop{\\rm arg,max}\\limits_{\\theta} B(\\theta, \\theta_i)\\\\ \u0026amp;= \\mathop{\\rm arg,max}\\limits_{\\theta} , \\left(\\ell(\\theta_i) + \\sum_Z P(Z|Y,\\theta_i) \\cdot \\log \\frac{p(Y|Z,\\theta) \\cdot p(Z|\\theta)}{P(Z|Y,\\theta_i) \\cdot p(Y|\\theta_i)} \\right) \u0026amp;\u0026amp; (2)\\\\ \u0026amp;= \\mathop{\\rm arg,max}\\limits_{\\theta} , \\left(\\sum_Z P(Z|Y,\\theta_i) \\cdot \\log \\frac{p(Y|Z,\\theta) \\cdot p(Z|\\theta)}{P(Z|Y,\\theta_i) \\cdot p(Y|\\theta_i)} \\right) \u0026amp;\u0026amp; (3)\\\\ \u0026amp;= \\mathop{\\rm arg,max}\\limits_{\\theta} , \\left(\\sum_Z P(Z|Y,\\theta_i) \\cdot [\\log p(Y|Z,\\theta) \\cdot p(Z|\\theta) - \\log P(Z|Y,\\theta_i) \\cdot p(Y|\\theta_i)] \\right) \u0026amp;\u0026amp; (4)\\\\ \u0026amp;= \\mathop{\\rm arg,max}\\limits_{\\theta} , \\left(\\sum_Z P(Z|Y,\\theta_i) \\cdot \\log p(Y|Z,\\theta) \\cdot p(Z|\\theta) \\right) \u0026amp;\u0026amp; (5)\\\\ \u0026amp;= \\mathop{\\rm arg,max}\\limits_{\\theta} , \\left(\\sum_Z P(Z|Y,\\theta_i) \\cdot \\log p(Y,Z|\\theta) \\right) \u0026amp;\u0026amp;(6)\\\\ \u0026amp;= \\mathop{\\rm arg,max}\\limits_{\\theta} , Q(\\theta, \\theta_i) \u0026amp;\u0026amp;(7)\\ \\end{aligned} $$\nRemark:\n  $(2) \\to (3)$ since $\\ell(\\theta_i)$ does not contain $\\theta$.\n  $(3) \\to (4)$ we used $\\log \\frac{A}{B} = \\log A - \\log B$.\n  $(4) \\to (5)$ since $\\log P(Z|Y,\\theta_i) \\cdot p(Y|\\theta_i)$ does not contain $\\theta$.\n  $(6) \\to (7)$ we define $Q(\\theta, \\theta_i) = \\sum_Z P(Z|Y,\\theta_i) \\cdot \\log p(Y,Z|\\theta)$.\n  since both $Y$ and $\\theta_i$ are known, we have the distribution of $Z \\sim p(Z|Y,\\theta_i)$. Therefore, the only unknown parameter is $\\theta$, which means this is now a complete case I mentioned early in the post. So this is now a MLE problem.\n  summary   $\\theta_{i+1} = \\mathop{\\rm arg,max}\\limits_{\\theta} \\ell(\\theta) - \\ell(\\theta_i) = \\mathop{\\rm arg,max}\\limits_{\\theta} , Q(\\theta, \\theta_i)$. This implies that maximizing $\\ell(\\theta) - \\ell(\\theta_i)$ is the same as maximizing $Q(\\theta, \\theta_i)$.\n  Note that $Q(\\theta, \\theta_i) = \\sum_Z P(Z|Y,\\theta_i) \\cdot \\log p(Y,Z|\\theta)$ is just the expectation of $\\log p(Y,Z|\\theta)$, where $Z$ is drawn from the current conditional distribution $P(Z|Y,\\theta_i)$. Therefore we have $$ Q(\\theta, \\theta_i) = E_{Z \\sim p(Z|Y,\\theta_i)}[\\log p(Y,Z|\\theta)]$$ That\u0026rsquo;s why it\u0026rsquo;s called the Expectation step. In E step, we are actually trying to calculate the expectation of the term $\\log p(Y,Z|\\theta)$, where unknown variable $Z$ follows the current conditional distribution given by $Y$ and $\\theta_i$. Then in the Maximazation step, we are trying to maximize this expectation. That\u0026rsquo;s why it\u0026rsquo;s called the M step.\n  Coordinate Ascent/ descent - view EM from a different prospect In the Expectation Step, we actually fixed $\\theta_i$, and tried to optimize $Q(\\theta, \\theta_i)$.\nIn the Maximization step, we actually fixed $Q(\\theta, \\theta_i)$, and tried to optimize $\\theta$ to get $\\theta_{i+1}$.\nEvery time we only optimize one variable and fix the rest. Therefore, from the graph above we see that in every iteration the gradient changes either vertically or horizontally.\nTo see how to perform the coordinate descent, check out my previous post.\nConvergence of EM algorithm By following the algorithm, we keep updating parameter $\\theta_i$ and calculating approximated log-likelihood $\\ell (\\theta_i)$. But do we actually keep $\\ell (\\theta_i)$ getting closer to $l(\\theta)$ as we do more iterations? Keep in mind that in MLE our final goal is to maximize $\\ell (\\theta)$.\nSuppose $\\theta_i$ and $\\theta_{i+1}$ are the parameters from two successive iterations of EM. We will now prove that $\\ell(\\theta_i) \\leq \\ell(\\theta_{i+1})$, which shows EM always monotonically improves the log-likelihood.\n$$ \\begin{aligned} \\ell(\\theta) \u0026amp;= \\log p(Y|\\theta) \\\\ \u0026amp;= \\log \\frac {p(Y,Z|\\theta)}{p(Z|Y, \\theta)} \\\\ \u0026amp;= \\log p(Y,Z|\\theta) - \\log p(Z|Y, \\theta) \\\\ \u0026amp;= \\sum_Z p(Z|Y, \\theta_i)\\cdot \\log p(Y,Z|\\theta) - \\sum_Z p(Z|Y, \\theta_i)\\cdot \\log p(Z|Y, \\theta) \\\\ \u0026amp;= Q(\\theta, \\theta_i) + H(\\theta, \\theta_i) \u0026amp;\u0026amp; (8)\\ \\end{aligned} $$\nwhere $H(\\theta, \\theta_i) = - \\sum_Z p(Z|Y, \\theta_i)\\cdot \\log p(Z|Y, \\theta)$. This last equation $(8)$ holds for every value of $\\theta$, including $\\theta=\\theta_i$, which means\n$$ \\ell(\\theta_i) = Q(\\theta_i, \\theta_i) + H(\\theta_i, \\theta_i)$$\nTherefore subtracting $\\ell(\\theta_i)$ from $\\ell(\\theta_{i+1})$ gives\n$$ \\begin{aligned} \\ell(\\theta_{i+1}) - \\ell(\\theta_i) \u0026amp;= [Q(\\theta_{i+1}, \\theta_{i}) + H(\\theta_{i+1}, \\theta_{i})] - [Q(\\theta_i, \\theta_i) + H(\\theta_i, \\theta_i)]\\\\ \u0026amp;= [Q(\\theta_{i+1}, \\theta_{i}) - Q(\\theta_i, \\theta_{i})] + [H(\\theta_{i+1}, \\theta_{i}) - H(\\theta_i, \\theta_{i})] \\ \\end{aligned} $$\nSince $\\theta_{i+1} = \\mathop{\\rm arg,max}\\limits_{\\theta} , Q(\\theta, \\theta_i)$, we have $Q(\\theta_{i+1}, \\theta_i) \\geq Q(\\theta_i, \\theta_i)$. The second parenthesis gives\n$$ \\begin{aligned} H(\\theta_{i+1}, \\theta_{i}) - H(\\theta_i, \\theta_{i}) \u0026amp;= - \\sum_Z p(Z|Y, \\theta_i)\\cdot \\log p(Z|Y, \\theta_{i+1}) + \\sum_Z p(Z|Y, \\theta_i)\\cdot \\log p(Z|Y, \\theta_i) \\\\ \u0026amp;= - \\left( \\sum_Z p(Z|Y, \\theta_i)\\cdot \\log p(Z|Y, \\theta_{i+1}) - \\sum_Z p(Z|Y, \\theta_i)\\cdot \\log p(Z|Y, \\theta_i)\\right) \\\\ \u0026amp;= - \\left( \\sum_Z p(Z|Y, \\theta_i) \\cdot \\log \\frac{p(Z|Y, \\theta_{i+1})}{p(Z|Y, \\theta_{i})} \\right) \u0026amp;\u0026amp; \\ \\ \\ (9)\\\\ \u0026amp;\\geq - \\log \\left( \\sum_Z p(Z|Y, \\theta_i) \\cdot \\frac{p(Z|Y, \\theta_{i+1})}{p(Z|Y, \\theta_{i})} \\right) \u0026amp;\u0026amp; (10)\\\\ \u0026amp;= - \\log \\sum_Z p(Z|Y, \\theta_{i+1}) = - \\log 1 = 0 \\end{aligned} $$\nFrom $(9)$ to $(10)$ we use the Jensen’s inequality (note that there is a negative sign in the front so we reverse the inequality).\nSince $Q(\\theta_{i+1}, \\theta_{i}) \\geq Q(\\theta_i, \\theta_{i})$ and $H(\\theta_{i+1}, \\theta_{i}) \\geq H(\\theta_i, \\theta_{i})$, we have\n$$ \\ell(\\theta_{i+1}) \\geq \\ell(\\theta_i) \\qquad\\qquad \\text{for all} , i$$\nSince it\u0026rsquo;s monotonically increasing and bounded above, we say $\\ell (\\theta_i)$ converges.\nWhat\u0026rsquo;s next? I\u0026rsquo;m going to write a post to discuss how EM algorithm is applied in K-means and GMM in the future. Stay tuned!\n Reference:\n https://en.wikipedia.org/wiki/Expectation–maximization_algorithm Part IX: The EM algorithm from CS229 Lecture notes by Andrew Ng http://cs229.stanford.edu/notes/cs229-notes8.pdf http://guillefix.me/cosmos/static/Jensen%27s%2520inequality.html https://en.wikipedia.org/wiki/Jensen%27s_inequality http://www.adeveloperdiary.com/data-science/machine-learning/introduction-to-coordinate-descent-using-least-squares-regression/  ","permalink":"https://tangliyan.com/blog/posts/em/","summary":"Jensen’s inequality  Theorem: Let $f$ be a convex function, and let $X$ be a random variable. Then:  $$E[f(X)] \\geq f(E[X])$$\n$\\quad$ Moreover, if $f$ is strictly convex, then $E[f(X)] = f(E[X])$ holds true if and only if $X$ is a constant.\n Later in the post we are going to use the following fact from the Jensen\u0026rsquo;s inequality: Suppose $\\lambda_j \\geq 0$ for all $j$ and $\\sum_j \\lambda_j = 1$, then  $$ \\log \\sum_j \\lambda_j y_j \\geq \\sum_j \\lambda_j , log , y_j$$","title":"EM (Expectation–Maximization) Algorithm"},{"content":"Comparison between CBOW and Skip-gram The major difference is that skip-gram is better for infrequent words than CBOW in word2vec. For simplicity, suppose there is a sentence \u0026ldquo;$w_1w_2w_3w_4$\u0026rdquo;, and the window size is $1$.\nFor CBOW, it learns to predict the word given a context, or to maximize the following probability\n$$ p(w_2|w_1,w_3) \\cdot P(w_3|w_2,w_4)$$\nThis is an issue for infrequent words, since they don’t appear very often in a given context. As a result, the model will assign them a low probabilities.\nFor Skip-gram, it learns to predict the context given a word, or to maximize the following probability\n$$ P(w_2|w_1) \\cdot P(w_1|w_2) \\cdot P(w_3|w_2) \\cdot P(w_2|w_3) \\cdot P(w_4|w_3) \\cdot P(w_3|w_4)$$\nIn this case, two words (one infrequent and the other frequent) are treated the same. Both are treated as word AND context observations. Hence, the model will learn to understand even rare words.\nSkip-gram Main idea of Skip-gram   Goal: The Skip-gram model aims to learn continuous feature representations for words by optimizing a neighborhood preserving likelihood objective.\n  Assumption: The Skip-gram objective is based on the distributional hypothesis which states that words in similar contexts tend to have similar meanings. That is, similar words tend to appear in similar word neighborhoods.\n  Algorithm: It scans over the words of a document, and for every word it aims to embed it such that the word’s features can predict nearby words (i.e., words inside some context window). The word feature representations are learned by optmizing the likelihood objective using SGD with negative sampling.\n  Skip-gram model formulation Skip-gram learns to predict the context given a word by optimizing the likelihood objective. Suppose now we have a sentence\n$$\\text{\u0026ldquo;I am writing a summary for NLP.\u0026quot;}$$\nand the model is trying to predict context words given a target word \u0026ldquo;summary\u0026rdquo; with window size $2$:\n$$ \\text {I am [ ] [ ] summary [ ] [ ] . }$$\nThen the model tries to optimize the likelihood\n$$ P(\\text{\u0026ldquo;writing\u0026rdquo;}|\\text{\u0026ldquo;summary\u0026rdquo;}) \\cdot P(\\text{\u0026ldquo;a\u0026rdquo;}|\\text{\u0026ldquo;summary\u0026rdquo;}) \\cdot P(\\text{\u0026ldquo;for\u0026rdquo;}|\\text{\u0026ldquo;summary\u0026rdquo;}) \\cdot P(\\text{\u0026ldquo;NLP\u0026rdquo;}|\\text{\u0026ldquo;summary\u0026rdquo;})$$\nIn fact, given a sentence, Skip-gram is going to, in turn, treat every word as a target word and predict context words. So the objective function is\n$$\\mathop{\\rm argmax} , P(\\text{\u0026ldquo;am\u0026rdquo;}|\\text{\u0026ldquo;I\u0026rdquo;}) \\cdot P(\\text{\u0026ldquo;writing\u0026rdquo;}|\\text{\u0026ldquo;I\u0026rdquo;}) \\cdot P(\\text{\u0026ldquo;I\u0026rdquo;}|\\text{\u0026ldquo;am\u0026rdquo;}) , \u0026hellip; , P(\\text{\u0026ldquo;for\u0026rdquo;}|\\text{\u0026ldquo;NLP\u0026rdquo;})$$\nTo put it in a formal way: given a corpus of words $w$ and their contexts $c$, we consider the conditional probabilities $p(c|w)$, and given a corpus $\\text{text}$, the goal is to set the parameters $\\theta$ of $p(c|w; \\theta)$ so as to maximize the corpus probability:\n$$\\mathop{\\rm argmax}\\limits_{\\theta} , \\prod_{w ,\\in ,\\text{text}} \\prod_{c ,\\in ,\\text{context($w$)}} p(c|w;\\theta) \\tag{1}$$\nAlternatively, we can write it as\n$$\\mathop{\\rm argmax}\\limits_{\\theta} , \\prod_{(w,c),\\in, D} p(c|w;\\theta) \\tag{2}$$\nwhere $D$ is the set of all word and context pairs we extract from the text. Now we rewrite the objective by taking the $\\log$:\n$$\\mathop{\\rm argmax}\\limits_{\\theta} , \\sum_{(w,c),\\in, D} \\log p(c|w;\\theta) \\tag{3}$$\nThe follow-up question comes immediately: how to define the term $p(c|w;\\theta)$? It must satisfy the following two conditions:\n  $0 \\le p(c|w;\\theta) \\le 1$;\n  $\\sum_{c , \\in , \\text{context(w)} } \\log p(c|w;\\theta) = 1$\n  A natural way is to use the softmax function, so we define it to be\n$$ p(c|w;\\theta) = \\frac{e^{u_c \\cdot v_w}}{\\sum_{c' , \\in , U}e^{u_{c'} \\cdot v_w}} \\tag{4}$$\nwhere $v_w, u_c \\in \\mathbb{R}^k$ are vector representations for $w$ and $c$ respectively, and $U$ is the set of all available contexts. Throughout this post, we assume that the target words and the contexts come from distinct vocabulary matrices $V$ and $U$ respectively, so that, for example, the vector associated with the word lunch will be different from the vector associated with the context lunch. One motivation is that every word plays two rules in the model, one as a target word and one as a context word. That\u0026rsquo;s why we need two separate matrices $U$ and $V$. Note that they must have the same dimension $|\\text{Vocab}| \\times k$, where $k$ is a hyperparameter and is the dimension of each word vector representation. We would like to set the parameters $\\theta$ such that the objective function $(3)$ is maximized.\nHere, we use the inner product to measure the similarity between two vectors $v_w$ and $u_c$. If they have a similar meaning, meaning they should have similar vector representation, then $p(c|w;\\theta)$ should be assigned for a high probability.\n(Side note: Comparison of cosine similarity and inner product as distance metrics \u0026ndash; Cosine similarity only cares about angle difference, while dot product cares about angle and magnitude. If you normalize your data to have the same magnitude, the two are indistinguishable.)\nBy plugging in our definition of $p(c|w;\\theta)$, we can write the objective function as\nWhile the objective can be computed, it is computationally expensive to do so, because the term $p(c|w;\\theta)$ is very expensive to compute due to the summation\n$$\\log (\\sum_{c' \\in U}e^{u_{c'} \\cdot v_w})$$\nover all the contexts $c'$ (there can be hundreds of thousands of them). The time complexity is $O(|\\text{Vocab}|)$.\nWhy prefer taking log inside the sum rather than outside Note: Usually we prefer having the $\\log$ inside the sum rather than outside. The log and the sum are part of a function that you want to optimize. That means that, at some point, you will be looking to set the gradient of that function to $0$. The derivative is a linear operator, so when you have the sum of the log, the derivative of the sum is a sum of derivatives. By contrast, the derivative of the log of the sum will have, as seen via the chain rule, a form like $1/\\text{(your sum)} \\cdot \\text{(derivative of the sum)}$. Finding zeros of this function will likely be a much more challenging task, especially if done analytically. On the other hand, since the summation is computational expensive, $\\log$ outside the sum often requires technique of approximation, such as using Jensen\u0026rsquo;s Inequality. Check out my post to know more about Jensen\u0026rsquo;s inequality.\nNow, it\u0026rsquo;s time to re-formulate the objective function and try to approximate it!\nNegative Sampling \u0026ndash; Skip-gram model RE-formulation\nIn our previous Skip-gram model formulation, we assume that if $(w,c)$ is a word and context pair in the training data, then the probability $p(c|w,\\theta)$ should be high. Now we can think about this backward and ask: if we have a high (low) probability $p(c|w,\\theta)$, is $(w,c)$ really (not) a word and context pair in the training data? In this way of thinking, we formulate a binary classification problem.\nLet $p(D=1|w,c)$ be the probability that the pair $(w,c)$ comes from the corpus and $p(D=0|w,c) = 1 - p(D=1|w,c)$ be the probability that the pair $(w,c)$ is not from the corpus.\nAs before, assume there are parameters $\\theta$ controlling the distribution: $p(D = 1|w,c;\\theta)$. Since it\u0026rsquo;s a binary classification problem, we can define it using sigmoid function\n$$ p(D=1| w,c;\\theta) = \\frac{1}{1 + exp(-u_c \\cdot v_w)} = \\sigma {(u_c \\cdot v_w)}$$\nOur goal is now finding parameters to maximize the following objective function:\nwhere the set $\\tilde D$ consists of random $(w,c)$ pairs not in $D$. We call a pair $(w,c)$ not in the corpus a negative sample (the name negative-sampling stems from the set $\\tilde D$ of randomly sampled negative examples). There are a few points worth mentioning:\n  $1 - \\sigma (x) = \\sigma (-x)$\n  This objective function looks quite similar to the objective function of logistic regression.\n  In this formulation, we successfully avoid having the $\\log$ outside the sum.\n  Usually, $|D| \u0026laquo; |\\tilde D|$, so we only pick $k$ negative samples for each data sample. From the original paper, the author suggested that values of $k$ in the range $5$-$20$ are useful for small training datasets, while for large datasets the $k$ can be as small as $2$–$5$.\nSo if we choose k negative samples for each data sample and denote these negative samples by $N(w)$, then out objective function becomes\n$$ L(\\theta) = \\mathop{\\rm argmax}\\limits_{\\theta} \\sum_{(w,c) ,\\in,D } \\left[\\log \\sigma(u_w \\cdot v_c) + \\sum_{c' \\in N(w)} \\log \\sigma (-u_w \\cdot v_{c'}) \\right] \\tag 5$$\nSGD for Skip-gram objective function\nConclusion\nLet\u0026rsquo;s end the topic of Skip-gram model with some details of code implementation:\n  Dynamic window size: the window size that is being used is dynamic – the parameter $k$ denotes the maximal window size. For each word in the corpus, a window size $k′$ is sampled uniformly from $1, . . . , k$.\n  Effect of subsampling and rare-word pruning: words appearing less than min-count times are not considered as either words or contexts, and frequent words (as defined by the sample parameter) are down-sampled. Importantly, these words are removed from the text before generating the contexts. This has the effect of increasing the effective window size for certain words. The motivation for sub-sampling is that frequent words are less informative.\n   reference:\n http://www.davidsbatista.net/blog/2018/12/06/Word_Embeddings/ https://www.quora.com/Why-is-it-preferential-to-have-the-log-inside-the-sum-rather-than-outside https://www.quora.com/Is-cosine-similarity-effective https://datascience.stackexchange.com/questions/744/cosine-similarity-versus-dot-product-as-distance-metrics https://stats.stackexchange.com/questions/180548/why-is-skip-gram-better-for-infrequent-words-than-cbow Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2016. Yoav Goldberg and Omer Levy. word2vec explained: deriving Mikolov et al.’s negative-sampling wordembedding method. arXiv preprint arXiv:1402.3722, 2014. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013b. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111–3119. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient Estimation of Word Representations in Vector Space. In ICLR Workshop Papers.  ","permalink":"https://tangliyan.com/blog/posts/skipgram/","summary":"Comparison between CBOW and Skip-gram The major difference is that skip-gram is better for infrequent words than CBOW in word2vec. For simplicity, suppose there is a sentence \u0026ldquo;$w_1w_2w_3w_4$\u0026rdquo;, and the window size is $1$.\nFor CBOW, it learns to predict the word given a context, or to maximize the following probability\n$$ p(w_2|w_1,w_3) \\cdot P(w_3|w_2,w_4)$$\nThis is an issue for infrequent words, since they don’t appear very often in a given context.","title":"Skip-gram"},{"content":"Overview of various word representation and Embedding methods Local Representation v.s. Distributed Representation  One-hot encoding is local representation and is good for local generalization; distributed representation is good for global generalization.  Comparison between local generalization and global generalization:\nHere is an example for better understanding this pair of concepts. Suppose now you have a bunch of ingredients and you\u0026rsquo;re able to cook 100 different meals with these ingredients.\n  Local Representation way of cooking:\n You cook every meal directly from these ingredients. Suppose you cook several meals a lot, then you would be good at cooking these meals. If you want to cook something new but similar to these meals (concept of \u0026ldquo;local\u0026rdquo;), you would probably also cook them well. Here we say you have a good local generalization.    Global Representation way of cooking:\n You first cook some semi-finished products. from these ingredients and then cook every meal from these semi-finished products. For every meal, you use all of semi-finished products. That means if you want your meal to be delicious, you have to improve the quality of these semi-finished products. Therefore, if one meal become more delicious, then all semi-finished products become better in general, which in turn make other meal more delicious (concept of \u0026ldquo;parameter sharing\u0026rdquo;). Here we say you have a good global generalization.    What are advantages of Distributed Representation over Local Representation?   Representational power: distributed representation forms a more efficient code than localist representations. A localist representation using $n$ neurons can represent just $n$ different entities. A distributed representation using $n$ binary neurons can represent up to $2^n$ different entities.\n  Continuity (in the mathematical sense): representing concepts in continuous vector spaces allows powerful gradient-based learning techniques such as back-propagation to be applied to many problems.\n  Soft capacity limits: distributed representations typically have soft limits on how many concepts can be represented simultaneously.\n  Context-free word representation v.s. Contextual word representation   Context-free word representation: a single word representation is generated for each word in the vocabulary.\n Example: CBOW, Skip-gram, GloVe, MF (matrix factorization).    Contextual word representation: the word representation depends on the context where that word occurs, meaning that the same word in different contexts can have different representations.\n Example: ELMo, Bert, XLNet, ALBert, GPT (Generative Pre-Training).    From Euclidean space to non-Euclidean space Geometry is a familiar field for the machine learning community; geometric ideas play an important role in many parts of machine learning. For example, we often study the convexity of functions, the dimensionality and projections of data representations, and we frequently produce visualizations to understand our data and training processes.\nIn machine laerning, most of the times, we\u0026rsquo;ve actually imposed a geometry. We\u0026rsquo;ve chosen to work with Euclidean space, with all of its inherent properties, among the most critical of which is its flatness. However, the Euclidean space cannot handle all the cases and we still need to work with other spaces to solve some problems. Below, I list two advantages of non-Euclidean space:\n  Better representations: Euclidean space simply doesn\u0026rsquo;t fit many types of data structures that we often need to work with. The core example is the hierarchy, or, its abstract network representation, the tree. There is a specific non-Euclidean choice that naturally fits trees: hyperbolic space, and I\u0026rsquo;m going to talk a little bit about it later.\n  More flexible operations: The flatness of Euclidean space means that certain operations require a large number of dimensions and complexity to perform\u0026mdash;while non-Euclidean spaces can potentially perform these operations in a more flexible way, with fewer dimensions.\n  Hyperbolic Space A specific and common non-Euclidean space is the hyperbolic space, which is a good fit for the hierarchical representation. Here I am going to show two examples.\nExample 1:\nSuppose we want a model to learn the embeddings for the following four words: \u0026ldquo;Country\u0026rdquo;, \u0026ldquo;Asia\u0026rdquo;, \u0026ldquo;China\u0026rdquo;, and \u0026ldquo;Japan\u0026rdquo;. Apparently, there is a tree structure between these words. \u0026ldquo;Country\u0026rdquo; is the root node; \u0026ldquo;Asia\u0026rdquo; is a child node of \u0026ldquo;Country\u0026rdquo;; \u0026ldquo;China\u0026rdquo; and \u0026ldquo;Japan\u0026rdquo; are two sibling nodes and also child nodes of \u0026ldquo;Asia\u0026rdquo;. Therefore, we want our learned embeddings, in some degree, show this hierarchical structure. The Euclidean space cannot achieve this, but we can resort to Hyperbolic space to learn this structure.\nExample 2:\nThere are hierarchical relationships places like WordNet, Wikipedia categories and knowledge graphs. Exploiting all of these with a machine learning model requires continuous representations, and so we must embed a tree into a continuous space (Graph Embedding). Specifically, we want to maintain the distances of nodes in the tree when these are embedded. Thus we’d like the embedded versions of a pair of sibling nodes to be at distance 2, an embedded node and its parent to be at distance 1, and so on. Ideally, we’d like a ball whose volume grows exponentially in the radius. This doesn’t happen in Euclidean space. But in fact, hyperbolic space offers exactly this property.\nGaussian Embedding Intuition:\nSuppose we have the following text corpus\n \u0026ldquo;I was tired.\u0026rdquo; \u0026ldquo;I am a student.\u0026rdquo; \u0026ldquo;I was studying.\u0026rdquo; \u0026ldquo;Am I good at studying?\u0026rdquo;  and we want to train word embeddings from this corpus. The problem is that words like \u0026ldquo;I\u0026rdquo; appears a lot, but words like \u0026ldquo;good\u0026rdquo; and \u0026ldquo;student\u0026rdquo; only occur once. If a word appears a lot in a corpus, then we would have confidence for the trained word embedding. Conversely, if a word only occur few times, the trained word embedding would have high uncertainty. Usually we have the following way to solve this problem:\n  Increse the size of the text corpus.\n  Remove words that have frequencies below a threshold.\n  Use a sub-word model (split a word into sub-words; this solves the oov (out of vocabulary) problem).\n  Learn the uncertainty from the data and add the uncertainty into the word embedding. We call this embedding Gaussian Embedding. For example, we can learn a Gussian Embedding for the word good as $E_{\\text{good}} \\sim \\mathcal{N}(\\mu_{\\text{good}}, \\Sigma_{\\text{good}})$\n  Gaussian Embedding:\nAn embedded vector representing a point estimate does not naturally express uncertainty about the target concepts with which the input may be associated. Point vectors are typically compared by dot products, cosine-distance or Euclean distance, none of which provide for asymmetric comparisons between objects (as is necessary to represent inclusion or entailment).\nIn the Gaussian Embedding, both means and variances are learned from the text corpus. Note that Gaussian distributions innately represent uncertainty. If we want to compare two Gaussian word embeddings, since the parameters are distributions, KL-Divergence comes naturally, which is naturally an asymmetric distance meansurement.\nGraph Embedding Definition:\nGraph embedding is an approach that is used to transform nodes, edges, and their features into a lower dimensional vector space and whilst maximally preserving properties like graph structure and information.\nBecause of the complexity of networks, there is no good graph embedding method that can apply to all networks. The embedding approaches vary in performance on different datasets.\nnode2vec:\nThere are many graph embedding methods, and a popular one is called node2vec.\nInspired by the Skip-gram model, recent research established an analogy for networks by representing a network as a \u0026ldquo;document\u0026rdquo;. The same way as a document is an ordered sequence of words, one could sample sequences of nodes from the underlying network and turn a network into a ordered sequence of nodes.\nThe node2vec model also introduce the way to make it more flexible to sample nodes from a network.\n It used Breadth-first Sampling (BFS) to obtain a micro-scopic view of the neighborhood of every node and Depth-first Sampling (DFS) to reflect a macro-scopic view of the neighborhood of every node.   It also introduced biased random walks, which use parameters $p$ and $q$ to control the weight of BFS and DFS.  I\u0026rsquo;m going to write a separate post to explain node2vec in detail in the future. Stay tuned!\n Reference:\n http://ling.umd.edu/~ellenlau/courses/ling646/Plate_2003.pdf https://www.quora.com/What-are-the-advantages-of-distributed-representations http://www.davidsbatista.net/blog/2018/12/06/Word_Embeddings/ https://towardsdatascience.com/overview-of-deep-learning-on-graph-embeddings-4305c10ad4a4 https://www.quora.com/What-is-hierarchical-softmax https://dawn.cs.stanford.edu/2019/10/10/noneuclidean/ https://towardsdatascience.com/overview-of-deep-learning-on-graph-embeddings-4305c10ad4a4 Vilnis, Luke and McCallum, Andrew. Word representations via gaussian embedding. In ICLR, 2015. Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2016.  ","permalink":"https://tangliyan.com/blog/posts/representation/","summary":"Overview of various word representation and Embedding methods Local Representation v.s. Distributed Representation  One-hot encoding is local representation and is good for local generalization; distributed representation is good for global generalization.  Comparison between local generalization and global generalization:\nHere is an example for better understanding this pair of concepts. Suppose now you have a bunch of ingredients and you\u0026rsquo;re able to cook 100 different meals with these ingredients.","title":"Distributed representation, Hyperbolic Space, Gaussian/Graph Embedding"},{"content":"NLP = NLU + NLG  NLU: Natural Language Understanding NLG: Natural Language Generation  NLG may be viewed as the opposite of NLU: whereas in NLU, the system needs to disambiguate the input sentence to produce the machine representation language, in NLG the system needs to make decisions about how to put a concept into human understandable words.\nClassical applications in NLP   Question Answering\n  Sentiment Analysis\n  Machine Translation\n  Text Summarization: Text summarization refers to the technique of shortening long pieces of text. The intention is to create a coherent and fluent summary having only the main points outlined in the document. It involves both NLU and NLG. It requires the machine to first understand human text and overcome the long distance dependence problems (NLU) and then generate human understandable text (NLG).\n Extraction-based summarization: The extractive text summarization technique involves pulling keyphrases from the source document and combining them to make a summary. The extraction is made according to the defined metric without making any changes to the texts. The grammar might not be right. Abstraction-based summarization: The abstraction technique entails paraphrasing and shortening parts of the source document. The abstractive text summarization algorithms create new phrases and sentences that relay the most useful information from the original text — just like humans do. Therefore, abstraction performs better than extraction. However, the text summarization algorithms required to do abstraction are more difficult to develop; that’s why the use of extraction is still popular.    Information Extraction: Information extraction is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents and other electronically represented sources. QA uses information extraction a lot.\n  Dialogue System\n task-oriented dialogue system.    Text preprocessing Tokenization For Chinese, classical methods are forward max-matching and backward max-matching.\nShortcoming: Do not take semantic meaning into account.\nTokenization based on Language Modeling. Given an input, generate all possible way to split the sentence and then find the one with the highest possibility.\nUnigram model: $$ P(s) = P(w_1)P(w_2)\u0026hellip;P(w_k) $$\nBigram model: $$ P(s) = P(w_1)P(w_2 | w_1)P(w_3|w_2) \u0026hellip; P(w_k |w_{k-1})$$\nAlternative: Use Viterbi algorithm (Dynamic Programming) to find the optimal way of splitting. Every directed graph path corresponds to a way to split a sentence.\n Spell Correction   Way 1: Go through the vocabulary and then return the words with the smallest edit distance.\n  Way 2: Generate all strings with edit distance 1 or 2, then filter and return (faster than Way 1). We will discuss how to filter at the end of the summary and the filtering method we introduce is called Noisy Channel model.\n    Lowercasing Lowercasing text data is one of the simplest and most effective form of text preprocessing. It can help in cases where your dataset is not very large and thus solve the sparsity issue.\nStop-word removal The intuition behind using stop words is that, by removing low information words from text, we can focus on the important words instead. Think about it as a feature selection process. In general, we won\u0026rsquo;t do stop-word removal when dealing with machine translation.\nZipf\u0026rsquo;s law: Zipf\u0026rsquo;s law is an empirical law refers to the fact that many types of data studied can be approximated with a Zipfian distribution. It states that given some corpus, the frequency of any word is inversely proportional to its rank in the frequency table. Thus the most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word.\nNormalization Text normalization is the process of transforming text into a standard form. For example, the word “gooood” and “gud” can be transformed to “good”. Another example is mapping of near identical words such as “stopwords”, “stop-words” and “stop words” to just “stopwords”. Text normalization is important for noisy texts such as social media comments, text messages and comments to blog posts where abbreviations, misspellings and use of out-of-vocabulary words (oov) are prevalent.\nTwo popular ways to normalizr are Stemming \u0026amp; Lemmatization.\nThe goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.\nHowever, the two words differ in their flavor. Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.\nTF-IDF TF-IDF works by increasing proportionally to the number of times a word appears in a document, but is offset by the number of documents that contain the word. So, words that are common in every document, such as this, what, and if, rank low even though they may appear many times, since they don’t mean much to that document in particular. However, if a word appears many times in a document, while not appearing many times in others, it probably means that it’s very relevant.\nFormula There are multiple ways to caluculate. Here is one: the TF-IDF score for each word $w$ in the document $d$ from the document set $D$ is calculated as follows:\n$$ tf\\text{-}idf , (w,d,D) = tf , (w,d) \\cdot idf , (w,D)$$\nwhere\n$$ tf , (w,d) = \\text{freq} , (w,d) \\ idf ,(w, D) = \\log \\frac{|D|}{N(w)} $$\nwhere $\\text{freq} , (w,d)$ is the frequency of word $w$ in the document $d$. $|D|$ is the number of document and $N(w)$ is the number of documents having word $w$. Therefore we have a TF-IDF representation for all words in all documents. Note that in every document, the number of word is equal to the number of TF-IDF representation.\nWays to compute similarities between sentences Common ways are using Dot Product, Cosine Similarity, Minkowski distance, and Euclidean distance.\nDot Product: $$d(x,y) = \\langle x, y\\rangle $$\nCosine Similarity: $$ d(x, y) = \\frac{\\langle x, y\\rangle}{|x||y|}$$\nEuclidean distance (squared): $$ d(x, y) = |x-y|^2 = |x|^2 +|y|^2 - 2\\langle x , y\\rangle$$\nMinkowski distance: $$ d(x, y) = (\\sum_{i=1}^n |x_i - y_i|^p ) ^ \\frac{1}{p}$$\nComparision between Cosine Similarity and Euclidean distance:\n  In general, the Cosine Similarity removes the effect of document length. For example, a postcard and a full-length book may be about the same topic, but will likely be quite far apart in pure \u0026ldquo;term frequency\u0026rdquo; space using the Euclidean distance. However,tThey will be right on top of each other in cosine similarity.\n  Euclidean distance mainly measures the numeric difference between $x$ and $y$. Cosine Similarity mainly measures the difference of direction between $x$ and $y$.\n  However, if we normalize x and y, the two calculations are equivalent. If we assume $x$ and $y$ are normalized, then Cosine Similarity is $\\langle x , y\\rangle$, and Euclidean distance (squared) is $2(1 - \\langle x , y\\rangle)$. As you can see, minimizing (square) euclidean distance is equivalent to maximizing cosine similarity if the vectors are normalized.\n  Noisy Channel model ( for spell correction) Intuition:\nThe intuition of the noisy channel model is to treat the misspelled word as if a correctly spelled word had been \u0026ldquo;distorted\u0026rdquo; by being passed through a noisy communication channel. This channel introduces noise, making it hard to recognize the true word. Our goal is to find the true word by passing every word of the language through our model of the noisy channel and seeing which one comes the closest to the misspelled word.\nProcess:\nWe see an observation $x$ (a misspelled word) and our job is to find the word $w$ that generated this misspelled word. Out of all possible words in the vocabulary $V$ we want to find the word $w$ such that $P(w|x)$ is highest. So our objective function is $$\\mathop{\\rm argmax}\\limits_{w , \\in , V} P(w|x)$$\nWe can re-write our objective function as $$\\mathop{\\rm argmax}\\limits_{w , \\in , V} \\frac{P(x|w)P(w)}{p(x)} $$\nSince $P(x)$ doesn’t change for each choice of word $w$, we can drop it and simplify the objective function as\n$$ \\mathop{\\rm argmax}\\limits_{w , \\in , V} P(x|w)P(w)$$\nThe channel model (or likelihood) of the noisy channel producing any particular observation sequence x is modeled by $P(x|w)$. The prior probability of a hidden word is modeled by $P(w)$. We can compute the most probable word $\\hat w$ given that we’ve seen some observed misspelling $x$ by multiplying the prior $P(w)$ and the likelihood $P(x|w)$ and choosing the word for which this product is greatest.\nWe apply the noisy channel approach to correcting non-word spelling errors by taking any word not in our spelling dictionary, generating a list of candidate words, ranking them according to the objective function defined above and then picking the highest-ranked one. In fact, we can modify the objective function to refer to this list of candidate words instead of the full vocabulary $V$ as follows:\nTo find this list of candidates we’ll use the Minimum Edit Distance algorithm. Note that the types of edits are:\n Insertion Deletion Substitution Transposition of two adjacent letters (perticular in tasks like spell correction)  why we prefer not to compute $p(w|x)$ directly?   Two distributions $p(x|w)$ and $p(w)$ (the language model) can be estimated seperately.\n  If we compute $p(w|x)$ directly, that means we just find a word that maximize the probability $p(w|x)$ but do not put the word in the context (surrounding words). Thus the accuracy of the spell correction is pretty low.\n  On the other hand, it\u0026rsquo;s worth noting that the surrounding words will make the choice of word clearer. If we maximize $P(x|w)P(w)$, there is a prior term $P(w)$ which we can use bigram, trigram, etc, to compute. Usually, bigram, trigram are better than unigram since they take surrounding words into account.\n  How to compute channel model $P(x|w)$ A perfect model of the probability that a word will be mistyped would condition on all sorts of factors: who the typist was, whether the typist was left-handed or right-handed, and so on. Luckily, we can get a pretty reasonable estimate of $P(x|w)$ just by looking at local context: the identity of the correct letter itself, the misspelling, and the surrounding letters. For example, the letters $m$ and $n$ are often substituted for each other; this is partly a fact about their identity (these two letters are pronounced similarly and they are next to each other on the keyboard) and partly a fact about context (because they are pronounced similarly and they occur in similar contexts). For more detail about how to compute $P(x|w)$, check out https://web.stanford.edu/~jurafsky/slp3/B.pdf.\n Reference:\n https://en.wikipedia.org/wiki/Natural-language_generation https://towardsdatascience.com/a-quick-introduction-to-text-summarization-in-machine-learning-3d27ccf18a9f https://en.wikipedia.org/wiki/Information_extraction https://kavita-ganesan.com/text-preprocessing-tutorial/ https://web.stanford.edu/~jurafsky/slp3/B.pdf https://en.wikipedia.org/wiki/Zipf%27s_law https://stackoverflow.com/questions/1787110/what-is-the-difference-between-lemmatization-vs-stemming https://monkeylearn.com/blog/what-is-tf-idf/ https://www.jianshu.com/p/4f0ee6d023a5 https://www.reddit.com/r/MachineLearning/comments/493exs/why_do_they_use_cosine_distance_over_euclidean/ https://stats.stackexchange.com/questions/72978/vector-space-model-cosine-similarity-vs-euclidean-distance https://datascience.stackexchange.com/questions/6506/shall-i-use-the-euclidean-distance-or-the-cosine-similarity-to-compute-the-seman https://www.quora.com/Natural-Language-Processing-What-is-a-noisy-channel-model  ","permalink":"https://tangliyan.com/blog/posts/nlp_basic/","summary":"NLP = NLU + NLG  NLU: Natural Language Understanding NLG: Natural Language Generation  NLG may be viewed as the opposite of NLU: whereas in NLU, the system needs to disambiguate the input sentence to produce the machine representation language, in NLG the system needs to make decisions about how to put a concept into human understandable words.\nClassical applications in NLP   Question Answering\n  Sentiment Analysis","title":"NLP Basics, Spell Correction with Noisy Channel"},{"content":"Kaggle: Google Quest Q\u0026amp;A Labeling summary General Part Congratulations to all winners of this competition. Your hard work paid off!\nFirst, I have to say thanks to the authors of the following three published notebooks:\nhttps://www.kaggle.com/akensert/bert-base-tf2-0-now-huggingface-transformer, https://www.kaggle.com/abhishek/distilbert-use-features-oof, https://www.kaggle.com/codename007/start-from-here-quest-complete-eda-fe.\nThese notebooks showed awesome ways to build models, visualize the dataset and extract features from non-text data.\nOur initial plan was to take question title, question body and answer all into a Bert based model. But after we analyzed the distribution of the lengths of question bodies and answers, we found two major problems:\n If we fitted all three parts as input, we had to adjust the input space for both question body and answer due to the limitation of the input size of the Bert based models. In order to do this, we had to trim a bunch of text, which was a waste of training data. Also, the question of how to trim a long text immediately presented itself to us. Roughly half of the question bodies and answers had code in them. When implementing tokenization, it really brought some troubles for us. The tokenization of the code looked extremely wired, and we didn’t know if these would have a large effect on the model prediction.  We later did some experiments (I will list some at the end of the post and these turned out to not help a lot) and finally decided to train two separate models. The first one was to fit Bert based models with only question title and question body. The second one was to fit with only question title and answer. In this way, almost all question bodies and answers can be fitted into models.\nLike other authors in the discussion, we split 30 output features into two parts. We used the question title and question body to predict features related to questions and used the question title and answer to predict features related to answers. We trained weights for several times, and the final output is the average of several models with these weights.\nAfter doing all of these, we received a 0.415 on the public LB (ranked around 90 at that time). For the rest of the days, we shifted our focus to the data post-processing part cause at that time with the highest scores being over 0.480. But we didn’t believe the models we used had such a big difference. There must have been some trick parts that we had not come to realize.\nSo, I checked and compared the distribution of the model prediction and the standard distribution of the training set. We had then found out the “magic” part: the ratings were DISCRETE in the training set. It makes sense, right? Probably raters could only choose certain scores that “make sense”. No one is going to give a score like 0.76354237 in real life :). So, I just did a simple adjustment: pick out every prediction output, find the closest score that “makes sense”, and then replace it. That simple change raised my scores and rank dramatically (LB over 0.450).\nI then plotted the distribution of prediction and the standard distribution of the training set again and fine-tuned (actually did some hard coding) every column. This process took me about two days. I submitted many versions, but it did not turn out well with the risk of overfitting. Finally, I selected one kernel with no fine-tuning and one with fine-tuning (hard coding) as my final submission, and the one with no fine-tuning won out.\nThings that didn’t work:\n Since humans can still understand a sentence even if it missed some words, I tried to define a function that can randomly drop some words from the text and then make it compatible with the input. This did increase my public LB score around 0.003-0.004, but after we decided to build a model with separate two parts, it had little effect. I roughly went through the text of the dataset and tried to find something in common among texts that have codes (It spent me almost a whole day \u0026gt;_\u0026lt; ). I then used some regular expression and tried to delete all codes and links in all texts. However, I am not entirely sure if it was good to delete them. Creating some special tokens might be a better idea, but we really didn’t have enough time to do that. It turned out to not work that well and even made the score lower, so we decided to not use it.  Technical part Feature Engineering: Inspired by pubic kernels, we extract URLs from the url column and implemented one-hot encoding to both the URLs and category columns. There are 64 new feature columns (51 for urls and 13 for categories) after one-hot encoding. According to the paper Universal Sentence Encoder, The Universal Sentence Encoder (USE) encodes text into high dimensional vectors that can be used for diverse tasks. The input is the variable-length English text, and the output is a 512-dimensional vector. So, we encoded question title, question body, and answer into 512-dimensional vectors and then compared the L2 distance and cosine similarities between them. We then added all of these as new features. There are 512 + 512 + 64 + 2 = 1090 new features.\nUnder the concern of overfitting, I tried to decrease the number of generated features by briefly reading through a few examples of each category and then manually reclassifying 51 categories into four sub-categories: have code, daily life, academic, literature readings. However, this didn’t improve the prediction.\nModel Structure: We used a combo of two pretrained Roberta base models to train the dataset. The first Roberta handles the question title and question body pairs, while the second Roberta handles the question title and answer pairs. We only fitted texts into Roberta models and ignored 1090 new features for now.\nThe Roberta base model has 12 hidden layers and there are 13 layers in total (which includes the final output layer). Every layer has the output dimension batch_size * 512 * 768 (batch _size * max_seq_len * emb_size). Initially, I tried out convolutions with different filter sizes such as 4 * 768, 6 * 768 or 8 * 768 to capture the meaning of the combination of words. However, it turned out to not work that well, and the average of the whole sequences outperformed them.\nWe took the last three output layers out, concatenated them together and then applied an average-pooling on it. After this, we averaged over 512 tokens for each input in the batch to capture the meaning of the whole sequence and hoped the model would learn some lower level features and added them to the final representation. Finally, we concatenated 1090 features with the average embedding and then added a fully connected layer with 21 units for the title-body pair model and 9 units for the title-answer pair model. I didn’t use the first unit of the output layer of the Roberta model since Roberta removes the Next Sentence Prediction (NSP) task from BERT’s pre-training, and the first unit seems not as useful as in Bert. The final predictions are the average of 6 pairs of Roberta models (6-fold).\nWe were supposed to freeze Roberta models and then fine-tune the added fully connected layer, but we didn’t due to the time limit.\nCustomized Learning rate: We used a range test in the beginning, to find out the proper learning rate during training. We then wrote up a customized scheduler inherited from PolynomialDecay to change the learning rate dynamically. We set up warm-up steps proportional to the total decay-steps to avoid the primacy effect at the beginning of the training process. The learning rate increases linearly over the warm-up period and we didn’t set up the learning rate to have cyclic behavior.\nTraining: Like most of the other groups, we use GroupKFold with n_splits=5, did 6-fold cv for 6 epochs and saved the training weights with the highest cv scores. We used Adam optimizer with mixed-precision data types, which dynamically and automatically adjusting the scaling to prevent Inf or NaN values and saved training time.\nPost-processing: We first considered using one-hot encoding for all output columns, given that every column has finite discrete values, but this shouldn’t work well since it didn’t take the order of the values into account. Because of the time limit, we just compared the distribution of predictions for each column with the distribution of actual values in the training set and then adjust the threshold based on the distribution. We compared the scores between before post-processing and after post-processing to see if it improved performance.\nLink to my Kaggle Competition summary: https://www.kaggle.com/c/google-quest-challenge/discussion/129893#742230.\n","permalink":"https://tangliyan.com/blog/posts/kaggle_google_quest/","summary":"Kaggle: Google Quest Q\u0026amp;A Labeling summary General Part Congratulations to all winners of this competition. Your hard work paid off!\nFirst, I have to say thanks to the authors of the following three published notebooks:\nhttps://www.kaggle.com/akensert/bert-base-tf2-0-now-huggingface-transformer, https://www.kaggle.com/abhishek/distilbert-use-features-oof, https://www.kaggle.com/codename007/start-from-here-quest-complete-eda-fe.\nThese notebooks showed awesome ways to build models, visualize the dataset and extract features from non-text data.\nOur initial plan was to take question title, question body and answer all into a Bert based model.","title":"Kaggle: Google Quest Q\u0026A Labeling - my solution"},{"content":"Linear SVM Idea We want to find a hyper-plane $w^\\top x + b = 0$ that maximizes the margin.\nSet up We first show that the vector $w$ is orthogonal to this hyper-plane. Let $x_1$, $x_2$ be any element on the hyper-plane. So we have $w^\\top x_1 + b = 0$ and $w^\\top x_2 + b = 0$. Then $w^\\top (x_1 - x_2) = 0$, which implies $w$ is orthogonal to the hyper-plane. Now, we set two dashed lines to $w^\\top x + b = 1$ and $w^\\top x + b = -1$. In fact, \u0026ldquo;$1$\u0026rdquo; doesn\u0026rsquo;t matter and we can pick any value here. \u0026ldquo;$1$\u0026rdquo; is just the convention.\nNow we pick any line parallel (orthogonal) to $w$ (hyper-plane), then the line intersect two dashed line with point $x^{(+)}$ and $x^{(-)}$. We want to maximize the margin\n$$margin = || x^{(+)} - x^{(-)}||$$\nRe-express margin Since $w$ is parallel to $x^{(+)} - x^{(-)}$, we have $x^{(+)} - x^{(-)}$ = $\\lambda w$ for some $\\lambda$. Then\n$$x^{(+)} = \\lambda w + x^{(-)}$$\nSince $w^\\top x^{(+)} + b = 1$, we have $w^\\top (\\lambda w + x^{(-)}) + b = 1$ and then $\\lambda w^\\top w + w^\\top x^{(-)} + b = 1$. Since $w^\\top x^{(-)} + b = -1$, we have $\\lambda = w^\\top w = 2$. So\n$$ \\lambda = \\frac{2}{w^\\top w}$$\nNow we can rewrite the margin as\n$$ margin = ||(\\lambda w + x^{(-)}) - x^{(-)}|| = ||\\lambda w|| = ||\\frac{2}{w^\\top w} w || = \\frac{2}{||w||}$$\nConstruct an optimization problem we can construct the following objective function for SVM:\nwe can re-write it as\nSoft version of linear SVM Note that the above constraints are hard constraints and it only works if the data are linearly separable.\nTherefore, if data is not linearly separable, we want to make a soft constraint (relaxation). That is, we allow the model to make some error, but we will add some penalty for them.\nNote that if $\\lambda \\to \\infty$, then we allow no error; if $\\lambda = 0$, then we add no penalty. We call $\\epsilon_i$ a slack variable. Ideally, we want $\\epsilon_i = 0$; if it makes an error, $\\epsilon_i \u0026gt; 0$.\nConvert to Hinge Loss Since $(w^\\top x^{(i)} + b)y^{(i)} \\geq 1 - \\epsilon_i$, we have $\\epsilon_i \\geq 1-(w^\\top x^{(i)} + b)y^{(i)}$. If $\\epsilon_i \\leq 0$, we have no loss. Otherwise, we add $\\epsilon_i = 1-(w^\\top x^{(i)} + b)y^{(i)}$ as loss. So right now our new objective function is\nStochastic Gradient descent for Hinge Loss objective function:\nNon-linear SVM We are going to map all points through a non-linear function and then used SVM in this transformed space. The idea is that if the non-linear map we use maps the two sets of points such that the two sets of points can be separated by a line after the transformation, then SVM can be used in this transformed space instead of the original space.\nLet $\\phi: \\mathcal{X} \\to \\mathcal{F}$ be the non linear map described in the earlier paragraph, where $\\mathcal{X}$ is the space from which inputs points are coming from and $\\mathcal{F}$ is the transformed space. For SVM to work, we don\u0026rsquo;t need to know $\\phi$ explicitly, but only need to know the dot product of the transformed points $⟨\\phi(x_i), \\phi(x_j)⟩$. So, instead of working with $\\phi$, they can instead work with $K: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}$ where $K$ takes two points as input and returns a real value that represents $⟨\\phi(x_i), \\phi(x_j)⟩$. Note that $\\phi$ exists only when $K$ is positive definite. With this, we are able to run SVM on an infinite dimensional space.\nGram Matrix: Given a set of vectors in $\\mathcal{V}$, the Gram Matrix is the matrix of all possible inner products in $\\mathcal{V}$. That is, $G_{ij} = v_i \\cdot v_j$.\nCurse of Dimensionality As the dimensionality increases, the classifier\u0026rsquo;s performance increases until the optimal number of features is reached. Further increasing the dimensionality without increasing the number of training samples results in a decrease in classifier performance. The common theme of these problems is that when the dimensionality increases, the volume of the space increases so fast that the available data become sparse. In order to obtain a statistically sound and reliable result, the amount of data needed to support the result often grows exponentially with the dimensionality. Also, the time complexity of a classification algorithm is proportional to the dimension of the data point. So, higher dimension means larger time complexity (not to mention space complexity to store those large dimensional points).\nMercer\u0026rsquo;s Theorem (simplified idea): In a finite input space, if the Kernel matrix $\\mathbf{K}$ (also known as Gram matrix) is positive semi-definite ($\\mathbf{K}_{ij} = K(x_i, x_j) = \\phi(x_i) \\cdot \\phi(x_j)$), then the matrix element, i.e. the function K, can be a kernel function.\nExample of a kernel function Let $x_i = (x_{i1}, x_{i2}), x_j = (x_{j1}, x_{j2})$ and $\\phi(x_i)=(x_{i1}^2, \\sqrt{2}x_{i1}x_{i2}, x_{i2}^2), \\phi(x_j)=(x_{j1}^2, \\sqrt{2}x_{j1}x_{j2}, x_{j2}^2)$. Then\nFrom this example, we see that even if the dimension of the feature space is higher than the input space, we can still do the computation in the low dimension input space as long as we choose a good kernel! Therefore, it\u0026rsquo;s possible to run SVM on an infinite dimensional feature space but do the same amount of computation as in the low dimension input space if we choose a good kernel.\nNote:\n Let $n$ be the dimension of input space, $N (\u0026raquo; n)$ be the dimension of the feature space, then if we choose a kernel $K$ properly, we are able to compute the dot product in higher dimensional space but in complexity $O(n)$ instead of $O(N)$. If the classification algorithm is only dependent on the dot product and has no dependency on the actual map $\\phi$, I can use the kernel trick to run the algorithm in high dimensional space with almost no additional cost.  Common Kernel Function Polynomial Kernel $$k(x_i, x_j) = (x_i \\cdot x_j + 1)^d$$\nwhere $d$ is the degree of the polynomial. This type of kernel represents the similarity of vectors in a feature space over polynomials of the original variables. It is popular in natural language processing.\nGaussian Kernel $$k(x, y) = \\text{exp}\\left(-\\frac{|x_i - x_j|^2}{2\\sigma^2}\\right)$$\nThis type of kernel is useful when there is no prior knowledge about the data; it has good performance when there is the assumption og general smoothness of the data. It is an example of the radial basis function kernel (below). $\\sigma$ is the regularization variable that can be tuned specifically for each problem.\nGaussian Radial Basis Function (RBF) $$k(x_i, x_j) = \\text{exp}(-\\gamma |x_i - x_j|^2)$$\nfor $\\gamma \u0026gt; 0$. The difference between this kernel and the gaussian kernel is the amount of regularization applied.\nExponential Kernel $$k(x, y) = \\text{exp}\\left(-\\frac{|x_i - x_j|}{2\\sigma^2}\\right)$$\nDual problem of SVM Our primal problem is\nNote that the primal problem of SVM is a convex problem and the constraints are convex. We know that for any convex optimization problem with differentiable objective and constraint functions, any points that satisfy the KKT conditions are primal and dual optimal, and have zero duality gap.\nWe can re-write the primal problem as\nwhere $\\phi(x_i)$ is a map of $x_i$ from input space to feature space. Now\nThen we can use the $4$-th KKT condition (gradient w.r.t. $w, b, \\epsilon_i$ is $0$):\nTherefore we have\nOur dual problem of SVM is\nNote that the maximization only depends on the dot product of $\\phi(x_i), \\phi(x_j)$. We define a function\n$$K(x_i, x_j) = \\phi(x_i) \\cdot \\phi(x_j)$$\nAll we need is the function $K$, a kernel function, which provides with the dot product of two vectors in another space and we don\u0026rsquo;t need to know the transformation into the other space.\nWe can re-write the problem as\n Reference:\n https://www.quora.com/Why-should-a-kernel-function-satisfy-Mercers-condition https://scikit-learn.org/stable/modules/svm.html#svm-kernels https://kfrankc.com/posts/2019/06/21/kernel-functions-svm# https://towardsdatascience.com/understanding-support-vector-machine-part-2-kernel-trick-mercers-theorem-e1e6848c6c4d https://www.datasciencecentral.com/profiles/blogs/about-the-curse-of-dimensionality https://en.wikipedia.org/wiki/Curse_of_dimensionality https://stats.stackexchange.com/questions/44166/kernelised-k-nearest-neighbour https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec13.pdf  ","permalink":"https://tangliyan.com/blog/posts/svm/","summary":"Linear SVM Idea We want to find a hyper-plane $w^\\top x + b = 0$ that maximizes the margin.\nSet up We first show that the vector $w$ is orthogonal to this hyper-plane. Let $x_1$, $x_2$ be any element on the hyper-plane. So we have $w^\\top x_1 + b = 0$ and $w^\\top x_2 + b = 0$. Then $w^\\top (x_1 - x_2) = 0$, which implies $w$ is orthogonal to the hyper-plane.","title":"SVM, Dual SVM, Non-linear SVM"},{"content":"Consider an optimization problem in the standard form (we call this a primal problem):\nWe denote the optimal value of this as $p^\\star$. We don\u0026rsquo;t assume the problem is convex.\nThe Lagrange dual function We define the Lagrangian $L$ associated with the problem as $$ L(x,\\lambda, v) = f_0(x) + \\sum^m_{i=1}\\lambda_if_i(x) + \\sum^p_{i=1}v_ih_i(x)$$ We call vectors $\\lambda$ and $v$ the dual variables or Lagrange multiplier vectors associated with the problem (1).\nWe define the Lagrange dual function (or just dual function) $g$ as the minimum value of the Lagrangian over $x$: for $\\lambda \\in \\mathbf{R}^m, v\\in\\mathbf{R}^p$, $$g(\\lambda,v) = \\mathop{\\rm inf}\\limits_{x\\in \\mathcal{D}} L(x, \\lambda, v) = \\mathop{\\rm inf}\\limits_{x\\in \\mathcal{D}} \\left( f_0(x) + \\sum^m_{i=1}\\lambda_if_i(x) + \\sum^p_{i=1}v_ih_i(x)\\right)$$\nNote that once we choose an $x$, $f_i(x)$ and $h_i(x)$ are fixed and therefore the dual function is a family of affine functions of ($\\lambda$, $v$), which is concave even the problem (1) is not convex.\nLower bound property The dual function yields lower bounds on the optimal value $p^\\star$ of the problem (1): For any $\\lambda \\succeq 0$ and any $v$ we have $$ g(\\lambda,v) \\leq p^\\star$$\nSuppose $\\tilde{x}$ is a feasible point for the problem (1), i.e., $f_i(\\tilde{x}) \\leq 0$ and $h_i(\\tilde{x}) = 0$, and $\\lambda \\succeq 0$. Then we have\n$$ L(\\tilde{x}, \\lambda, v) = f_0(\\tilde{x}) + \\sum^m_{i=1}\\lambda_if_i(\\tilde{x}) + \\sum^p_{i=1}v_ih_i(\\tilde{x}) \\leq f_0(\\tilde{x})$$\nHence $$ g(\\lambda,v) = \\mathop{\\rm inf}\\limits_{x\\in \\mathcal{D}} L(x, \\lambda, v) \\leq L(\\tilde{x}, \\lambda, v) \\leq f_0(\\tilde{x})$$\nSince $g(\\lambda,v) \\leq f_0(\\tilde{x})$ holds for every feasible point $\\tilde{x}$, the inequality $g(\\lambda,v) \\leq p$ follows. The inequality holds, but is vacuous, when $g(\\lambda,v) = -\\infty$. The dual function gives a nontrivial lower bound on $p^\\star$ only when $\\lambda \\succeq 0$ and $(\\lambda,v) \\in \\textbf{dom},g$, i.e., $g(\\lambda,v) \u0026gt; - \\infty$. We refer to a pair $(\\lambda,v)$ with $\\lambda \\succeq 0$ and $(\\lambda,v) \\in \\textbf{dom},g$ as dual feasible.\nDerive an analytical expression for the Lagrange dual function Practice problem 1: Least-squares solution of linear equations The Lagrangian is $L(x,v) = x^\\top x + v^\\top(Ax-b)$. The dual function is given by $g(v) = \\text{inf}_x L(x,v)$. Since $L(x,v)$ is a convex quadratic function of $x$, we can find the minimizing $x$ from the optimality condition\n$$ \\nabla_x L(x,v) = 2x + A^\\top v = 0$$\nwhich yields $x = -(1/2)A^\\top v$. Therefore the dual function is\n$$ g(v) = L(-(1/2)A^\\top v, v) = -(1/4)v^\\top AA^\\top v - b^\\top v$$\nTherefore, $p^\\star \\geq -(1/4)v^\\top AA^\\top v - b^\\top v$. The next step is to maximize $-(1/4)v^\\top AA^\\top v - b^\\top v$.\nPractice problem 2: Standard form Linear Programming The Lagrangian is $$ L(x, \\lambda, v) = c^\\top x - \\sum^n_{i=1}\\lambda_ix_i + v^\\top(Ax-b) = -b^\\top v + (c + A^\\top v - \\lambda)^\\top x$$\nThe dual function is $$ g(\\lambda, v) = \\mathop{\\rm inf}\\limits_{x} L(x, \\lambda, v) = -b^\\top v + \\mathop{\\rm inf}\\limits_{x}, (c + A^\\top v - \\lambda)^\\top x$$\nWe see that $g(\\lambda,v)$ is a linear function. Since a linear function is bounded below only when it is zero. Thus, $g(\\lambda,v) = -\\infty$ except when $c + A^\\top v - \\lambda = 0$. Therefore,\nThe lower bound property is nontrivial only when $\\lambda$ and $v$ satisfy $\\lambda \\succeq 0$ and $c + A^\\top v - \\lambda$. When this occurs, $-b^\\top v$ is a lower bound on the optimal value of the LP. We can form an equivalent dual problem by making these equality constraints explicit:\nThis problem, in turn, can be expressed as\nThe Lagrange dual problem For each pair $(\\lambda,v)$ with $\\lambda \\succeq 0$, the Lagrange dual function gives us a lower bound on the optimal value $p^\\star$ of the optimization problem (1). Thus we have a lower bound that depends on some parameters $\\lambda, v$.\nThis leads to the optimization problem\nThis problem is called the Lagrange dual problem associated with the problem (1). In this context the original problem (1) is sometimes called the primal problem. We refer to $(\\lambda^\\star, v^\\star)$ as dual optimal or optimal Lagrange multipliers if they are optimal for the problem (2). The Lagrange dual problem (2) is a convex optimization problem, since the objective to be maximized is concave and the constraint is convex. This is the case whether or not the primal problem (5.1) is convex.\nNote: the dual problem is always convex.\nWeak/ Strong duality The optimal value of the Lagrange dual problem, which we denote $d^\\star$, is, by definition, the best lower bound on $p^\\star$ that can be obtained from the Lagrange dual function. The inequality\n$$ d^\\star \\leq p^\\star$$\nwhich holds even if the original problem is not convex. This property is called weak duality.\nWe refer to the difference $p^\\star - d^\\star$ as the optimal duality gap of the original problem. Note that th optimal duality gap is always nonnegative.\nWe say that strong duality holds if\n$$ d^\\star = p^\\star$$\nNote that strong duality does not hold in general. But if the primal problem (11) is convex with $f_1, \u0026hellip;, f_k$ convex, we usually (but not always) have strong duality.\nSlater’s condition Slater’s condition: There exists an $x \\in \\mathbf{relint}, D$ such that $$f_i(x) \u0026lt; 0, \\quad i = 1,\u0026hellip;,m, \\quad Ax = b $$\nSuch a point is called strictly feasible.\nSlater\u0026rsquo;s theorem: If Slater\u0026rsquo;s condition holds for a convex problem, then the strong duality holds.\nComplementary slackness Suppose the strong duality holds. Let $x^\\star$ be a primal optimal and $(\\lambda^\\star, v^\\star)$ be a dual optimal point. This means that\nWe conclude that the two inequalities in this chain hold with equality. Since the inequality in the third line is an equality, we conclude that $x^\\star$ minimizes $L(x, \\lambda^\\star, v^\\star)$ over $x$.\nAnother important conclusion is\n$$ \\lambda_i^\\star f_i(x^\\star) = 0, \\quad i = 1,\u0026hellip;,m$$\nKKT optimality conditions We now assume that the functions $f_0, \u0026hellip;, f_m, h_1, \u0026hellip;,h_p$ are differentiable, but we make no assumptions yet about convexity.\nKKT conditions for nonconvex problems Suppose the strong duality holds. Let $x^\\star$ be a primal optimal and $(\\lambda^\\star, v^\\star)$ be a dual optimal point. Since $x^\\star$ minimizes $L(x, \\lambda^\\star, v^\\star)$ over $x$, it follows that its gradient must vanish at $x^\\star$, i.e.,\n$$ \\nabla f_0(x^\\star) + \\sum^m_{i=1}\\lambda_i^\\star \\nabla f_i(x^\\star) + \\sum^p_{i=1}v_i^\\star \\nabla h_i(x^\\star) = 0$$\nThe KKT conditions are the following: For any optimization problem with differentiable objective and constraint functions for which strong duality obtains, any pair of primal and dual optimal points must satisfy the KKT conditions.\nKKT conditions for convex problems When the primal problem is convex, the KKT conditions are also sufficient for the points to be primal and dual optimal. That is, if $f_i$ are convex and $h_i$ are affine, and $\\tilde{x}, \\tilde{\\lambda}, \\tilde{v}$ are any points that satisfy the KKT conditions\nthen $\\tilde{x}$ and $(\\tilde{\\lambda_i}, \\tilde{v_i})$ are primal and dual optimal, with zero duality gap. To see this, note that the first two conditions state that $\\tilde{x}$ is primal feasible. Since $\\tilde{\\lambda_i}$ ≥ 0, $L(x,\\tilde{\\lambda},\\tilde{v})$ is convex in $x$; the last KKT condition states that its gradient with respect to $x$ vanishes at $x = \\tilde{x}$, so it follows that $\\tilde{x}$ minimizes $L(x,\\tilde{\\lambda},\\tilde{v})$ over $x$. From this we conclude that\nThis shows that $\\tilde{x}$ and $(\\tilde{\\lambda},\\tilde{v})$ have zero duality gap, and therefore are primal and dual optimal.\nWe conclude the following:\n For any convex optimization problem with differentiable objective and constraint functions, any points that satisfy the KKT conditions are primal and dual optimal, and have zero duality gap. If a convex optimization problem with differentiable objective and constraint functions satisfies Slater’s condition, then the KKT conditions provide necessary and sufficient conditions for optimality: Slater’s condition implies that the optimal duality gap is zero and the dual optimum is attained, so $x$ is optimal iff there are $(\\lambda, v)$ that, together with $x$, satisfy the KKT conditions.  Solving the primal problem via the dual Note that if strong duality holds and a dual optimal solution $(\\lambda^\\star, v^\\star)$ exists, then any primal optimal point is also a minimizer of $L(x, \\lambda^\\star, v^\\star)$. This fact sometimes allows us to compute a primal optimal solution from a dual optimal solution.\nMore precisely, suppose we have strong duality and an optimal $(\\lambda^\\star, v^\\star)$ is known. Suppose that the minimizer of $L(x, \\lambda^\\star, v^\\star)$, i.e., the solution of\nis unique (For a convex problem this occurs). Then if the solution is primal feasible, it must be primal optimal; if it is not primal feasible, then no primal optimal point can exist, i.e., we can conclude that the primal optimum is not attained.\n Reference:\n Convex Optimization* by Stephen Boyd and Lieven Vandenberghe.  ","permalink":"https://tangliyan.com/blog/posts/convex2/","summary":"Consider an optimization problem in the standard form (we call this a primal problem):\nWe denote the optimal value of this as $p^\\star$. We don\u0026rsquo;t assume the problem is convex.\nThe Lagrange dual function We define the Lagrangian $L$ associated with the problem as $$ L(x,\\lambda, v) = f_0(x) + \\sum^m_{i=1}\\lambda_if_i(x) + \\sum^p_{i=1}v_ih_i(x)$$ We call vectors $\\lambda$ and $v$ the dual variables or Lagrange multiplier vectors associated with the problem (1).","title":"Introduction to Convex Optimization - Primal problem to Dual problem"},{"content":"Optimization problem All optimization problems can be written as:\nOptimization Categories   convex v.s. non-convex Deep Neural Network is non-convex\n  continuous v.s.discrete Most are continuous variable; tree structure is discrete\n  constrained v.s. non-constrained We add prior to make it a constrained problem\n  smooth v.s.non-smooth Most are smooth optimization\n  Different initialization brings different optimum (if not convex) Idea: Give up global optimal and find a good local optimal.\n  Purpose of pre-training: Find a good initialization to start training, and then find a better local optimal.\n  Relaxation: Convert to a convex optimization problem.\n  Brute force: If a problem is small, we can use brute force.\n  Affine sets A set $C \\subseteq \\mathbf R^n$ is affine if the line through any two distinct points in $C$ lies in $C$, i.e., if for any $x1$, $x2 \\in C$ and $\\theta \\in \\mathbf R$, we have $$\\theta x_1 + (1-\\theta) x_2 \\in C.$$\nNote: The line passing throught $x_1$ and $x_2$: $y=\\theta x_1 + (1-\\theta)x_2$.\nAffine combination We refer to a point of the form $\\theta_1 x_1 + \\theta_2 x_2 + \u0026hellip; + \\theta_k x_k$, where $\\theta_1 + \\theta_2 + \u0026hellip; + \\theta_k = 1$ as an affine combination of the points $x_1, x_2, \u0026hellip;, x_k$. An affine set contains every affine combination of its points.\nAffine hull The set of all affine combinations of points in some set $C \\subseteq \\mathbf R^n$ is called the affine hull of $C$, and denoted $\\mathbf{aff}, C$:\n$$ \\mathbf{aff}, C ={\\theta_1 x_1 + \\theta_2 x_2 + \u0026hellip; + \\theta_k x_k , | x_1, x_2, \u0026hellip;, x_k \\in C, \\theta_1 + \\theta_2 + \u0026hellip; + \\theta_k = 1}.$$\nThe affine hull is the smallest affine set that contains $C$, in the following sense: if $S$ is any affine set with $C \\subseteq S$, then $\\operatorname{aff} C \\subseteq S$.\nAffine dimension: We define the affine dimension of a set $C$ as the dimension of its affine hull.\nConvex Sets A set $C$ is convex if the line segment between any two points in $C$ lies in $C$, i.e., if for any $x1$, $x2 \\in C$ and any $\\theta$ with $0 \\leq \\theta \\leq 1$, we have $$\\theta x_1 + (1-\\theta) x_2 \\in C.$$\nRoughly speaking, a set is convex if every point in the set can be seen by every other point. Every affine set is also convex, since it contains the entire line between any two distinct points in it, and therefore also the line segment between the points.\nConvex combination We call a point of the form $\\theta_1 x_1 + \\theta_2 x_2 + \u0026hellip; + \\theta_k x_k$, where $\\theta_1 + \\theta_2 + \u0026hellip; + \\theta_k = 1$ and $\\theta_i \\geq 0, i = 1,2,\u0026hellip;k$, a convex combination of the points $x_1, \u0026hellip;, x_k$.\nConvex hull The convex hull of a set $C$, denoted $\\mathbf{conv} , C$, is the set of all convex combinations of points in $C$:\n$$ \\mathbf{conv}, C ={\\theta_1 x_1 + \\theta_2 x_2 + \u0026hellip; + \\theta_k x_k , | x_i \\in C, \\theta_i \\geq 0, i=1,\u0026hellip;,k, \\theta_1 + \\theta_2 + \u0026hellip; + \\theta_k = 1}.$$\nThe convex hull $\\operatorname{conv} C$ is always convex. It is the smallest convex set that contains $C$: If $B$ is any convex set that contains $C$, then $\\operatorname{conv} C \\subseteq B$.\nCones A set $C$ is called a cone, or nonnegative homogeneous, if for every $x \\in C$ and $\\theta \\geq 0$ we have $\\theta x \\in C$. A set $C$ is a convex cone if it is convex and a cone, which means that for any $x_1, x_2 \\in C$ and $\\theta_1, \\theta_2 \\geq 0$, we have\n$$\\theta_1 x_1 + \\theta_2 x_2 \\in C$$\nHyperplanes and halfspaces A hyperplane is a set of the form $${ x , | a^T x = b},$$\nwhere $a \\in \\mathbf R^n, a \\neq 0$, and $b \\in \\mathbf R$.\nThis geometric interpretation can be understood by expressing the hyperplane in the form\n$$ { x , | a^T (x - x_0) = 0}, $$ where $x_0$ is any point in the hyperplane.\nA hyperplane divides $\\mathbf R^n$ into two halfspaces. A (closed) halfspace is a set of the form\n$$ {x , | a^T x \\leq b }.$$\nwhere $x_0 \\neq 0$. Halfspaces are convex but not affine. The set $ {x | a^T \u0026lt; b }$ is called an open halfspace.\nPolyhedra A polyhedron is defined as the solution set of a finite number of linear equalities and inequalities:\n$$ P = { x, | a_j^T \\leq b_j, j=1,\u0026hellip;,m, c_j^T x = d_j, j = 1, \u0026hellip;, p}$$\nA polyhedron is thus the intersection of a finite number of halfspaces and hyperplanes. Here is the compact notations:\n$$ P = { x, | Ax \\preceq b, Cx=d}$$ Linearly Independent v.s. Affinely Independent Consider the vectors (1,0), (0,1) and (1,1). These are affinely independent, but not independent. If you remove any one of them, their affine hull has dimension one. In contrast, the span of any two of them is all of $\\mathbf R^2$, and hence these are not independent.\nSimplexes Suppose the $k+1$ points $v_0, \u0026hellip;, v_k \\in \\mathbf R^n$ are affinely independent, which means $v_1 - v_0, \u0026hellip;, v_k - v_0$ are linearly independent. The simplex determined by them is given by\n$$ C = \\mathbf{conv}{ v_0, \u0026hellip;, v_k} = { \\theta_0 v_0 + \u0026hellip; + \\theta_k v_k ,| \\theta \\succeq 0, \\mathbf 1^T \\theta = 1}$$\nNote:\n The affine dimension of this simplex is $k$.  A 1-dimensional simplex is a line segment; a 2-dimensional simplex is a triangle (including its interior); and a 3-dimensional simplex is a tetrahedron.\nWhat is the key distinction between a convex hull and a simplex? If the elements of the set on which the convex hull is defined are affinely independent, then the convex hull and the simplex defined on this set are the same. Otherwise, simplex can’t be defined on this set, but convex hull can.\nConvex Functions  A function $f: \\mathbf{R}^n \\rightarrow \\mathbf{R}$ is convex if dom $f$ is a convex set and if for all $x$, $y \\in \\mathbf{dom} , f$, and $\\theta$ with $ 0 \\leq \\theta \\leq 1$, we have  $$f(\\theta x + (1-\\theta)y) \\leq \\theta f(x) + (1-\\theta) f(y).$$\n  We say $f$ is concave is $-f$ is convex, and strictly concave if $-f$ is strictly convex.\n  A function is convex if and only if it is convex when restricted to any line that intersects its domain. In other words f is convex if and only if for all $x \\in \\mathbf{dom} , f$ and all $v$, the function $g(t) = f(x + tv)$ is convex (on its domain, ${t , | , x + tv \\in \\mathbf{dom} , f }$).\n  First-order conditions  Suppose $f$ is differentiable, then $f$ is convex if and only if $\\mathbf{dom} , f$ is convex and $$ f(y) \\geq f(x) + \\nabla f(x)^{T}(y-x)$$ holds for all $x,y \\in \\mathbf{dom} , f$    For a convex function, the first-order Taylor approximation is in fact a global underestimator of the function. Conversely, if the first-order Taylor approximation of a function is always a global underestimator of the function, then the function is convex.\n  The inequality shows that from local information about a convex function (i.e., its value and derivative at a point) we can derive global information (i.e., a global underestimator of it).\n  Second-order conditions   Suppose that $f$ is twice differentiable. The $f$ is convex if and only if $\\mathbf{dom} , f$ is convex and its Hessian is positive semidefinite: for all $x \\in \\mathbf{dom} f$, $$ \\nabla^2f(x) \\succeq 0.$$\n  $f$ is concave if and only if $\\mathbf{dom} f$ is convex and $\\nabla^2f(x) \\preceq 0$ for all $x \\in \\mathbf{dom} , f$.\n  If $ \\nabla^2f(x) \\succ 0$ for all $x \\in \\mathbf{dom} , f$, then $f$ is strictly convex. The converse is not true. e.x. $f(x) = x^4$ has zero second derivative at $x=0$ but is strictly convex.\n  Quadratic functions: Consider the quadratic function $f:\\mathbf{R}^n \\rightarrow \\mathbf{R}$, with $\\mathbf{dom} , f = \\mathbf{R}^n$, given by $$ f(x) = (1/2)x^{T}Px + q^Tx + r,$$ with $P \\in \\mathbf{S}^n, q \\in \\mathbf R^n$, and $r \\in \\mathbf{R}$. Since $\\nabla^2f(x) = P$ for all x, f is convex if and only if $P \\succeq 0$ (and concave if and only if $P \\preceq 0$).\n  Examples of Convex and Concave Functions   Exponential. $e^{ax}$ is convex on $\\mathbf{R}$, for any $a \\in \\mathbf{R}$.\n  Powers. $x^a$ is convex on $\\mathbf R_{++}$ when $a \\geq 1$ or $a \\leq 0$, and concave for $0 \\leq a \\leq 1$.\n  Powers of absolute value. $|x|^p$, for $p \\geq 1$, is convex on $\\mathbf R$.\n  Logarithm. $log , x$ is concave on $R_{++}$.\n  Negative Entropy. $x,log,x$ (either on $\\mathbf{R}{++}$, or on $\\mathbf R+$, defined as $0$ for $x = 0$) is convex.\n  Norms. Every norm on $\\mathbf{R}^n$ is convex.\n  Max function. $f(x) = max { x_1, \u0026hellip;, x_n}$ is convex on $\\mathbf R^n$.\n  Quadratic-over-linear function. The function $f(x,y) = x^2/y$, with $$ \\mathbf{dom} , f = \\mathbf R \\times \\mathbf R_{++} = { (x,y) \\in \\mathbf R^2, | y \u0026gt; 0},$$ is convex.\n  Log-sum-exp. The function $f(x) = log (e^{x_1} + · · · + e^{x_n} )$ is convex on $\\mathbf R^n$.\n  Geometric mean. The geometric mean $f(x) = (\\prod^n_{i = 1} x_i)^{1/n}$ is concave on $\\mathbf {dom} , f = \\mathbf S^n_{++}$\n  Log-determinant. The function $f(X) =\\mathrm{log , det ,} X$ is concave.\n   Reference:\n Convex Optimization* by Stephen Boyd and Lieven Vandenberghe.  ","permalink":"https://tangliyan.com/blog/posts/convex1/","summary":"Optimization problem All optimization problems can be written as:\nOptimization Categories   convex v.s. non-convex Deep Neural Network is non-convex\n  continuous v.s.discrete Most are continuous variable; tree structure is discrete\n  constrained v.s. non-constrained We add prior to make it a constrained problem\n  smooth v.s.non-smooth Most are smooth optimization\n  Different initialization brings different optimum (if not convex) Idea: Give up global optimal and find a good local optimal.","title":"Introduction to Convex Optimization - Basic Concepts"},{"content":"Bagging v.s. Boosting: Bagging: Leverages unstable base learners that are weak because of overfitting.\nBoosting: Leverages stable base learners that are weak because of underfitting.\nXGBoost Learning Process through XGBoost:\n How to set a objective function. Hard to solve directly, how to approximate. How to add tree structures into the objective function. Still hard to optimize, then have to use greedy algorithms.  Step 1. How to set an objective function Suppose we trained $K$ trees, then the prediction for the ith sample is $$\\hat{y}_i = \\sum_{k=1}^K f_k(x_i), f_k \\in \\mathcal{F}$$ where $K$ is the number of trees, $f$ is a function in the functional space $\\mathcal{F}$, and $\\mathcal{F}$ is the set of all possible CARTs.\nThe objective function to be optimized is given by $$\\text{obj} = \\sum_i^n l(y_i, \\hat{y}_i) + \\sum_{k=1}^K \\Omega(f_k)$$ The first term is the loss function and the second term controls trees' complexity. We see the undefined terms in this objective function are the loss function $l$ and model complexity $\\Omega (f_i)$. Functions $f_i$ are what we need to learn, each containing the structure of the tree and the leaf scores.\nwe use an additive strategy to build the model. That is, fix what we have learned, and add one new tree at a time. This is called Additive Training.\nNote that given $x_i$, the prediction $\\hat{y}_i^{(K)} = \\hat{y}_i^{(K-1)} + f_K(x_i)$, where the term $\\hat{y}_i^{(K-1)}$ is known and only $f_K(x_i)$ is unknown.\nNow, we can rewrite the objective function as Step 2. Hard to solve directly, how to approximate If the loss function is the MSE, then the objective function has a nice form. But for a general loss function, it is hard to optimize directly. Therefore, we need to approximate it. Here, we use second order Taylor approximation\nSo we can re-write the objective function as If we define constant term $g_i$, $h_i$ as $$g_i = \\partial_{\\hat{y}_i^{(K-1)}} l(y_i, \\hat{y}i^{(K-1)}), \\ \\ f_i = \\partial{\\hat{y}_i^{(K-1)}}^2 l(y_i, \\hat{y}i^{(K-1)})$$ Also, since previous $K-1$ trees are fixed, we have $\\sum{i=1}^n l(y_i, \\hat{y}_i^{(K-1)})$ fixed. Then we can write the objective function as\nNow, all previous trees' information is stored in terms $g_i$ and $h_i$. That\u0026rsquo;s how the $K$th tree is related to previous trees. Our new objective function now is $$ \\text{obj}_K = \\sum_{i=1}^n [g_if_K(x_i) + \\frac{1}{2}h_if_K^2(x_i)] + \\Omega(f_K)$$ The unknown at this point are $f_K$ and $\\Omega(f_K)$.\nStep 3. How to add tree structures into the objective function Redefine a tree:\n Define $I_j = {i|q(x_i)=j}$ to be the set of indices of data points assigned to the 𝑗-th leaf. $T$ is the number of leaves. $q(x_i)$ is which leaf the $i$-th sample is assigned to. Term $q(x)$ defines the tree structure. $w_{q(x_i)}$ is the score of the assigned leaf. In XGBoost, we define the complexity as $\\Omega(f) = \\gamma T + \\frac{1}{2}\\lambda \\sum_{j=1}^T w_j^2$  Now the revised objective function is\nDefine $G_j = \\sum_{i\\in I_j} g_i$ and $H_j = \\sum_{i\\in I_j} h_i$ constants. The the objective function is $$\\text{obj}{K} = \\sum^T{j=1} [G_jw_j + \\frac{1}{2} (H_j+\\lambda) w_j^2] +\\gamma T$$\nRight now, the only unknown is $w_j$, which is independent respect to other terms. Take the derivative and set it to zero, we have $$w_j = -\\frac{G_j}{H_j+\\lambda}$$ Then the objective function achieve its minimum at $$ \\text{obj}K = -\\frac{1}{2} \\sum{j=1}^T \\frac{G_j^2}{H_j+\\lambda} + \\gamma T$$\nStep 4. Still hard to optimize, then have to use greedy algorithms For any tree structure, we are now able to find the minimum value of the objective function. Ideally we would brute force all possible tree structures and pick the one with the minimum objective function. However, this is impossible in practice and we use greedy algorithm instead. When we build decision trees, we use entropy to calculate the information gain. Now we are still going to use information gain, but the formula changes to\nwhere $L$ is the left leaf and $R$ is the right leaf.\n Reference:\n https://xgboost.readthedocs.io/en/latest/tutorials/model.html  ","permalink":"https://tangliyan.com/blog/posts/xgboost/","summary":"Bagging v.s. Boosting: Bagging: Leverages unstable base learners that are weak because of overfitting.\nBoosting: Leverages stable base learners that are weak because of underfitting.\nXGBoost Learning Process through XGBoost:\n How to set a objective function. Hard to solve directly, how to approximate. How to add tree structures into the objective function. Still hard to optimize, then have to use greedy algorithms.  Step 1. How to set an objective function Suppose we trained $K$ trees, then the prediction for the ith sample is $$\\hat{y}_i = \\sum_{k=1}^K f_k(x_i), f_k \\in \\mathcal{F}$$ where $K$ is the number of trees, $f$ is a function in the functional space $\\mathcal{F}$, and $\\mathcal{F}$ is the set of all possible CARTs.","title":"XGBoost"},{"content":"MLE v.s. MAP  MLE: learn parameters from data. MAP: add a prior (experience) into the model; more reliable if data is limited. As we have more and more data, the prior becomes less useful. As data increase, MAP $\\rightarrow$ MLE.  Notation: $D = {(x_1, y_1), (x_2, y_2), \u0026hellip;, (x_n, y_n)}$\nFramework:\n MLE: $\\mathop{\\rm arg\\ max} P(D \\ |\\ \\theta)$ MAP: $\\mathop{\\rm arg\\ max} P(\\theta \\ |\\ D)$  Note that taking a product of some numbers less than 1 would approaching 0 as the number of those numbers goes to infinity, it would be not practical to compute, because of computation underflow. Hence, we will instead work in the log space.\nComparing both MLE and MAP equation, the only thing differs is the inclusion of prior $P(\\theta)$ in MAP, otherwise they are identical. What it means is that, the likelihood is now weighted with some weight coming from the prior.\nIf the prior follows the normal distribution, then it is the same as adding a $L2$ regularization.\nWe assume $P(\\theta) \\sim \\mathcal{N}(0, \\sigma^2)$, then $P(x) = \\frac{1}{\\sigma \\sqrt {2\\pi}}exp(-\\frac{\\theta^2}{2\\sigma^2})$\nIf the prior follows the Laplace distribution, then it is the same as adding a $L1$ regularization.\nWe assume $P(\\theta) \\sim Laplace \\ (\\mu=0, b)$, then $P(\\theta) = \\frac{1}{2b}exp(-\\frac{|\\theta|}{b})$\n","permalink":"https://tangliyan.com/blog/posts/mle/","summary":"MLE v.s. MAP  MLE: learn parameters from data. MAP: add a prior (experience) into the model; more reliable if data is limited. As we have more and more data, the prior becomes less useful. As data increase, MAP $\\rightarrow$ MLE.  Notation: $D = {(x_1, y_1), (x_2, y_2), \u0026hellip;, (x_n, y_n)}$\nFramework:\n MLE: $\\mathop{\\rm arg\\ max} P(D \\ |\\ \\theta)$ MAP: $\\mathop{\\rm arg\\ max} P(\\theta \\ |\\ D)$  Note that taking a product of some numbers less than 1 would approaching 0 as the number of those numbers goes to infinity, it would be not practical to compute, because of computation underflow.","title":"Maximum Likelihood Estimation (MLE) and Maximum a Posteriori (MAP)"},{"content":"Generative model v.s. Discriminative model： Examples:\n Generative model: Naive Bayes, HMM, VAE, GAN. Discriminative model：Logistic Regression, CRF.  Objective function:\n Generative model: $max \\ p \\ (x,y)$ Discriminative model：$max \\ p \\ (y \\ |\\ x)$  Difference:\n Generative model: We first assume a distribution of the data in the consideration of computation efficiency and features of the data. Next, the model will learn the parameters of distribution of the data. Then, we can use the model to generate new data. (e.g. generate new data from a normal distribution.) Discriminative model: The only purpose is to classify, that is, to tell the difference. As long as it can find a way to tell the difference, it doesn\u0026rsquo;t need to learn anything else.  Relation:\n Generative model: $p\\ (x,y) = p\\ (y \\ |\\ x) \\ p(y)$, it has a prior term $p(y)$. Discriminative model：$p\\ (y \\ |\\ x)$ Both models can do classification problems, but discriminative model can do classification problems only. Usually, for classification problems, discriminative model performs better. On the other hand, if we have limited data, generative model might perform better (since it has a prior term, which plays the role of a regularization).  Logistic regression Formula:\n$$ \\sigma(x) = \\frac{1}{1 + e^{-w^Tx}}$$\nDerivative formula:\n $\\sigma'(x) = \\sigma(x)(1-\\sigma(x))$  Logistic Regression does not have analytic solutions and we need to use iterative optimization to find a solution recursively.\nIt spends a lot of computational power to calculate $e^x$ because of floating points. In most code implementation, people will pre-calculate values and then do approximate with a new $x$ comes.\nDerivation of Logistic Regression $$ \\begin{aligned} p(y | x, w, b) \u0026amp;=p(y=1 | x, w, b)^{y}[1-p(y=1 | x, w, b)]^{1-y} \\\\ \\hat{w}_{MLE}, \\hat{b}_{MLE} \u0026amp;= \\argmax_{w, b} \\prod_{i=1}^{n} p\\left(y_{i} | x_{i}, w, b\\right) \\\\ \u0026amp;=\\argmax_{w, b} \\sum_{i=1}^{n} \\log p\\left(y_{i} | x_{i}, w, b\\right) \\\\ \u0026amp;=\\argmin_{w, b}-\\sum_{i=1}^{n} \\log p\\left(y_{i} | x_{i}, w, b\\right) \\\\ \u0026amp;=\\argmin_{w, b}-\\sum_{i=1}^{n} \\log \\left[p\\left(y_{i}\\right) | x_{i}, w, b\\right)^{y_{i}}\\left[1-p\\left(y_{i}=1 | x_{2}, w, b\\right)^{-y_{i}}\\right] \\\\ \u0026amp;=\\argmin_{w, b}-\\sum_{i=1}^{n} y_{i} \\log p\\left(y_{i}=1 | x_{i}, w, b\\right)+\\left(1-y_{i}\\right) \\log \\left[1-p\\left(y_{i}=1 | x_{i} w, b\\right)\\right] \\\\ \u0026amp;=\\argmin_{w, b} L \\ \\frac{\\partial L}{\\partial b} \u0026amp;= -\\sum_{i=1}^{n}\\left(y_{i} \\cdot \\frac{\\sigma (w^{T}x_{i} + b) \\cdot [1-\\sigma (w^{T}x_{i} + b)]}{\\sigma(w^{T}x_i+b)}-(1-y_i) \\cdot \\frac{-\\sigma\\left(w^{T} x_{i}+b\\right)\\left[1-\\sigma\\left(w^{T} x_{i}+b\\right)\\right]}{1-\\sigma\\left(w ^{T}x_i+b\\right)}\\right] \u0026amp; \\\\ \u0026amp;=-\\sum_{i=1}^{n}\\left(y_{i}\\left[1-\\sigma\\left(w^{T} x_{i}+b\\right)\\right]-\\left(1-y_{i}\\right)\\sigma(w^{T}x_i + b) \\right) \\\\ \u0026amp;= \\sum_{i=1}^{n}\\left(\\sigma\\left(w^{T} x_{i}+b\\right)-y_{i}\\right)\\ \\frac{\\partial L}{\\partial w}\u0026amp;=\\sum_{i=1}^{n}\\left(\\sigma\\left(w^{T} x_{i}+b\\right)-y_{i}\\right) \\cdot x_{i} \\end{aligned} $$\nGradient: True gradient: The true gradient of the population.\nEmpirical gradient: Use sample gradient to approximate true gradient.\n SGD, approximate bad. Asymptotically converge to true gradient. GD, approximate good. mini-batch DG, middle.  For smooth optimization, we can use gradient descent.\nFor non-smooth optimization, we can use coordinate descent (e.g. L1).\nMini-Batch Gradient Descent for Logistic Regression Way to prevent overfitting:  More data. Regularization. Ensemble models. Less complicate models. Less Feature. Add noise (e.g. Dropout)  L1 regularization L1: Feature Selection, PCA: Features changed.\nWhy prefer sparsity:\n reduce dimension, then less computation. Higher interpretability.  Problem of L1:\n Group Effect: If there is collinearity in features, L1 will randomly choose one from each group. Therefore, the best feature in each group might not be selected.  Coordinate Descent for Lasso   Intuition: If we are at a point $x$ such that $f(x)$ is minimized along each coordinate axis, then we find a global minimizer.\n  step of Coordinate Descent: Note that we don\u0026rsquo;t need a learning rate here since we are finding the optimum values.\n  Large eigenvalue by co-linear columns If a matrix 𝐴 has an eigenvalue $\\lambda$, then $A^{-1}$ have an eigenvalue $\\frac{1}{\\lambda}$\n$$A\\mathbf{v} = \\lambda\\mathbf{v} \\implies A^{-1}A\\mathbf{v} = \\lambda A^{-1}\\mathbf{v}\\implies A^{-1}\\mathbf{v} = \\frac{1}{\\lambda}\\mathbf{v}$$\nIf a matrix $A = X^{T}X$ has a very small eigenvalue, then its inverse $(X^{T}X)^{-1}$ has a very big eigenvalue. We know that the formula for $\\beta$ is $$\\hat \\beta = (X^{T}X)^{-1}X^{T}y$$\nIf we plug in $y = X\\beta^{} + \\epsilon$, then $$\\hat \\beta = \\beta^{} + (X^{T}X)^{-1}X^{T}\\epsilon$$\nMultiplying the noise by $(X^{T}X)^{-1}$ has the potential to blow up the noise. This is called \u0026ldquo;noise amplification\u0026rdquo;. If we set the error term $\\epsilon$ to zero, then there is no noise to amplify. Hence there are no problems with the huge eigenvalues of $(X^{T}X)^{-1}$, and we still recover the correct answer. But even a little bit of error, and this goes out the window.\nIf we now add some regularization (aka weight decay), then $$\\hat \\beta = (X^{T}X + \\lambda I)^{-1}X^{T}y$$\nAdding a small multiple of the identity to $X^{T}X$ barely changes the large eigenvalues, but it drastically changes the smallest eigenvalue \u0026ndash; it increases it to $\\lambda$. Thus in the inverse, the largest eigenvalue will be at most $\\frac{1}{\\lambda}$.\nBuilding Models with prior knowledge (put relation into models via regularization):  model + $\\lambda \\cdot$ Regularization Constraint optimization Probabilistic model (e.g. Probabilistic Graph Model (PGM))  ","permalink":"https://tangliyan.com/blog/posts/logistic_regression/","summary":"Generative model v.s. Discriminative model： Examples:\n Generative model: Naive Bayes, HMM, VAE, GAN. Discriminative model：Logistic Regression, CRF.  Objective function:\n Generative model: $max \\ p \\ (x,y)$ Discriminative model：$max \\ p \\ (y \\ |\\ x)$  Difference:\n Generative model: We first assume a distribution of the data in the consideration of computation efficiency and features of the data. Next, the model will learn the parameters of distribution of the data.","title":"Logistic Regression, L1, L2 regularization, Gradient/Coordinate descent"}]