<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Cross-Lingual Learning | Liyan Tang</title>
<meta name=keywords content="NLP,MULTI-LINGUAL">
<meta name=description content="Cross-lingual learning Most languages do not have training data available to create state-of-the-art models and thus our ability to create intelligent systems for these languages is limited as well.
Cross-lingual learning (CLL) is one possible remedy to solve the lack of data for low-resource languages. In essence, it is an effort to utilize annotated data from other languages when building new NLP models. When CLL is considered, target languages usually lack resources, while source languages are resource-rich and they can be used to improve the results for the former.">
<meta name=author content>
<link rel=canonical href=https://tangliyan.com/blog/posts/corss_lingual/>
<link crossorigin=anonymous href=/blog/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css integrity="sha256-yIlj/i15RiAA/Q+xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/blog/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://tangliyan.com/blog/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://tangliyan.com/blog/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://tangliyan.com/blog/favicon-32x32.png>
<link rel=apple-touch-icon href=https://tangliyan.com/blog/apple-touch-icon.png>
<link rel=mask-icon href=https://tangliyan.com/blog/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.92.0">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-202974782-1','auto'),ga('send','pageview'))</script><meta property="og:title" content="Cross-Lingual Learning">
<meta property="og:description" content="Cross-lingual learning Most languages do not have training data available to create state-of-the-art models and thus our ability to create intelligent systems for these languages is limited as well.
Cross-lingual learning (CLL) is one possible remedy to solve the lack of data for low-resource languages. In essence, it is an effort to utilize annotated data from other languages when building new NLP models. When CLL is considered, target languages usually lack resources, while source languages are resource-rich and they can be used to improve the results for the former.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://tangliyan.com/blog/posts/corss_lingual/"><meta property="og:image" content="https://tangliyan.com/blog/papermod-cover.png"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2020-12-28T00:00:00+00:00">
<meta property="article:modified_time" content="2020-12-28T00:00:00+00:00">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://tangliyan.com/blog/papermod-cover.png">
<meta name=twitter:title content="Cross-Lingual Learning">
<meta name=twitter:description content="Cross-lingual learning Most languages do not have training data available to create state-of-the-art models and thus our ability to create intelligent systems for these languages is limited as well.
Cross-lingual learning (CLL) is one possible remedy to solve the lack of data for low-resource languages. In essence, it is an effort to utilize annotated data from other languages when building new NLP models. When CLL is considered, target languages usually lack resources, while source languages are resource-rich and they can be used to improve the results for the former.">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://tangliyan.com/blog/posts/"},{"@type":"ListItem","position":2,"name":"Cross-Lingual Learning","item":"https://tangliyan.com/blog/posts/corss_lingual/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Cross-Lingual Learning","name":"Cross-Lingual Learning","description":"Cross-lingual learning Most languages do not have training data available to create state-of-the-art models and thus our ability to create intelligent systems for these languages is limited as well.\nCross-lingual learning (CLL) is one possible remedy to solve the lack of data for low-resource languages. In essence, it is an effort to utilize annotated data from other languages when building new NLP models. When CLL is considered, target languages usually lack resources, while source languages are resource-rich and they can be used to improve the results for the former.","keywords":["NLP","MULTI-LINGUAL"],"articleBody":"Cross-lingual learning Most languages do not have training data available to create state-of-the-art models and thus our ability to create intelligent systems for these languages is limited as well.\nCross-lingual learning (CLL) is one possible remedy to solve the lack of data for low-resource languages. In essence, it is an effort to utilize annotated data from other languages when building new NLP models. When CLL is considered, target languages usually lack resources, while source languages are resource-rich and they can be used to improve the results for the former.\nCross-lingual resources The domain shift, i.e. the difference between source and target languages, is often quite severe. The languages might have different vocabularies, syntax or even alphabets. Various cross-lingual resources are often employed to address the gap between languages.\nHere is a short overview of different resources that might be used.\nMultilingual distributional representations With multilingual word embeddings (MWE), words from multiple languages share one semantic vector space. In this space semantically similar words are close together independently on the language they come from.\nDuring the training MWE usually require additional cross-lingual resources, e.g. bilingual dictionaries or parallel corpora. Multilingual sentence embeddings work on similar principle, but they use sentences instead of words. Ideally, corresponding sentences should have similar representations.\nEvaluation of multilingual distributional representations Evaluation of multilingual distributional representations can be done either intrinsically or extrinsically.\n  With intrinsic evaluation authors usually measure how well does semantic similarity reflect in the vector space, i.e. how far apart are semantically similar words or sentences.\n  On the other hand, extrinsic evaluation measure how good are the representations for downstream tasks, i.e. they are evaluated on how well they perform for CLL.\n  Parallel corpus Parallel corpora are one of the most basic linguistic resources. In most cases, sentence-aligned parallel corpus of two languages is used. Wikipedia is sometimes used as a comparable parallel corpus, although due to its complex structure it can also be used as a multilingual knowledge base.\nParallel corpora are most often created for specific domains, such as politics, religion or movie subtitles. Parallel corpora are also used as training sets for machine translation systems and for creating multilingual distributional representations, which makes parallel corpora even more important.\nWord Alignments In some cases, sentence alignment in parallel corpora might not be enough.\nFor word alignments, one word from a sentence in language $\\ell_A$ can be aligned with any number of words from corresponding sentence in language $\\ell_B$. In most cases automatic tools are used to perform word alignment over existing parallel sentences. Machine Translation systems can also often provide word alignment information for their generated sentences.\nMachine Translation Machine translation (MT) can be used instead of parallel corpora to generate parallel sentences. Parallel corpus generated by MT is called pseudo parallel corpus. Although in recent years MT achieved great improvements by using neural encoder-decoder models, machine translation is still far away from providing perfect translations. MT models are usually trained from parallel corpora.\nBy using samples generated by MT systems we inevitably inject noise into our models; The domain shift between a language $\\ell_A$ and what MT systems generate as language $\\ell_A$ needs to be addressed.\nUniversal features (out of fashion) Universal features are inherently language independent to some extent, e.g. emojis or punctuation. These can be used as features for any language. As such, model trained with such universal features should be easily applied to other languages.\nThe process of creating language independent features for words is called delexicalization. Delexicalized text has words replaced with universal features, such as POS tags. We lose the lexical information of the words in this process, thus the name delexicalization.\nBilingual dictionary Bilingual dictionaries are the most available cross-lingual resource in our list. They exist for many language pairs and provide a very easy and natural way of connecting words from different languages. However, they are often incomplete and context insensitive.\nPre-trained multilingual language models Pre-trained language models are a state-of-the-art NLP technique. A large amount of text data is used to train a high capacity language model. Then we can use the parameters from this language model to initialize further training with different NLP tasks. The parameters are fine-tuned with the additional target task data. This is a form of transfer learning, where we use language modeling as a source task. The most well known pre-trained language models are BERT.\nMultilingual language models (MLMs) are an extension of this concept. A single language model is trained with multiple languages at the same time.\n  This can be done without any cross-lingual supervision, i.e. we feed the model with text from multiple languages and we do not provide the model with any additional information about the relations between the languages. Such is the case of multilingual BERT model (mBERT). Interestingly enough, even with no information about how are the languages related, the representations this model creates are partially language independent. The model is able to understand the connections between languages even without being explicitly told to do so. To know more about mBERT, check my previous post about common MLMs. \n  The other case are models that directly work with some sort of cross-lingual supervision, i.e. they use data that help them establish a connection between different languages. Such is the case of XLM, which make use of parallel corpora and machine translation to directly teach the model about corresponding sentences. To know more about XLM, check my previous post about common MLMs. \n  Transfer learning techniques for Cross-lingual Learning Four main categories for CLL:\n Label transfer: Labels or annotations are transferred between corresponding $L_S$ and $L_T$ samples. Feature transfer: Similar to label transfer, but sample features are transferred instead (transfer knowledge about the features of the sample). parameter transfer: Parameter values are transferred between parametric models. This effectively transfers the behaviour of the model. Representation transfer: The expected values for hidden representation are transferred between models. The target model is taught to create desired representations.  Note: Representation transfer is similar to feature transfer. However, instead of simply transferring the features, it teaches the $L_T$model to create these features instead.\nLabel transfer Transferring labels means transferring these labels between the samples from different languages. First, a correspondence is established between $L_S$ and $L_T$ samples. Correspondence means that the pair of samples should have the same label. Then the labels are transferred from one language to the other â€“ this step is also called annotation projection. The projected labels can than be used for further training.\nThere are three types of distinct label transfer and they differ in the language the resulting model takes as an input.\nTransferring labels during training (To train an $L_T$ model) To create an $L_T$ model with label transfer we need an existing annotated dataset or model in $L_S$. Then we establish a correspondence between $L_S$ annotated samples and $L_T$ unannotated samples. The annotations are projected to $L_T$ and resulting distantly supervised $L_T$ dataset is used to train an $L_T$ model.\nWhen machine translation is used to generate the corresponding samples we can\n either take existing $L_S$ samples and translate them into $L_T$ along with their annotations. Or, we can take unannotated $L_T$ data, translate them into $L_S$ and then annotate these translated $L_S$ samples, and finally, the annotations are projected back to the original samples. The former is the more frequent alternative.  Transferring labels during inference (Use existing $L_S$ for inference) An unannotated $L_T$ sample has a corresponding $L_S$ sample generated (e.g. by MT) and then a pre-existing $L_S$ model is used to infer its label. This label is then projected back to the original $L_T$ sample. This approach is quite slow and prone to errors.\nParallel Model This is the least frequent type of label transfer, so I skip it here.\nCommons of label transfer techniques  All label-based transfer techniques require a process to generate corresponding samples. Two main approaches are using machine translation and parallel corpora.   Machine translation. The biggest disadvantage is that MT systems are still not perfect and only generate very specific dialects of the output languages. This shift between the natural lan- guage and the generated language is a source of noise in CLL. parallel corpus. Parallel corpora can be used to avoid the problem with noisy translation. In parallel corpus both sides are written in natural language. We can then use existing model to annotate the LS side of the corpus and then project the labels to the other half. However, the annotations from the existing model is the source of noise as well. Usually NLP models have limited accuracy and some samples will be mislabeled. Manually labeled parallel corpora exist, but they are very rare.   Cross-lingual projection of the labels is the step when the transfer of knowledge between languages happens for label-based approaches.\n  $L_S$ model can be trained with additional data translated from $L_T$. This can improve the results during inference, since the model was already exposed to the translated data during training and does not suffer from the domain shift that much.\n  Label based transfer is notoriously noisy. It consists of several steps and each step can be a source of noise, e.g. imperfect machine translations, imperfect word alignments, imperfect pre-existing models, domain shift between parallel corpora and testing data, etc.\n  Feature Transfer This is not a frequent type of transfer, so I skip it.\nParameter transfer In parameter transfer, the behavior of the model is transfered. The transfer of knowledge happens only on shared layers. The most important technologies for parameter transfer are different language independent representations and pre-trained multilingual language models. There are three scenarios for parameter transfer.\nZero-shot transfer No $L_T$ data are used during the training. We train the model with $L_S$ only and then apply it to other languages. This is sometimes called direct transfer or model transfer. It is most often used together with language independent representations (e.g. MWE or MLMs. Contextualized word embedding from BERT is an example of MWE).\nJoint learning Both $L_S$ and $L_T$ data are used during the training, and they are used at the same time. The $L_T$ and $L_S$ models share a subset of their parameters, i.e. when one model updates its parameters, it affects the other model(s) as well. This technique of working with parameters is called parameter sharing.\u000f\nThere are two strategies of training:\n  Mixed dataset, which can be applied only when all the parameters are shared. During this strategy the training samples for all the languages are mixed into one training set. Then during the training, one batch can contain samples from multiple languages.\n  Alternate training, which can be applied even when only a subset of parameters is shared. During this strategy, the batch is sampled from one language only. Then this batch is run through the language-specific model and the parameter update is propagated to other models, that share some of the parameters.\n  Parameter sharing strategies (for each layer):\n Shared. The parameters are shared between multiple languages.\u000f Partially-private. Part of the layer isshared, while the other part is private. The most common way to implement this strategy is to have two distinct parallel layers, one with private strategy and the other with shared strategy. Then, the output of these two layers is concatenated.\u000f Private. The parameters are specific for one language.  The sharing of parameters can be realized in two ways:\n Hard sharing. With hard sharing, the shared layers are exactly the same. Soft sharing. Parameters can also be bound by a regularization term instead. E.g., add an additional term $\\left|\\theta_{S}-\\theta_{T}\\right|{2}^{2},$ where $\\theta{S}$ and $\\theta_{T}$ are the shared parameters for source model and target model, respectively.  Three different parameter sharing strategies: a) Mixed batch with samples from both languages ($X_{ST}$) is processed by the model. All theparameters are shared betweenLSandLTLoss functionJSTis used for these batches.\nb) Alternate training is used with all parameters shared, each batch is created in either $L_S$ or $L_T$. Loss function $J_{ST}$ is still the same for both cases.\nc) Alternate training with only a subset of parameters shared. In this example, the second layer is language-specific(private), while the other layers are shared. Loss function is calculated differently for each language.\nCascade learning Both $L_S$ and $L_T$ data are used during the training, but not at the same time. Instead we pre-train a model with $L_S$ data (or simply take an existing $L_S$ model) and then fine-tune it with $L_T$ data.\nPre-trained multilingual language models Pre-trained multilingual language models can be used for parameter based transfer as well.\n We can use them as a source of multilingual distributed representations and then build a model on these representations. We can use the MLM as an initialization of a multilingual model. Then we fine-tune the parameters to perform a target task.  Multilingual language models were able to achieve state-of-the-art results recently and they might become the predominant cross-lingual learning paradigm in the near future.\nRepresentation transfer With representation transfer, the knowledge about how hidden representations should look like within a model is transferred. It is a technique to extend other approaches. It is often used with deep models that use hidden representations during the calculation. This technique is often using corresponding samples or words from different languages, i.e. we usually need a parallel corpus or a bilingual dictionary.\nFuture Directions   Multilingual datasets. We consider the lack of multilingual datasets to be currently the biggest challenge for CLL. We believe that it is important to provide standardized variants of the datasets in the future for better reproducibility in these additional settings as well.\n  Standardization of linguistic resources. It is hard to compare between various resources when the corpora they are trained on might differ. It is then hard to distinguish, whether an eventual performance improvement comes from a better method or from a better corpus used for training.\n  Pre-trained multilingual language models. Training, fine-tuning and inference are all costly for MLMs. We expect to see methods that optimize the training and inference of these behemoths. As of today, the simple $L_S$ fine-tuning might lead to catastrophic forgetting, including forgetting related to the cross-lingual knowledge.\n  Truly low-resource languages and excluded languages. The CLL methods often rely on an existence of various linguistic resources, such as parallel corpora or MT systems. However,truly low-resource languages might not have these resources in this quantity and/or quality.The fact that methods are currently evaluated on resource-rich languages might then create unrealistic expectations about how well would the methods work on truly low-resource languages.\n  Curse of Multilinguality. Researchers report that adding more languages help initially, but after certain number the performance of the models actually starts to degrade. This is the curse of multilinguality, the apparent inability of models to learn too many languages. This curse is caused by a limited capacity of current parametric models. It can be addressed by increasing the capacity of the models, but the models are costly to train even today and increasing their size even further has only diminishing returns.\n  Combination with multitask learning. We believe,that a combination of cross-lingual and cross-task supervision might lead to more universal models, that are able to solve multiple tasks in multiple languages and then also generalize their broad knowledge into new tasks and new languages more easily.\n  Machine translation. One open question is how to mitigate the domain shift between natural languages and languages generated by MT systems.\n   Reference\n Cross-lingual learning for text processing: A survey. https://www.sciencedirect.com/science/article/pii/S0957417420305893?via%3Dihub  ","wordCount":"2557","inLanguage":"en","datePublished":"2020-12-28T00:00:00Z","dateModified":"2020-12-28T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://tangliyan.com/blog/posts/corss_lingual/"},"publisher":{"@type":"Organization","name":"Liyan Tang","logo":{"@type":"ImageObject","url":"https://tangliyan.com/blog/favicon.ico"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://tangliyan.com/blog/ accesskey=h title="Liyan Tang (Alt + H)">Liyan Tang</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
<li>
<a href=https://tangliyan.com/blog/archives title=Archive>
<span>Archive</span>
</a>
</li>
<li>
<a href=https://tangliyan.com/blog/tags/ title=Tags>
<span>Tags</span>
</a>
</li>
<li>
<a href=https://tangliyan.com/blog/search/ title="Search (Alt + /)" accesskey=/>
<span>Search</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<div class=breadcrumbs><a href=https://tangliyan.com/blog/>Home</a>&nbsp;Â»&nbsp;<a href=https://tangliyan.com/blog/posts/>Posts</a></div>
<h1 class=post-title>
Cross-Lingual Learning
</h1>
<div class=post-meta><span title="2020-12-28 00:00:00 +0000 UTC">December 28, 2020</span>&nbsp;Â·&nbsp;13 min
</div>
</header> <div class=toc>
<details>
<summary accesskey=c title="(Alt + C)">
<span class=details>Table of Contents</span>
</summary>
<div class=inner><ul>
<li>
<a href=#cross-lingual-learning aria-label="Cross-lingual learning">Cross-lingual learning</a></li>
<li>
<a href=#cross-lingual-resources aria-label="Cross-lingual resources">Cross-lingual resources</a><ul>
<li>
<a href=#multilingual-distributional-representations aria-label="Multilingual distributional representations">Multilingual distributional representations</a><ul>
<li>
<a href=#evaluation-of-multilingual-distributional-representations aria-label="Evaluation of multilingual distributional representations">Evaluation of multilingual distributional representations</a></li></ul>
</li>
<li>
<a href=#parallel-corpus aria-label="Parallel corpus">Parallel corpus</a></li>
<li>
<a href=#word-alignments aria-label="Word Alignments">Word Alignments</a></li>
<li>
<a href=#machine-translation aria-label="Machine Translation">Machine Translation</a></li>
<li>
<a href=#universal-features-out-of-fashion aria-label="Universal features (out of fashion)">Universal features (out of fashion)</a></li>
<li>
<a href=#bilingual-dictionary aria-label="Bilingual dictionary">Bilingual dictionary</a></li>
<li>
<a href=#pre-trained-multilingual-language-models aria-label="Pre-trained multilingual language models">Pre-trained multilingual language models</a></li></ul>
</li>
<li>
<a href=#transfer-learning-techniques-for-cross-lingual-learning aria-label="Transfer learning techniques for Cross-lingual Learning">Transfer learning techniques for Cross-lingual Learning</a><ul>
<li>
<a href=#label-transfer aria-label="Label transfer">Label transfer</a><ul>
<li>
<a href=#transferring-labels-during-training-to-train-an-l_t-model aria-label="Transferring labels during training (To train an $L_T$ model)">Transferring labels during training (To train an $L_T$ model)</a></li>
<li>
<a href=#transferring-labels-during-inference-use-existing-l_s-for-inference aria-label="Transferring labels during inference (Use existing $L_S$ for inference)">Transferring labels during inference (Use existing $L_S$ for inference)</a></li>
<li>
<a href=#parallel-model aria-label="Parallel Model">Parallel Model</a></li>
<li>
<a href=#commons-of-label-transfer-techniques aria-label="Commons of label transfer techniques">Commons of label transfer techniques</a></li></ul>
</li>
<li>
<a href=#feature-transfer aria-label="Feature Transfer">Feature Transfer</a></li>
<li>
<a href=#parameter-transfer aria-label="Parameter transfer">Parameter transfer</a><ul>
<li>
<a href=#zero-shot-transfer aria-label="Zero-shot transfer">Zero-shot transfer</a></li>
<li>
<a href=#joint-learning aria-label="Joint learning">Joint learning</a></li>
<li>
<a href=#cascade-learning aria-label="Cascade learning">Cascade learning</a></li>
<li>
<a href=#pre-trained-multilingual-language-models-1 aria-label="Pre-trained multilingual language models">Pre-trained multilingual language models</a></li></ul>
</li>
<li>
<a href=#representation-transfer aria-label="Representation transfer">Representation transfer</a></li></ul>
</li>
<li>
<a href=#future-directions aria-label="Future Directions">Future Directions</a>
</li>
</ul>
</div>
</details>
</div>
<div class=post-content><h3 id=cross-lingual-learning>Cross-lingual learning<a hidden class=anchor aria-hidden=true href=#cross-lingual-learning>#</a></h3>
<p>Most languages do not have training data available to create state-of-the-art models and thus our ability to create intelligent systems for these languages is limited as well.</p>
<p>Cross-lingual learning (CLL) is one possible remedy to solve the lack of data for low-resource languages. In essence, it is an effort to utilize annotated data from other languages when building new NLP models. When CLL is considered, target languages usually lack resources, while source languages are resource-rich and they can be used to improve the results for the former.</p>
<h3 id=cross-lingual-resources>Cross-lingual resources<a hidden class=anchor aria-hidden=true href=#cross-lingual-resources>#</a></h3>
<p>The <strong>domain shift</strong>, i.e. the difference between source and target languages, is often quite severe. The languages might have different vocabularies, syntax or even alphabets. Various cross-lingual resources are often employed to address the gap between languages.</p>
<p>Here is a short overview of different resources that might be used.</p>
<h4 id=multilingual-distributional-representations>Multilingual distributional representations<a hidden class=anchor aria-hidden=true href=#multilingual-distributional-representations>#</a></h4>
<img src=https://img-blog.csdnimg.cn/20201228140913205.png#pic_center width=750>
<p>With multilingual word embeddings (MWE), words from multiple languages share one semantic vector space. In this space semantically similar words are close together independently on the language they come from.</p>
<p>During the training MWE usually require additional cross-lingual resources, e.g. bilingual dictionaries or parallel corpora. Multilingual sentence embeddings work on similar principle, but they use sentences instead of words. Ideally, corresponding sentences should have similar representations.</p>
<h5 id=evaluation-of-multilingual-distributional-representations>Evaluation of multilingual distributional representations<a hidden class=anchor aria-hidden=true href=#evaluation-of-multilingual-distributional-representations>#</a></h5>
<p>Evaluation of multilingual distributional representations can be done either intrinsically or extrinsically.</p>
<ul>
<li>
<p>With <strong>intrinsic evaluation</strong> authors usually measure how well does semantic similarity reflect in the vector space, i.e. how far apart are semantically similar words or sentences.</p>
</li>
<li>
<p>On the other hand, <strong>extrinsic evaluation</strong> measure how good are the representations for downstream tasks, i.e. they are evaluated on how well they perform for CLL.</p>
</li>
</ul>
<h4 id=parallel-corpus>Parallel corpus<a hidden class=anchor aria-hidden=true href=#parallel-corpus>#</a></h4>
<p><strong>Parallel corpora are one of the most basic linguistic resources.</strong> In most cases, sentence-aligned parallel corpus of two languages is used. Wikipedia is sometimes used as a comparable parallel corpus, although due to its complex structure it can also be used as a multilingual knowledge base.</p>
<p><strong>Parallel corpora are most often created for specific domains</strong>, such as politics, religion or movie subtitles. <em>Parallel corpora are also used as training sets for machine translation systems and for creating multilingual distributional representations, which makes parallel corpora even more important.</em></p>
<h4 id=word-alignments>Word Alignments<a hidden class=anchor aria-hidden=true href=#word-alignments>#</a></h4>
<p>In some cases, sentence alignment in parallel corpora might not be enough.</p>
<p><strong>For word alignments, one word from a sentence in language $\ell_A$ can be aligned with any number of words from corresponding sentence in language $\ell_B$.</strong> In most cases automatic tools are used to perform word alignment over existing parallel sentences. Machine Translation systems can also often provide word alignment information for their generated sentences.</p>
<h4 id=machine-translation>Machine Translation<a hidden class=anchor aria-hidden=true href=#machine-translation>#</a></h4>
<p><strong>Machine translation (MT) can be used instead of parallel corpora to generate parallel sentences</strong>. <em>Parallel corpus generated by MT is called pseudo parallel corpus.</em> Although in recent years MT achieved great improvements by using neural encoder-decoder models, machine translation is still far away from providing perfect translations. MT models are usually trained from parallel corpora.</p>
<p>By using samples generated by MT systems we inevitably inject noise into our models; The domain shift between a language $\ell_A$ and what MT systems generate as language $\ell_A$ needs to be addressed.</p>
<h4 id=universal-features-out-of-fashion>Universal features (out of fashion)<a hidden class=anchor aria-hidden=true href=#universal-features-out-of-fashion>#</a></h4>
<p><strong>Universal features</strong> are inherently language independent to some extent, e.g. emojis or punctuation. These can be used as features for any language. As such, model trained with such universal features should be easily applied to other languages.</p>
<p>The process of creating language independent features for words is called delexicalization. Delexicalized text has words replaced with universal features, such as POS tags. We lose the lexical information of the words in this process, thus the name delexicalization.</p>
<h4 id=bilingual-dictionary>Bilingual dictionary<a hidden class=anchor aria-hidden=true href=#bilingual-dictionary>#</a></h4>
<p>Bilingual dictionaries are the most available cross-lingual resource in our list. They exist for many language pairs and provide a very easy and natural way of connecting words from different languages. However, they are often incomplete and context insensitive.</p>
<h4 id=pre-trained-multilingual-language-models>Pre-trained multilingual language models<a hidden class=anchor aria-hidden=true href=#pre-trained-multilingual-language-models>#</a></h4>
<p>Pre-trained language models are a state-of-the-art NLP technique. <em>A large amount of text data is used to train a high capacity language model. Then we can use the parameters from this language model to initialize further training with different NLP tasks. The parameters are fine-tuned with the additional target task data.</em> This is a form of transfer learning, where we use language modeling as a source task. The most well known pre-trained language models are BERT.</p>
<p>Multilingual language models (MLMs) are an extension of this concept. A single language model is trained with multiple languages at the same time.</p>
<ul>
<li>
<p>This can be done without any cross-lingual supervision, i.e. we feed the model with text from multiple languages and we do not provide the model with any additional information about the relations between the languages. Such is the case of multilingual BERT model (mBERT). Interestingly enough, even with no information about how are the languages related, the representations this model creates are partially language independent. The model is able to understand the connections between languages even without being explicitly told to do so. To know more about mBERT, check my previous post about <a href=https://liyantang.blog.csdn.net/article/details/107873888>common MLMs.
</a></p>
</li>
<li>
<p>The other case are models that directly work with some sort of cross-lingual supervision, i.e. they use data that help them establish a connection between different languages. Such is the case of XLM, which make use of parallel corpora and machine translation to directly teach the model about corresponding sentences. To know more about XLM, check my previous post about <a href=https://liyantang.blog.csdn.net/article/details/107873888>common MLMs.
</a></p>
</li>
</ul>
<h3 id=transfer-learning-techniques-for-cross-lingual-learning>Transfer learning techniques for Cross-lingual Learning<a hidden class=anchor aria-hidden=true href=#transfer-learning-techniques-for-cross-lingual-learning>#</a></h3>
<p><img loading=lazy src="https://img-blog.csdnimg.cn/20201228003731142.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_1,color_FFFFFF,t_70#pic_center" alt=åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°>
Four main categories for CLL:</p>
<ul>
<li><strong>Label transfer</strong>: Labels or annotations are transferred between corresponding $L_S$ and $L_T$ samples.</li>
<li><strong>Feature transfer</strong>: Similar to label transfer, but sample features are transferred instead (transfer knowledge about the features of the sample).</li>
<li><strong>parameter transfer</strong>: Parameter values are transferred between parametric models. This effectively transfers the behaviour of the model.</li>
<li><strong>Representation transfer</strong>: The expected values for hidden representation are transferred between models. The target model is taught to create desired representations.</li>
</ul>
<p>Note: <em>Representation transfer</em> is similar to <em>feature transfer</em>. However, instead of simply transferring the features, it teaches the $L_T$model to create these features instead.</p>
<h4 id=label-transfer>Label transfer<a hidden class=anchor aria-hidden=true href=#label-transfer>#</a></h4>
<p>Transferring labels means transferring these labels between the samples from different languages. First, a correspondence is established between $L_S$ and $L_T$ samples. Correspondence means that the pair of samples should have the same label. Then the labels are transferred from one language to the other â€“ this step is also called <strong>annotation projection</strong>. The projected labels can than be used for further training.</p>
<p>There are three types of distinct label transfer and they differ in the language the resulting model takes as an input.</p>
<h5 id=transferring-labels-during-training-to-train-an-l_t-model>Transferring labels during training (To train an $L_T$ model)<a hidden class=anchor aria-hidden=true href=#transferring-labels-during-training-to-train-an-l_t-model>#</a></h5>
<p>To create an $L_T$ model with label transfer we need an existing annotated dataset or model in $L_S$. Then we establish a correspondence between $L_S$ annotated samples and $L_T$ unannotated samples. The annotations are projected to $L_T$ and resulting distantly supervised $L_T$ dataset is used to train an $L_T$ model.</p>
<p>When machine translation is used to generate the corresponding samples we can</p>
<ul>
<li>either take existing $L_S$ samples and translate them into $L_T$ along with their annotations.</li>
<li>Or, we can take unannotated $L_T$ data, translate them into $L_S$ and then annotate these translated $L_S$ samples, and finally, the annotations are projected back to the original samples. The former is the more frequent alternative.</li>
</ul>
<h5 id=transferring-labels-during-inference-use-existing-l_s-for-inference>Transferring labels during inference (Use existing $L_S$ for inference)<a hidden class=anchor aria-hidden=true href=#transferring-labels-during-inference-use-existing-l_s-for-inference>#</a></h5>
<p>An unannotated $L_T$ sample has a corresponding $L_S$ sample generated (e.g. by MT) and then a pre-existing $L_S$ model is used to infer its label. This label is then projected back to the original $L_T$ sample. This approach is quite slow and prone to errors.</p>
<h5 id=parallel-model>Parallel Model<a hidden class=anchor aria-hidden=true href=#parallel-model>#</a></h5>
<p>This is the least frequent type of label transfer, so I skip it here.</p>
<h5 id=commons-of-label-transfer-techniques>Commons of label transfer techniques<a hidden class=anchor aria-hidden=true href=#commons-of-label-transfer-techniques>#</a></h5>
<ol>
<li>All label-based transfer techniques require a process to generate corresponding samples. Two main approaches are using <em>machine translation</em> and <em>parallel corpora</em>.</li>
</ol>
<ul>
<li><strong>Machine translation</strong>. The biggest disadvantage is that MT systems are still not perfect and only generate very specific dialects of the output languages. This shift between the natural lan- guage and the generated language is a source of noise in CLL.</li>
<li><strong>parallel corpus.</strong> Parallel corpora can be used to avoid the problem with noisy translation. <strong>In parallel corpus both sides are written in natural language.</strong> We can then use existing model to annotate the LS side of the corpus and then project the labels to the other half. However, the annotations from the existing model is the source of noise as well. Usually NLP models have limited accuracy and some samples will be mislabeled. Manually labeled parallel corpora exist, but they are very rare.</li>
</ul>
<ol start=2>
<li>
<p>Cross-lingual projection of the labels is the step when the transfer of knowledge between languages happens for label-based approaches.</p>
</li>
<li>
<p>$L_S$ model can be trained with additional data translated from $L_T$. This can improve the results during inference, <strong>since the model was already exposed to the translated data during training and does not suffer from the domain shift that much</strong>.</p>
</li>
<li>
<p><strong>Label based transfer is notoriously noisy. It consists of several steps and each step can be a source of noise</strong>, e.g. imperfect machine translations, imperfect word alignments, imperfect pre-existing models, domain shift between parallel corpora and testing data, etc.</p>
</li>
</ol>
<h4 id=feature-transfer>Feature Transfer<a hidden class=anchor aria-hidden=true href=#feature-transfer>#</a></h4>
<p>This is not a frequent type of transfer, so I skip it.</p>
<h4 id=parameter-transfer>Parameter transfer<a hidden class=anchor aria-hidden=true href=#parameter-transfer>#</a></h4>
<p>In parameter transfer, the behavior of the model is transfered. The transfer of knowledge happens only on shared layers. The most important technologies for parameter transfer are different language independent representations and pre-trained multilingual language models. There are three scenarios for parameter transfer.</p>
<h5 id=zero-shot-transfer>Zero-shot transfer<a hidden class=anchor aria-hidden=true href=#zero-shot-transfer>#</a></h5>
<p>No $L_T$ data are used during the training. We train the model with $L_S$ only and then apply it to other languages. This is sometimes called <strong>direct transfer</strong> or <strong>model transfer</strong>. It is most often used together with <em>language independent representations</em> (e.g. MWE or MLMs. Contextualized word embedding from BERT is an example of MWE).</p>
<h5 id=joint-learning>Joint learning<a hidden class=anchor aria-hidden=true href=#joint-learning>#</a></h5>
<p>Both $L_S$ and $L_T$ data are used during the training, and they are used at the same time. <em>The $L_T$ and $L_S$ models share a subset of their parameters, i.e. when one model updates its parameters</em>, it affects the other model(s) as well. This technique of working with parameters is called <strong>parameter sharing</strong>.</p>
<p>There are two strategies of training:</p>
<ul>
<li>
<p><strong>Mixed dataset</strong>, which can be applied only when all the parameters are shared. During this strategy the training samples for all the languages are mixed into one training set. Then during the training, one batch can contain samples from multiple languages.</p>
</li>
<li>
<p><strong>Alternate training</strong>, which can be applied even when only a subset of parameters is shared. During this strategy, the batch is sampled from one language only. Then this batch is run through the language-specific model and the parameter update is propagated to other models, that share some of the parameters.</p>
</li>
</ul>
<p>Parameter sharing strategies (for each layer):</p>
<ul>
<li><strong>Shared</strong>. The parameters are shared between multiple languages.</li>
<li><strong>Partially-private</strong>. Part of the layer isshared, while the other part is private. The most common way to implement this strategy is to have two distinct parallel layers, one with private strategy and the other with shared strategy. Then, the output of these two layers is concatenated.</li>
<li><strong>Private</strong>. The parameters are specific for one language.</li>
</ul>
<p>The sharing of parameters can be realized in two ways:</p>
<ul>
<li><strong>Hard sharing</strong>. With hard sharing, the shared layers are exactly the same.</li>
<li><strong>Soft sharing</strong>. Parameters can also be bound by a regularization term instead. E.g., add an additional term $\left|\theta_{S}-\theta_{T}\right|<em>{2}^{2},$ where $\theta</em>{S}$ and $\theta_{T}$ are the shared parameters for source model and target model, respectively.</li>
</ul>
<p>Three different parameter sharing strategies:
<img loading=lazy src="https://img-blog.csdnimg.cn/20201228040340898.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_1,color_FFFFFF,t_70#pic_center" alt=åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°>
a) Mixed batch with samples from both languages ($X_{ST}$) is processed by the model. All theparameters are shared betweenLSandLTLoss functionJSTis used for these batches.</p>
<p>b) Alternate training is used with all parameters shared, each batch is created in either $L_S$ or $L_T$. Loss function $J_{ST}$ is still the same for both cases.</p>
<p>c) Alternate training with only a subset of parameters shared. In this example, the second layer is language-specific(private), while the other layers are shared. Loss function is calculated differently for each language.</p>
<h5 id=cascade-learning>Cascade learning<a hidden class=anchor aria-hidden=true href=#cascade-learning>#</a></h5>
<p>Both $L_S$ and $L_T$ data are used during the training, but not at the same time. Instead we pre-train a model with $L_S$ data (or simply take an existing $L_S$ model) and then fine-tune it with $L_T$ data.</p>
<h5 id=pre-trained-multilingual-language-models-1>Pre-trained multilingual language models<a hidden class=anchor aria-hidden=true href=#pre-trained-multilingual-language-models-1>#</a></h5>
<p>Pre-trained multilingual language models can be used for parameter based transfer as well.</p>
<ol>
<li>We can use them as a source of multilingual distributed representations and then build a model on these representations.</li>
<li>We can use the MLM as an initialization of a multilingual model. Then we fine-tune the parameters to perform a target task.</li>
</ol>
<p>Multilingual language models were able to achieve state-of-the-art results recently and they might become the predominant cross-lingual learning paradigm in the near future.</p>
<h4 id=representation-transfer>Representation transfer<a hidden class=anchor aria-hidden=true href=#representation-transfer>#</a></h4>
<p>With representation transfer, <em>the knowledge about how hidden representations should look like within a model</em> is transferred. It is a technique to extend other approaches. It is often used with deep models that use hidden representations during the calculation. This technique is often using corresponding samples or words from different languages, i.e. we usually need a parallel corpus or a bilingual dictionary.</p>
<h3 id=future-directions>Future Directions<a hidden class=anchor aria-hidden=true href=#future-directions>#</a></h3>
<ul>
<li>
<p><strong>Multilingual datasets</strong>. We consider the lack of multilingual datasets to be currently the biggest challenge for CLL. We believe that it is important to provide standardized variants of the datasets in the future for better reproducibility in these additional settings as well.</p>
</li>
<li>
<p><strong>Standardization of linguistic resources</strong>. It is hard to compare between various resources when the corpora they are trained on might differ. It is then hard to distinguish, whether an eventual performance improvement comes from a better method or from a better corpus used for training.</p>
</li>
<li>
<p><strong>Pre-trained multilingual language models</strong>. Training, fine-tuning and inference are all costly for MLMs. We expect to see methods that optimize the training and inference of these behemoths. As of today, the simple $L_S$ fine-tuning might lead to catastrophic forgetting, including forgetting related to the cross-lingual knowledge.</p>
</li>
<li>
<p><strong>Truly low-resource languages and excluded languages</strong>. The CLL methods often rely on an existence of various linguistic resources, such as parallel corpora or MT systems. However,truly low-resource languages might not have these resources in this quantity and/or quality.The fact that methods are currently evaluated on resource-rich languages might then create unrealistic expectations about how well would the methods work on truly low-resource languages.</p>
</li>
<li>
<p><strong>Curse of Multilinguality</strong>. Researchers report that adding more languages help initially, but after certain number the performance of the models actually starts to degrade. This is the <em>curse of multilinguality</em>, the apparent inability of models to learn too many languages. This curse is caused by a limited capacity of current parametric models. It can be addressed by increasing the capacity of the models, but the models are costly to train even today and increasing their size even further has only diminishing returns.</p>
</li>
<li>
<p><strong>Combination with multitask learning</strong>. We believe,that a combination of cross-lingual and cross-task supervision might lead to more universal models, that are able to solve multiple tasks in multiple languages and then also generalize their broad knowledge into new tasks and new languages more easily.</p>
</li>
<li>
<p><strong>Machine translation</strong>. One open question is how to mitigate the domain shift between natural languages and languages generated by MT systems.</p>
</li>
</ul>
<hr>
<p>Reference</p>
<ul>
<li>Cross-lingual learning for text processing: A survey. <a href=https://www.sciencedirect.com/science/article/pii/S0957417420305893?via%3Dihub>https://www.sciencedirect.com/science/article/pii/S0957417420305893?via%3Dihub</a></li>
</ul>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=https://tangliyan.com/blog/tags/nlp/>NLP</a></li>
<li><a href=https://tangliyan.com/blog/tags/multi-lingual/>MULTI-LINGUAL</a></li>
</ul>
<nav class=paginav>
<a class=prev href=https://tangliyan.com/blog/posts/connecting_the_dots/>
<span class=title>Â« Prev Page</span>
<br>
<span>Paper Review - Connecting the Dots: Document-level Neural Relation Extraction with Edge-oriented Graphs</span>
</a>
<a class=next href=https://tangliyan.com/blog/posts/bert_roberta/>
<span class=title>Next Page Â»</span>
<br>
<span>BERT and RoBERTa </span>
</a>
</nav>
<div class=share-buttons>
<a target=_blank rel="noopener noreferrer" aria-label="share Cross-Lingual Learning on twitter" href="https://twitter.com/intent/tweet/?text=Cross-Lingual%20Learning&url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fcorss_lingual%2f&hashtags=NLP%2cMULTI-LINGUAL"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Cross-Lingual Learning on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fcorss_lingual%2f&title=Cross-Lingual%20Learning&summary=Cross-Lingual%20Learning&source=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fcorss_lingual%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Cross-Lingual Learning on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fcorss_lingual%2f&title=Cross-Lingual%20Learning"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Cross-Lingual Learning on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fcorss_lingual%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Cross-Lingual Learning on whatsapp" href="https://api.whatsapp.com/send?text=Cross-Lingual%20Learning%20-%20https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fcorss_lingual%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Cross-Lingual Learning on telegram" href="https://telegram.me/share/url?text=Cross-Lingual%20Learning&url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fcorss_lingual%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a>
</div>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2022 <a href=https://tangliyan.com/blog/>Liyan Tang</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
<script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerText='copy';function d(){a.innerText='copied!',setTimeout(()=>{a.innerText='copy'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script>
</body>
</html>