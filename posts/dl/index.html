<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Intro to Deep Learning and Backpropagation | Liyan Tang</title>
<meta name=keywords content="ML,MATH">
<meta name=description content="Deep Learning v.s. Machine Learning The major difference between Deep Learning and Machine Learning technique is the problem solving approach. Deep Learning techniques tend to solve the problem end to end, where as Machine learning techniques need the problem statements to break down to different parts to be solved first and then their results to be combine at final stage.
Forward Propagation The general procedure is the following:
$$ \begin{aligned} a^{(1)}(x) &= w^{(1)^T} \cdot x + b^{(1)} \\ h^{(1)}(x) &= g_1(a^{(1)}(x)) \\ a^{(2)}(x) &= w^{(2)^T} \cdot h^{(1)}(x) + b^{(2)} \\ h^{(2)}(x) &= g_2(a^{(2)}(x)) \\ &&mldr;&mldr; \\ a^{(L+1)}(x) &= w^{(L+1)^T} \cdot h^{(L)}(x) + b^{(L+1)} \\ h^{(L+1)}(x) &= g_{L+1}(a^{(L+1)}(x)) \end{aligned} $$">
<meta name=author content>
<link rel=canonical href=https://tangliyan.com/blog/posts/dl/>
<link crossorigin=anonymous href=/blog/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css integrity="sha256-yIlj/i15RiAA/Q+xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/blog/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://tangliyan.com/blog/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://tangliyan.com/blog/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://tangliyan.com/blog/favicon-32x32.png>
<link rel=apple-touch-icon href=https://tangliyan.com/blog/apple-touch-icon.png>
<link rel=mask-icon href=https://tangliyan.com/blog/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.92.0">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-202974782-1','auto'),ga('send','pageview'))</script><meta property="og:title" content="Intro to Deep Learning and Backpropagation">
<meta property="og:description" content="Deep Learning v.s. Machine Learning The major difference between Deep Learning and Machine Learning technique is the problem solving approach. Deep Learning techniques tend to solve the problem end to end, where as Machine learning techniques need the problem statements to break down to different parts to be solved first and then their results to be combine at final stage.
Forward Propagation The general procedure is the following:
$$ \begin{aligned} a^{(1)}(x) &= w^{(1)^T} \cdot x + b^{(1)} \\ h^{(1)}(x) &= g_1(a^{(1)}(x)) \\ a^{(2)}(x) &= w^{(2)^T} \cdot h^{(1)}(x) + b^{(2)} \\ h^{(2)}(x) &= g_2(a^{(2)}(x)) \\ &&mldr;&mldr; \\ a^{(L+1)}(x) &= w^{(L+1)^T} \cdot h^{(L)}(x) + b^{(L+1)} \\ h^{(L+1)}(x) &= g_{L+1}(a^{(L+1)}(x)) \end{aligned} $$">
<meta property="og:type" content="article">
<meta property="og:url" content="https://tangliyan.com/blog/posts/dl/"><meta property="og:image" content="https://tangliyan.com/blog/papermod-cover.png"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2020-05-26T00:00:00+00:00">
<meta property="article:modified_time" content="2020-05-26T00:00:00+00:00">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://tangliyan.com/blog/papermod-cover.png">
<meta name=twitter:title content="Intro to Deep Learning and Backpropagation">
<meta name=twitter:description content="Deep Learning v.s. Machine Learning The major difference between Deep Learning and Machine Learning technique is the problem solving approach. Deep Learning techniques tend to solve the problem end to end, where as Machine learning techniques need the problem statements to break down to different parts to be solved first and then their results to be combine at final stage.
Forward Propagation The general procedure is the following:
$$ \begin{aligned} a^{(1)}(x) &= w^{(1)^T} \cdot x + b^{(1)} \\ h^{(1)}(x) &= g_1(a^{(1)}(x)) \\ a^{(2)}(x) &= w^{(2)^T} \cdot h^{(1)}(x) + b^{(2)} \\ h^{(2)}(x) &= g_2(a^{(2)}(x)) \\ &&mldr;&mldr; \\ a^{(L+1)}(x) &= w^{(L+1)^T} \cdot h^{(L)}(x) + b^{(L+1)} \\ h^{(L+1)}(x) &= g_{L+1}(a^{(L+1)}(x)) \end{aligned} $$">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://tangliyan.com/blog/posts/"},{"@type":"ListItem","position":2,"name":"Intro to Deep Learning and Backpropagation","item":"https://tangliyan.com/blog/posts/dl/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Intro to Deep Learning and Backpropagation","name":"Intro to Deep Learning and Backpropagation","description":"Deep Learning v.s. Machine Learning The major difference between Deep Learning and Machine Learning technique is the problem solving approach. Deep Learning techniques tend to solve the problem end to end, where as Machine learning techniques need the problem statements to break down to different parts to be solved first and then their results to be combine at final stage.\nForward Propagation The general procedure is the following:\n$$ \\begin{aligned} a^{(1)}(x) \u0026amp;= w^{(1)^T} \\cdot x + b^{(1)} \\\\ h^{(1)}(x) \u0026amp;= g_1(a^{(1)}(x)) \\\\ a^{(2)}(x) \u0026amp;= w^{(2)^T} \\cdot h^{(1)}(x) + b^{(2)} \\\\ h^{(2)}(x) \u0026amp;= g_2(a^{(2)}(x)) \\\\ \u0026amp;\u0026hellip;\u0026hellip; \\\\ a^{(L+1)}(x) \u0026amp;= w^{(L+1)^T} \\cdot h^{(L)}(x) + b^{(L+1)} \\\\ h^{(L+1)}(x) \u0026amp;= g_{L+1}(a^{(L+1)}(x)) \\end{aligned} $$","keywords":["ML","MATH"],"articleBody":"Deep Learning v.s. Machine Learning The major difference between Deep Learning and Machine Learning technique is the problem solving approach. Deep Learning techniques tend to solve the problem end to end, where as Machine learning techniques need the problem statements to break down to different parts to be solved first and then their results to be combine at final stage.\nForward Propagation The general procedure is the following:\n$$ \\begin{aligned} a^{(1)}(x) \u0026= w^{(1)^T} \\cdot x + b^{(1)} \\\\ h^{(1)}(x) \u0026= g_1(a^{(1)}(x)) \\\\ a^{(2)}(x) \u0026= w^{(2)^T} \\cdot h^{(1)}(x) + b^{(2)} \\\\ h^{(2)}(x) \u0026= g_2(a^{(2)}(x)) \\\\ \u0026…… \\\\ a^{(L+1)}(x) \u0026= w^{(L+1)^T} \\cdot h^{(L)}(x) + b^{(L+1)} \\\\ h^{(L+1)}(x) \u0026= g_{L+1}(a^{(L+1)}(x)) \\end{aligned} $$\nNote:\n  $w^{(i)}$ has dimension: (# of (hidden) units in layer $i$) $\\times$ (# of (hidden) units in layer $i-1$).\n  $b^{(i)}, a^{(i)}, h^{(i)}$ have the same dimension: (# of (hidden) units in layer $i$, $1$).\n  $g_{i}$ is an activation function. Sigmoid, tanh, relu are common activation functions. In the last layer, the choose of activation function $g_{(L+1)}$ depends on problems, usually sigmoid, and softmax.\n  Loss functions of neural network Common loss functions are cross-entropy loss, hinge loss, triple loss, etc. In fact, depending on specifice problems, we can define arbitrarily loss functions. We can also use AUC as a loss.\nIn this post, we focus on the cross-entropy loss.\nFor example, let the output of the neural network be $(0.6, 0.2, 0.1, 0.1)$, and the true label $(0, 1, 0, 0)$, then we can write the cross-entropy loss as\n$$ \\ell \\left(\\left[\\begin{array}{l} 0.6 \\\\ 0.2 \\\\ 0.1 \\\\ 0.1 \\end{array}\\right],\\left[\\begin{array}{l} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{array}\\right]\\right) = -(0 \\cdot \\log 0.6 + 1 \\cdot \\log 0.2 + 0 \\cdot \\log 0.1 + 0 \\cdot \\log 0.1) = -\\log 0.2 $$\nFrom now on, we call the output of the neural network $f(x)$, and the true label w.r.t $x$ is y, then the corss-entropy loss is written by\n$$\\ell \\left( f(x), y\\right) = - \\log f(x)_y$$\nwhere $f(x)_y$ is the $y$-th entry of $f(x)$.\nBack-propagation In fitting a neural network, backpropagation computes the gradient of the loss function with respect to the weights of the network. The backpropagation algorithm works by computing the gradient of the loss function with respect to each weight by the chain rule, computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule.\nIn the following, we would try to compute $\\frac{\\partial \\ell}{\\partial f(x)}, \\frac{\\partial \\ell}{\\partial a^{(L+1)}(x)}, \\frac{\\partial \\ell}{\\partial h^{(k)}(x)}, \\frac{\\partial \\ell}{\\partial a^{(k)}(x)}, \\frac{\\partial \\ell}{\\partial w^{(k)}}, \\frac{\\partial \\ell}{\\partial b^{(k)}}$, and then make a summary of the back-propagation process.\ncompute $\\frac{\\partial \\ell}{\\partial f(x)}$ First consider single element: $$ \\begin{aligned} \\frac{\\partial \\ell}{\\partial f(x)_j} \u0026= \\frac{\\partial -\\log f(x)_y}{\\partial f(x)_j} \\\\ \u0026= \\frac{-1}{f(x)_y} \\cdot \\frac{\\partial f(x)_y}{\\partial f(x)_j} \\\\ \u0026= \\frac{-1\\cdot I(y=j)}{f(x)_y} \\end{aligned} $$\nPut it into vector form:\n$$ \\frac{\\partial \\ell}{\\partial f(x)} = \\frac{\\partial -\\log f(x)_y}{\\partial f(x)} = \\frac{-1}{f(x)_y} \\cdot\\left(\\begin{array}{l} I(y=1) \\\\ I(y=2) \\\\ \\quad … \\\\ I(y=n) \\ \\end{array}\\right) = \\frac{-e(y)}{f(x)_y} \\tag 1 $$\nwhere $e(y)$ is the one-hot encoding of label $y$.\ncompute $\\frac{\\partial \\ell}{\\partial a^{(L+1)}(x)}$ First consider single element:\n$$ \\begin{aligned} \\frac{\\partial \\ell}{\\partial a^{(L+1)}(x)_j} \u0026= \\frac{\\partial -\\log f(x)_y}{\\partial a^{(L+1)}(x)_j} \\\\ \u0026= \\frac{-1}{f(x)_y} \\cdot \\frac{\\partial f(x)_y}{\\partial a^{(L+1)}(x)_j}\\\\ \u0026= \\frac{-1}{f(x)_y} \\cdot \\frac{\\partial}{\\partial a^{(L+1)}(x)_j} \\cdot \\frac{\\exp (a^{(L+1)}(x)_y)}{\\sum_{j'} \\exp (a^{(L+1)}(x)_{j'})} \\\\ \u0026= f(x)_j - I(y=j) \\ \\end{aligned} $$\nPut it into vector form:\n$$\\frac{\\partial \\ell}{\\partial a^{(L+1)}(x)} = \\left(\\begin{array}{l} f(x)_1 - I(y=1) \\\\ f(x)_2 - I(y=2) \\\\ \\quad … \\\\ f(x)_n - I(y=n) \\ \\end{array}\\right) = f(x) - e(y) \\tag 2 $$\nNote that here, we assume the last activation function is softmax.\ncompute $\\frac{\\partial \\ell}{\\partial h^{(k)}(x)}$ $$ \\begin{aligned} \\frac{\\partial \\ell}{\\partial h^{(k)}(x)_j} \u0026= \\sum_i \\frac{\\partial \\ell}{\\partial a^{(k+1)}(x)_i} \\cdot \\frac{\\partial a^{(k+1)}(x)_i}{\\partial h^{(k)}(x)_j} \\\\ \u0026= \\sum_i \\frac{\\partial \\ell}{\\partial a^{(k+1)}(x)_i} \\cdot \\frac{\\partial \\sum_j w_{ij}^{(k+1)}h^{(k)}(x)_j + b^{(k+1)}_i}{\\partial h^{(k)}(x)_j} \\\\ \u0026= \\sum_i \\frac{\\partial \\ell}{\\partial a^{(k+1)}(x)_i} \\cdot w_{ij}^{(k+1)} \\\\ \u0026= w_{ij}^{(k+1)^T} \\cdot \\frac{\\partial -\\log f(x)_y}{\\partial a^{(k+1)}(x)} \\end{aligned} $$\nwhere $a^{(k+1)}(x) = w^{(k+1)}h^{(k)}(x) + b^{(k+1)}$.\nPut it into vector form:\n$$ \\frac{\\partial \\ell}{\\partial h^{(k)}(x)} = w^{(k+1)^T} \\cdot \\frac{\\partial -\\log f(x)_y}{\\partial a^{(k+1)}(x)} \\tag 3$$\ncompute $\\frac{\\partial \\ell}{\\partial a^{(k)}(x)}$ $$ \\begin{aligned} \\frac{\\partial \\ell}{\\partial a^{(k)}(x)_j} \u0026= \\frac{\\partial \\ell}{\\partial h^{(k)}(x)_j} \\cdot \\frac{\\partial h^{(k)}(x)_j}{\\partial a^{(k)}(x)_j} \\\\ \u0026= \\frac{\\partial \\ell}{\\partial h^{(k)}(x)_j} \\cdot g'_k(a^{(k)}(x)_j) \\end{aligned} $$\nwhere $ h^{(k)}(x) = g_k(a^{(k)}(x))$. Now put it into vector form:\n$$ \\begin{aligned} \\frac{\\partial \\ell}{\\partial a^{(k)}(x)} \u0026= \\left( \\frac{\\partial \\ell}{\\partial h^{(k)}(x)_1} \\cdot g'_k(a^{(k)}(x)_1), \\ … \\ ,\\ \\frac{\\partial \\ell}{\\partial h^{(k)}(x)_n} \\cdot g'_k(a^{(k)}(x)_n) \\right) \\\\ \u0026= \\left(\\begin{array}{l} \\frac{\\partial \\ell}{\\partial h^{(k)}(x)_1} \\\\ \\quad … \\\\ \\frac{\\partial \\ell}{\\partial h^{(k)}(x)_n} \\\\ \\end{array}\\right) \\odot \\left(\\begin{array}{l} g'_k(a^{(k)}(x)_1) \\\\ \\quad … \\\\ g'_k(a^{(k)}(x)_n) \\ \\end{array}\\right) \\\\ \u0026= \\frac{\\partial \\ell}{\\partial h^{(k)}(x)} \\odot g'(a^{(k)}(x)) \\end{aligned} \\tag 4 $$\nwhere “$\\odot$” means the element-wise product.\ncompute $\\frac{\\partial \\ell}{\\partial w^{(k)}}$ $$ \\begin{aligned} \\frac{\\partial \\ell}{\\partial w_{ij}^{(k)}} \u0026= \\frac{\\partial \\ell}{\\partial a^{(k)}(x)_i} \\cdot \\frac{\\partial a^{(k)}(x)_i}{\\partial w_{ij}^{(k)}} \\\\ \u0026= \\frac{\\partial \\ell}{\\partial a^{(k)}(x)_i} \\cdot \\frac{\\sum_j w_{ij}^{(k)}h^{(k-1)}(x)_j + b^{(k)}_i}{\\partial w_{ij}^{(k)}} \\\\ \u0026= \\frac{\\partial \\ell}{\\partial a^{(k)}(x)_i} \\cdot h^{(k-1)}(x)_j \\end{aligned} $$\nput it into vector form:\n$$ \\begin{aligned} \\frac{\\partial \\ell}{\\partial w^{(k)}} \u0026= \\frac{\\partial -\\log f(x)_y}{\\partial a^{(k)}(x)} \\cdot( h^{(k-1)}(x))^T \\\\ \u0026= \\left(\\begin{array}{l} \\frac{\\partial -\\log f(x)_y}{\\partial a^{(k)}(x)_1} \\\\ \\quad … \\\\ \\frac{\\partial -\\log f(x)_y}{\\partial a^{(k)}(x)_m} \\ \\end{array}\\right) \\cdot \\left( h^{(k-1)}(x)_1, ,… , , , h^{(k-1)}(x)_n\\right) \\end{aligned} \\tag 5 $$\ncompute $\\frac{\\partial \\ell}{\\partial b^{(k)}}$ $$ \\begin{aligned} \\frac{\\partial \\ell}{\\partial b_{i}^{(k)}} \u0026= \\frac{\\partial \\ell}{\\partial a^{(k)}(x)_i} \\cdot \\frac{\\partial a^{(k)}(x)_i}{\\partial b_i^{(k)}} \\\\ \u0026= \\frac{\\partial \\ell}{\\partial a^{(k)}(x)_i} \\cdot 1 \\\\ \u0026= \\frac{\\partial \\ell}{\\partial a^{(k)}(x)_i} \\end{aligned} $$\nput it into vector form:\n$$ \\frac{\\partial \\ell}{\\partial b^{(k)}} = \\frac{\\partial \\ell}{\\partial a^{(k)}(x)} = \\frac{\\partial -\\log f(x)_y}{\\partial a^{(k)}(x)} \\tag 6 $$\nBack-propagation Precedure (using SGD) Summary For each data (x,y):\n  Forward progagation: Compute $a^{(k)}(x), h^{(k)}(x)$, loss.\n  Back Propagation:\n Compute output gradient: $$ \\nabla_{a^{(L+1)}(x)} -\\log f(x)_y = f(x) - e(y)$$ for $k = L+1$ to $1$:  Compute the gradients of parameters: $$ \\begin{aligned} \\nabla_{w^{(k)}(x)} -\\log f(x)y \u0026= (\\nabla{a^{(k)}(x)} -\\log f(x)y) \\cdot (h^{(k-1)}(x))^T \\\\ \\nabla{b^{(k)}(x)} -\\log f(x)y \u0026= \\nabla{a^{(k)}(x)} -\\log f(x)_y \\end{aligned} $$ Compute the gradients of hidden layers: $$ \\begin{aligned} \\nabla_{h^{(k-1)}(x)} -\\log f(x)y \u0026= (w^{(k)}(x))^T \\cdot (\\nabla{a^{(k)}(x)} -\\log f(x)y) \\\\ \\nabla{a^{(k-1)}(x)} -\\log f(x)y \u0026= (\\nabla{h^{(k-1)}(x)} -\\log f(x)_y) \\odot g'(a^{(k-1)}(x)) \\end{aligned} $$      Debugging: Gradient Checking Since gradient computation can be notoriously difficult to debug and get right, even with a buggy implementation, it may not at all be apparent that anything is amiss. Gradient checking is a method for numerically checking the derivatives computed by your code to make sure that your implementation is correct.\nRecall the mathematical definition of the derivative as:\n$$ \\frac{d}{d\\theta}J(\\theta) = \\lim_{\\epsilon \\rightarrow 0} \\frac{J(\\theta+ \\epsilon) - J(\\theta-\\epsilon)}{2 \\epsilon}$$\nSuppose we have a function $g_i(\\theta)$ that computes $\\textstyle \\frac{\\partial}{\\partial \\theta_i} J(\\theta)$; we’d like to check if $g_i$ is outputting correct derivative values. We would choose a very small $\\epsilon$ and choose a threshold $\\delta$ and check if the following is true:\n$$ | \\frac{J(\\theta+ \\epsilon) - J(\\theta-\\epsilon)}{2 \\epsilon} - g_i(\\theta)| Dropout Dropout refers to ignoring units during the training phase of certain set of neurons which is chosen at random. “ignoring” means these units are not considered during a particular forward or backward pass. More technically, at each training stage, individual nodes are either dropped out of the net with probability $1-p$ or kept with probability $p$, so that a reduced network is left; incoming and outgoing edges to a dropped-out node are also removed.\nDropout is an approach of regularization in neural networks which helps reducing interdependent learning amongst the neurons.\nTraining Phase \u0026 Testing Phase: For each hidden layer, for each training sample, for each iteration, ignore (zero out) a random fraction, $p$, of nodes (and corresponding activations).\nIn testing phase, we won’t use dropout.\nNote: Dropout roughly doubles the number of iterations required to converge. However, training time for each epoch is less.\nEarly Stopping Early stopping is a form of regularization used to avoid overfitting when training a learner with an iterative method, such as gradient descent. Such methods update the learner so as to make it better fit the training data with each iteration. Up to a point, this improves the learner’s performance on data outside of the training set. Past that point, however, improving the learner’s fit to the training data comes at the expense of increased generalization error. Early stopping rules provide guidance as to how many iterations can be run before the learner begins to over-fit.\nSGD with Converge Theoretically. SGD would converge if it satisfies\n$$ \\sum_{t=1}^{+\\infty} \\eta_t = +\\infty \\tag 7$$ $$\\sum_{t=1}^{+\\infty} \\eta_t^2 where $\\eta$ is the learning rate.\n Reference:\n https://en.wikipedia.org/wiki/Backpropagation http://deeplearning.stanford.edu/tutorial/supervised/DebuggingGradientChecking/ https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5 https://en.wikipedia.org/wiki/Early_stopping https://www.researchgate.net/figure/Cross-validation-Early-stopping-Principe-2000_fig4_228469923 https://towardsdatascience.com/why-deep-learning-is-needed-over-traditional-machine-learning-1b6a99177063  ","wordCount":"1398","inLanguage":"en","datePublished":"2020-05-26T00:00:00Z","dateModified":"2020-05-26T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://tangliyan.com/blog/posts/dl/"},"publisher":{"@type":"Organization","name":"Liyan Tang","logo":{"@type":"ImageObject","url":"https://tangliyan.com/blog/favicon.ico"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://tangliyan.com/blog/ accesskey=h title="Liyan Tang (Alt + H)">Liyan Tang</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
<li>
<a href=https://tangliyan.com/blog/archives title=Archive>
<span>Archive</span>
</a>
</li>
<li>
<a href=https://tangliyan.com/blog/tags/ title=Tags>
<span>Tags</span>
</a>
</li>
<li>
<a href=https://tangliyan.com/blog/search/ title="Search (Alt + /)" accesskey=/>
<span>Search</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<div class=breadcrumbs><a href=https://tangliyan.com/blog/>Home</a>&nbsp;»&nbsp;<a href=https://tangliyan.com/blog/posts/>Posts</a></div>
<h1 class=post-title>
Intro to Deep Learning and Backpropagation
</h1>
<div class=post-meta><span title="2020-05-26 00:00:00 +0000 UTC">May 26, 2020</span>&nbsp;·&nbsp;7 min
</div>
</header> <div class=toc>
<details>
<summary accesskey=c title="(Alt + C)">
<span class=details>Table of Contents</span>
</summary>
<div class=inner><ul>
<li>
<a href=#deep-learning-vs-machine-learning aria-label="Deep Learning v.s. Machine Learning">Deep Learning v.s. Machine Learning</a></li>
<li>
<a href=#forward-propagation aria-label="Forward Propagation">Forward Propagation</a></li>
<li>
<a href=#loss-functions-of-neural-network aria-label="Loss functions of neural network">Loss functions of neural network</a></li>
<li>
<a href=#back-propagation aria-label=Back-propagation>Back-propagation</a><ul>
<li>
<a href=#compute-fracpartial-ellpartial-fx aria-label="compute $\frac{\partial \ell}{\partial f(x)}$">compute $\frac{\partial \ell}{\partial f(x)}$</a></li>
<li>
<a href=#compute-fracpartial-ellpartial-al1x aria-label="compute $\frac{\partial \ell}{\partial a^{(L+1)}(x)}$">compute $\frac{\partial \ell}{\partial a^{(L+1)}(x)}$</a></li>
<li>
<a href=#compute--fracpartial-ellpartial-hkx aria-label="compute  $\frac{\partial \ell}{\partial h^{(k)}(x)}$">compute $\frac{\partial \ell}{\partial h^{(k)}(x)}$</a></li>
<li>
<a href=#compute--fracpartial-ellpartial-akx aria-label="compute  $\frac{\partial \ell}{\partial a^{(k)}(x)}$">compute $\frac{\partial \ell}{\partial a^{(k)}(x)}$</a></li>
<li>
<a href=#compute-fracpartial-ellpartial-wk aria-label="compute $\frac{\partial \ell}{\partial w^{(k)}}$">compute $\frac{\partial \ell}{\partial w^{(k)}}$</a></li>
<li>
<a href=#compute-fracpartial-ellpartial-bk aria-label="compute $\frac{\partial \ell}{\partial b^{(k)}}$">compute $\frac{\partial \ell}{\partial b^{(k)}}$</a></li></ul>
</li>
<li>
<a href=#back-propagation-precedure-using-sgd-summary aria-label="Back-propagation Precedure (using SGD) Summary">Back-propagation Precedure (using SGD) Summary</a></li>
<li>
<a href=#debugging-gradient-checking aria-label="Debugging: Gradient Checking">Debugging: Gradient Checking</a></li>
<li>
<a href=#dropout aria-label=Dropout>Dropout</a><ul>
<li>
<a href=#training-phase--testing-phase aria-label="Training Phase &amp;amp; Testing Phase:">Training Phase & Testing Phase:</a></li></ul>
</li>
<li>
<a href=#early-stopping aria-label="Early Stopping">Early Stopping</a></li>
<li>
<a href=#sgd-with-converge aria-label="SGD with Converge">SGD with Converge</a>
</li>
</ul>
</div>
</details>
</div>
<div class=post-content><h2 id=deep-learning-vs-machine-learning>Deep Learning v.s. Machine Learning<a hidden class=anchor aria-hidden=true href=#deep-learning-vs-machine-learning>#</a></h2>
<img src="https://img-blog.csdnimg.cn/20200526061440141.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_16,color_FFFFFF,t_70#pic_center" width=460>
<p>The major difference between <strong>Deep Learning</strong> and <strong>Machine Learning</strong> technique is the problem solving approach. Deep Learning techniques tend to solve the problem end to end, where as Machine learning techniques need the problem statements to break down to different parts to be solved first and then their results to be combine at final stage.</p>
<img src="https://img-blog.csdnimg.cn/20200526062009467.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_16,color_FFFFFF,t_70#pic_center">
<h2 id=forward-propagation>Forward Propagation<a hidden class=anchor aria-hidden=true href=#forward-propagation>#</a></h2>
<p>The general procedure is the following:</p>
<p>$$
\begin{aligned}
a^{(1)}(x) &= w^{(1)^T} \cdot x + b^{(1)} \\ h^{(1)}(x) &= g_1(a^{(1)}(x)) \\ a^{(2)}(x) &= w^{(2)^T} \cdot h^{(1)}(x) + b^{(2)} \\ h^{(2)}(x) &= g_2(a^{(2)}(x)) \\ &&mldr;&mldr; \\ a^{(L+1)}(x) &= w^{(L+1)^T} \cdot h^{(L)}(x) + b^{(L+1)} \\ h^{(L+1)}(x) &= g_{L+1}(a^{(L+1)}(x))
\end{aligned}
$$</p>
<img src="https://img-blog.csdnimg.cn/20200526042156728.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_25,color_FFFFFF,t_70#pic_center" width=700>
<p>Note:</p>
<ul>
<li>
<p>$w^{(i)}$ has dimension: (# of (hidden) units in layer $i$) $\times$ (# of (hidden) units in layer $i-1$).</p>
</li>
<li>
<p>$b^{(i)}, a^{(i)}, h^{(i)}$ have the same dimension: (# of (hidden) units in layer $i$, $1$).</p>
</li>
<li>
<p>$g_{i}$ is an activation function. <strong>Sigmoid</strong>, <strong>tanh</strong>, <strong>relu</strong> are common activation functions. In the last layer, the choose of activation function $g_{(L+1)}$ depends on problems, usually <em>sigmoid</em>, and <em>softmax</em>.</p>
</li>
</ul>
<h2 id=loss-functions-of-neural-network>Loss functions of neural network<a hidden class=anchor aria-hidden=true href=#loss-functions-of-neural-network>#</a></h2>
<p>Common loss functions are <em>cross-entropy loss</em>, <em>hinge loss</em>, <em>triple loss</em>, etc. In fact, depending on specifice problems, we can define arbitrarily loss functions. We can also use <em>AUC</em> as a loss.</p>
<p>In this post, we focus on the <em>cross-entropy loss</em>.</p>
<p>For example, let the output of the neural network be $(0.6, 0.2, 0.1, 0.1)$, and the true label $(0, 1, 0, 0)$, then we can write the cross-entropy loss as</p>
<p>$$ \ell
\left(\left[\begin{array}{l}
0.6 \\ 0.2 \\ 0.1 \\ 0.1
\end{array}\right],\left[\begin{array}{l}
0 \\ 1 \\ 0 \\ 0
\end{array}\right]\right)
= -(0 \cdot \log 0.6 + 1 \cdot \log 0.2 + 0 \cdot \log 0.1 + 0 \cdot \log 0.1) = -\log 0.2
$$</p>
<p>From now on, we call the output of the neural network $f(x)$, and the true label w.r.t $x$ is y, then the corss-entropy loss is written by</p>
<p>$$\ell \left( f(x), y\right) = - \log f(x)_y$$</p>
<p>where $f(x)_y$ is the $y$-th entry of $f(x)$.</p>
<h2 id=back-propagation>Back-propagation<a hidden class=anchor aria-hidden=true href=#back-propagation>#</a></h2>
<p>In fitting a neural network, backpropagation computes the gradient of the loss function with respect to the weights of the network. The backpropagation algorithm works by computing the gradient of the loss function with respect to each weight by the chain rule, computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule.</p>
<p>In the following, we would try to compute $\frac{\partial \ell}{\partial f(x)}, \frac{\partial \ell}{\partial a^{(L+1)}(x)}, \frac{\partial \ell}{\partial h^{(k)}(x)}, \frac{\partial \ell}{\partial a^{(k)}(x)}, \frac{\partial \ell}{\partial w^{(k)}}, \frac{\partial \ell}{\partial b^{(k)}}$, and then make a summary of the back-propagation process.</p>
<h3 id=compute-fracpartial-ellpartial-fx>compute $\frac{\partial \ell}{\partial f(x)}$<a hidden class=anchor aria-hidden=true href=#compute-fracpartial-ellpartial-fx>#</a></h3>
<p>First consider single element:
$$
\begin{aligned}
\frac{\partial \ell}{\partial f(x)_j}
&= \frac{\partial -\log f(x)_y}{\partial f(x)_j} \\ &= \frac{-1}{f(x)_y} \cdot \frac{\partial f(x)_y}{\partial f(x)_j} \\ &= \frac{-1\cdot I(y=j)}{f(x)_y}
\end{aligned}
$$</p>
<p>Put it into vector form:</p>
<p>$$ \frac{\partial \ell}{\partial f(x)} = \frac{\partial -\log f(x)_y}{\partial f(x)} =
\frac{-1}{f(x)_y} \cdot\left(\begin{array}{l}
I(y=1) \\ I(y=2) \\ \quad &mldr; \\ I(y=n) \
\end{array}\right)
= \frac{-e(y)}{f(x)_y} \tag 1
$$</p>
<p>where $e(y)$ is the one-hot encoding of label $y$.</p>
<h3 id=compute-fracpartial-ellpartial-al1x>compute $\frac{\partial \ell}{\partial a^{(L+1)}(x)}$<a hidden class=anchor aria-hidden=true href=#compute-fracpartial-ellpartial-al1x>#</a></h3>
<p>First consider single element:</p>
<p>$$
\begin{aligned}
\frac{\partial \ell}{\partial a^{(L+1)}(x)_j}
&= \frac{\partial -\log f(x)_y}{\partial a^{(L+1)}(x)_j} \\ &= \frac{-1}{f(x)_y} \cdot \frac{\partial f(x)_y}{\partial a^{(L+1)}(x)_j}\\ &= \frac{-1}{f(x)_y} \cdot \frac{\partial}{\partial a^{(L+1)}(x)_j} \cdot \frac{\exp (a^{(L+1)}(x)_y)}{\sum_{j'} \exp (a^{(L+1)}(x)_{j'})} \\ &= f(x)_j - I(y=j) \
\end{aligned}
$$</p>
<p>Put it into vector form:</p>
<p>$$\frac{\partial \ell}{\partial a^{(L+1)}(x)} =
\left(\begin{array}{l}
f(x)_1 - I(y=1) \\ f(x)_2 - I(y=2) \\ \quad &mldr; \\ f(x)_n - I(y=n) \
\end{array}\right) = f(x) - e(y) \tag 2
$$</p>
<p>Note that here, we assume the last activation function is <em>softmax</em>.</p>
<h3 id=compute--fracpartial-ellpartial-hkx>compute $\frac{\partial \ell}{\partial h^{(k)}(x)}$<a hidden class=anchor aria-hidden=true href=#compute--fracpartial-ellpartial-hkx>#</a></h3>
<p>$$
\begin{aligned}
\frac{\partial \ell}{\partial h^{(k)}(x)_j}
&= \sum_i \frac{\partial \ell}{\partial a^{(k+1)}(x)_i} \cdot \frac{\partial a^{(k+1)}(x)_i}{\partial h^{(k)}(x)_j} \\ &= \sum_i \frac{\partial \ell}{\partial a^{(k+1)}(x)_i} \cdot \frac{\partial \sum_j w_{ij}^{(k+1)}h^{(k)}(x)_j + b^{(k+1)}_i}{\partial h^{(k)}(x)_j} \\ &= \sum_i \frac{\partial \ell}{\partial a^{(k+1)}(x)_i} \cdot w_{ij}^{(k+1)} \\ &= w_{ij}^{(k+1)^T} \cdot \frac{\partial -\log f(x)_y}{\partial a^{(k+1)}(x)}
\end{aligned}
$$</p>
<p>where $a^{(k+1)}(x) = w^{(k+1)}h^{(k)}(x) + b^{(k+1)}$.</p>
<p>Put it into vector form:</p>
<p>$$ \frac{\partial \ell}{\partial h^{(k)}(x)} = w^{(k+1)^T} \cdot \frac{\partial -\log f(x)_y}{\partial a^{(k+1)}(x)} \tag 3$$</p>
<h3 id=compute--fracpartial-ellpartial-akx>compute $\frac{\partial \ell}{\partial a^{(k)}(x)}$<a hidden class=anchor aria-hidden=true href=#compute--fracpartial-ellpartial-akx>#</a></h3>
<p>$$
\begin{aligned}
\frac{\partial \ell}{\partial a^{(k)}(x)_j}
&= \frac{\partial \ell}{\partial h^{(k)}(x)_j} \cdot \frac{\partial h^{(k)}(x)_j}{\partial a^{(k)}(x)_j} \\ &= \frac{\partial \ell}{\partial h^{(k)}(x)_j} \cdot g'_k(a^{(k)}(x)_j)
\end{aligned}
$$</p>
<p>where $ h^{(k)}(x) = g_k(a^{(k)}(x))$. Now put it into vector form:</p>
<p>$$
\begin{aligned}
\frac{\partial \ell}{\partial a^{(k)}(x)}
&= \left( \frac{\partial \ell}{\partial h^{(k)}(x)_1} \cdot g'_k(a^{(k)}(x)_1), \ &mldr; \ ,\ \frac{\partial \ell}{\partial h^{(k)}(x)_n} \cdot g'_k(a^{(k)}(x)_n) \right) \\ &=
\left(\begin{array}{l}
\frac{\partial \ell}{\partial h^{(k)}(x)_1} \\ \quad &mldr; \\ \frac{\partial \ell}{\partial h^{(k)}(x)_n} \\ \end{array}\right) \odot
\left(\begin{array}{l}
g'_k(a^{(k)}(x)_1) \\ \quad &mldr; \\ g'_k(a^{(k)}(x)_n) \
\end{array}\right) \\ &= \frac{\partial \ell}{\partial h^{(k)}(x)} \odot g'(a^{(k)}(x))
\end{aligned} \tag 4
$$</p>
<p>where &ldquo;$\odot$&rdquo; means the element-wise product.</p>
<h3 id=compute-fracpartial-ellpartial-wk>compute $\frac{\partial \ell}{\partial w^{(k)}}$<a hidden class=anchor aria-hidden=true href=#compute-fracpartial-ellpartial-wk>#</a></h3>
<p>$$
\begin{aligned}
\frac{\partial \ell}{\partial w_{ij}^{(k)}}
&= \frac{\partial \ell}{\partial a^{(k)}(x)_i} \cdot \frac{\partial a^{(k)}(x)_i}{\partial w_{ij}^{(k)}} \\ &= \frac{\partial \ell}{\partial a^{(k)}(x)_i} \cdot \frac{\sum_j w_{ij}^{(k)}h^{(k-1)}(x)_j + b^{(k)}_i}{\partial w_{ij}^{(k)}} \\ &= \frac{\partial \ell}{\partial a^{(k)}(x)_i} \cdot h^{(k-1)}(x)_j
\end{aligned}
$$</p>
<p>put it into vector form:</p>
<p>$$
\begin{aligned}
\frac{\partial \ell}{\partial w^{(k)}}
&= \frac{\partial -\log f(x)_y}{\partial a^{(k)}(x)} \cdot( h^{(k-1)}(x))^T \\ &=
\left(\begin{array}{l}
\frac{\partial -\log f(x)_y}{\partial a^{(k)}(x)_1} \\ \quad &mldr; \\ \frac{\partial -\log f(x)_y}{\partial a^{(k)}(x)_m} \
\end{array}\right) \cdot
\left( h^{(k-1)}(x)_1, ,&mldr; , , , h^{(k-1)}(x)_n\right)
\end{aligned} \tag 5
$$</p>
<h3 id=compute-fracpartial-ellpartial-bk>compute $\frac{\partial \ell}{\partial b^{(k)}}$<a hidden class=anchor aria-hidden=true href=#compute-fracpartial-ellpartial-bk>#</a></h3>
<p>$$
\begin{aligned}
\frac{\partial \ell}{\partial b_{i}^{(k)}}
&= \frac{\partial \ell}{\partial a^{(k)}(x)_i} \cdot \frac{\partial a^{(k)}(x)_i}{\partial b_i^{(k)}} \\ &= \frac{\partial \ell}{\partial a^{(k)}(x)_i} \cdot 1 \\ &= \frac{\partial \ell}{\partial a^{(k)}(x)_i}
\end{aligned}
$$</p>
<p>put it into vector form:</p>
<p>$$ \frac{\partial \ell}{\partial b^{(k)}} = \frac{\partial \ell}{\partial a^{(k)}(x)} = \frac{\partial -\log f(x)_y}{\partial a^{(k)}(x)} \tag 6 $$</p>
<h2 id=back-propagation-precedure-using-sgd-summary>Back-propagation Precedure (using SGD) Summary<a hidden class=anchor aria-hidden=true href=#back-propagation-precedure-using-sgd-summary>#</a></h2>
<p>For each data (x,y):</p>
<ul>
<li>
<p>Forward progagation: Compute $a^{(k)}(x), h^{(k)}(x)$, loss.</p>
</li>
<li>
<p>Back Propagation:</p>
<ol>
<li>Compute output gradient:
$$ \nabla_{a^{(L+1)}(x)} -\log f(x)_y = f(x) - e(y)$$</li>
<li>for $k = L+1$ to $1$:
<ol>
<li>Compute the gradients of parameters:
$$
\begin{aligned}
\nabla_{w^{(k)}(x)} -\log f(x)<em>y &= (\nabla</em>{a^{(k)}(x)} -\log f(x)<em>y) \cdot (h^{(k-1)}(x))^T \\ \nabla</em>{b^{(k)}(x)} -\log f(x)<em>y &= \nabla</em>{a^{(k)}(x)} -\log f(x)_y
\end{aligned}
$$</li>
<li>Compute the gradients of hidden layers:
$$
\begin{aligned}
\nabla_{h^{(k-1)}(x)} -\log f(x)<em>y &= (w^{(k)}(x))^T \cdot (\nabla</em>{a^{(k)}(x)} -\log f(x)<em>y) \\ \nabla</em>{a^{(k-1)}(x)} -\log f(x)<em>y &= (\nabla</em>{h^{(k-1)}(x)} -\log f(x)_y) \odot g'(a^{(k-1)}(x))
\end{aligned}
$$</li>
</ol>
</li>
</ol>
</li>
</ul>
<h2 id=debugging-gradient-checking>Debugging: Gradient Checking<a hidden class=anchor aria-hidden=true href=#debugging-gradient-checking>#</a></h2>
<p>Since gradient computation can be notoriously difficult to debug and get right, even with a buggy implementation, it may not at all be apparent that anything is amiss. <strong>Gradient checking</strong> is a method for numerically checking the derivatives computed by your code to make sure that your implementation is correct.</p>
<p>Recall the mathematical definition of the derivative as:</p>
<p>$$ \frac{d}{d\theta}J(\theta) = \lim_{\epsilon \rightarrow 0}
\frac{J(\theta+ \epsilon) - J(\theta-\epsilon)}{2 \epsilon}$$</p>
<p>Suppose we have a function $g_i(\theta)$ that computes $\textstyle \frac{\partial}{\partial \theta_i} J(\theta)$; we’d like to check if $g_i$ is outputting correct derivative values. We would choose a very small $\epsilon$ and choose a threshold $\delta$ and check if the following is true:</p>
<p>$$ | \frac{J(\theta+ \epsilon) - J(\theta-\epsilon)}{2 \epsilon} - g_i(\theta)| &lt; \delta$$</p>
<h2 id=dropout>Dropout<a hidden class=anchor aria-hidden=true href=#dropout>#</a></h2>
<img src="https://img-blog.csdnimg.cn/20200526042250296.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_20,color_FFFFFF,t_70#pic_center" width=600>
<p>Dropout refers to ignoring units during the training phase of certain set of neurons which is chosen at random. “ignoring” means these units are not considered during a particular forward or backward pass. More technically, at each training stage, individual nodes are either dropped out of the net with probability $1-p$ or kept with probability $p$, so that a reduced network is left; incoming and outgoing edges to a dropped-out node are also removed.</p>
<p>Dropout is an approach of regularization in neural networks which helps reducing interdependent learning amongst the neurons.</p>
<h3 id=training-phase--testing-phase>Training Phase & Testing Phase:<a hidden class=anchor aria-hidden=true href=#training-phase--testing-phase>#</a></h3>
<p>For each hidden layer, for each training sample, for each iteration, ignore (zero out) a random fraction, $p$, of nodes (and corresponding activations).</p>
<p>In testing phase, we won&rsquo;t use dropout.</p>
<p><em>Note: Dropout roughly doubles the number of iterations required to converge. However, training time for each epoch is less.</em></p>
<h2 id=early-stopping>Early Stopping<a hidden class=anchor aria-hidden=true href=#early-stopping>#</a></h2>
<img src="https://img-blog.csdnimg.cn/20200526042323871.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_16,color_FFFFFF,t_70#pic_center" width=350>
<p>Early stopping is a form of regularization used to avoid overfitting when training a learner with an iterative method, such as gradient descent. Such methods update the learner so as to make it better fit the training data with each iteration. Up to a point, this improves the learner&rsquo;s performance on data outside of the training set. Past that point, however, improving the learner&rsquo;s fit to the training data comes at the expense of increased generalization error. Early stopping rules provide guidance as to how many iterations can be run before the learner begins to over-fit.</p>
<h2 id=sgd-with-converge>SGD with Converge<a hidden class=anchor aria-hidden=true href=#sgd-with-converge>#</a></h2>
<p>Theoretically. SGD would converge if it satisfies</p>
<p>$$ \sum_{t=1}^{+\infty} \eta_t = +\infty \tag 7$$
$$\sum_{t=1}^{+\infty} \eta_t^2 &lt; \infty \tag 8$$</p>
<p>where $\eta$ is the learning rate.</p>
<hr>
<p>Reference:</p>
<ul>
<li><a href=https://en.wikipedia.org/wiki/Backpropagation>https://en.wikipedia.org/wiki/Backpropagation</a></li>
<li><a href=http://deeplearning.stanford.edu/tutorial/supervised/DebuggingGradientChecking/>http://deeplearning.stanford.edu/tutorial/supervised/DebuggingGradientChecking/</a></li>
<li><a href=https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5>https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5</a></li>
<li><a href=https://en.wikipedia.org/wiki/Early_stopping>https://en.wikipedia.org/wiki/Early_stopping</a></li>
<li><a href=https://www.researchgate.net/figure/Cross-validation-Early-stopping-Principe-2000_fig4_228469923>https://www.researchgate.net/figure/Cross-validation-Early-stopping-Principe-2000_fig4_228469923</a></li>
<li><a href=https://towardsdatascience.com/why-deep-learning-is-needed-over-traditional-machine-learning-1b6a99177063>https://towardsdatascience.com/why-deep-learning-is-needed-over-traditional-machine-learning-1b6a99177063</a></li>
</ul>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=https://tangliyan.com/blog/tags/ml/>ML</a></li>
<li><a href=https://tangliyan.com/blog/tags/math/>MATH</a></li>
</ul>
<nav class=paginav>
<a class=prev href=https://tangliyan.com/blog/posts/lstm/>
<span class=title>« Prev Page</span>
<br>
<span>Recurrent Neural Network (RNN) and Long Short Term Memory (LSTM)</span>
</a>
<a class=next href=https://tangliyan.com/blog/posts/crf/>
<span class=title>Next Page »</span>
<br>
<span>Log-Linear Model, Conditional Random Field(CRF)</span>
</a>
</nav>
<div class=share-buttons>
<a target=_blank rel="noopener noreferrer" aria-label="share Intro to Deep Learning and Backpropagation on twitter" href="https://twitter.com/intent/tweet/?text=Intro%20to%20Deep%20Learning%20and%20Backpropagation&url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fdl%2f&hashtags=ML%2cMATH"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Intro to Deep Learning and Backpropagation on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fdl%2f&title=Intro%20to%20Deep%20Learning%20and%20Backpropagation&summary=Intro%20to%20Deep%20Learning%20and%20Backpropagation&source=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fdl%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Intro to Deep Learning and Backpropagation on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fdl%2f&title=Intro%20to%20Deep%20Learning%20and%20Backpropagation"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Intro to Deep Learning and Backpropagation on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fdl%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Intro to Deep Learning and Backpropagation on whatsapp" href="https://api.whatsapp.com/send?text=Intro%20to%20Deep%20Learning%20and%20Backpropagation%20-%20https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fdl%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Intro to Deep Learning and Backpropagation on telegram" href="https://telegram.me/share/url?text=Intro%20to%20Deep%20Learning%20and%20Backpropagation&url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fdl%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a>
</div>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2022 <a href=https://tangliyan.com/blog/>Liyan Tang</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
<script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerText='copy';function d(){a.innerText='copied!',setTimeout(()=>{a.innerText='copy'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script>
</body>
</html>