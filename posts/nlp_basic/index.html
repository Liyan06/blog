<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>NLP Basics, Spell Correction with Noisy Channel | Liyan Tang</title>
<meta name=keywords content="NLP,MATH">
<meta name=description content="NLP = NLU + NLG  NLU: Natural Language Understanding NLG: Natural Language Generation  NLG may be viewed as the opposite of NLU: whereas in NLU, the system needs to disambiguate the input sentence to produce the machine representation language, in NLG the system needs to make decisions about how to put a concept into human understandable words.
Classical applications in NLP   Question Answering
  Sentiment Analysis">
<meta name=author content>
<link rel=canonical href=https://tangliyan.com/blog/posts/nlp_basic/>
<link crossorigin=anonymous href=/blog/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css integrity="sha256-yIlj/i15RiAA/Q+xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/blog/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://tangliyan.com/blog/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://tangliyan.com/blog/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://tangliyan.com/blog/favicon-32x32.png>
<link rel=apple-touch-icon href=https://tangliyan.com/blog/apple-touch-icon.png>
<link rel=mask-icon href=https://tangliyan.com/blog/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.92.0">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-202974782-1','auto'),ga('send','pageview'))</script><meta property="og:title" content="NLP Basics, Spell Correction with Noisy Channel">
<meta property="og:description" content="NLP = NLU + NLG  NLU: Natural Language Understanding NLG: Natural Language Generation  NLG may be viewed as the opposite of NLU: whereas in NLU, the system needs to disambiguate the input sentence to produce the machine representation language, in NLG the system needs to make decisions about how to put a concept into human understandable words.
Classical applications in NLP   Question Answering
  Sentiment Analysis">
<meta property="og:type" content="article">
<meta property="og:url" content="https://tangliyan.com/blog/posts/nlp_basic/"><meta property="og:image" content="https://tangliyan.com/blog/papermod-cover.png"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2020-04-10T00:00:00+00:00">
<meta property="article:modified_time" content="2020-04-10T00:00:00+00:00">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://tangliyan.com/blog/papermod-cover.png">
<meta name=twitter:title content="NLP Basics, Spell Correction with Noisy Channel">
<meta name=twitter:description content="NLP = NLU + NLG  NLU: Natural Language Understanding NLG: Natural Language Generation  NLG may be viewed as the opposite of NLU: whereas in NLU, the system needs to disambiguate the input sentence to produce the machine representation language, in NLG the system needs to make decisions about how to put a concept into human understandable words.
Classical applications in NLP   Question Answering
  Sentiment Analysis">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://tangliyan.com/blog/posts/"},{"@type":"ListItem","position":2,"name":"NLP Basics, Spell Correction with Noisy Channel","item":"https://tangliyan.com/blog/posts/nlp_basic/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"NLP Basics, Spell Correction with Noisy Channel","name":"NLP Basics, Spell Correction with Noisy Channel","description":"NLP = NLU + NLG  NLU: Natural Language Understanding NLG: Natural Language Generation  NLG may be viewed as the opposite of NLU: whereas in NLU, the system needs to disambiguate the input sentence to produce the machine representation language, in NLG the system needs to make decisions about how to put a concept into human understandable words.\nClassical applications in NLP   Question Answering\n  Sentiment Analysis","keywords":["NLP","MATH"],"articleBody":"NLP = NLU + NLG  NLU: Natural Language Understanding NLG: Natural Language Generation  NLG may be viewed as the opposite of NLU: whereas in NLU, the system needs to disambiguate the input sentence to produce the machine representation language, in NLG the system needs to make decisions about how to put a concept into human understandable words.\nClassical applications in NLP   Question Answering\n  Sentiment Analysis\n  Machine Translation\n  Text Summarization: Text summarization refers to the technique of shortening long pieces of text. The intention is to create a coherent and fluent summary having only the main points outlined in the document. It involves both NLU and NLG. It requires the machine to first understand human text and overcome the long distance dependence problems (NLU) and then generate human understandable text (NLG).\n Extraction-based summarization: The extractive text summarization technique involves pulling keyphrases from the source document and combining them to make a summary. The extraction is made according to the defined metric without making any changes to the texts. The grammar might not be right. Abstraction-based summarization: The abstraction technique entails paraphrasing and shortening parts of the source document. The abstractive text summarization algorithms create new phrases and sentences that relay the most useful information from the original text — just like humans do. Therefore, abstraction performs better than extraction. However, the text summarization algorithms required to do abstraction are more difficult to develop; that’s why the use of extraction is still popular.    Information Extraction: Information extraction is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents and other electronically represented sources. QA uses information extraction a lot.\n  Dialogue System\n task-oriented dialogue system.    Text preprocessing Tokenization For Chinese, classical methods are forward max-matching and backward max-matching.\nShortcoming: Do not take semantic meaning into account.\nTokenization based on Language Modeling. Given an input, generate all possible way to split the sentence and then find the one with the highest possibility.\nUnigram model: $$ P(s) = P(w_1)P(w_2)…P(w_k) $$\nBigram model: $$ P(s) = P(w_1)P(w_2 | w_1)P(w_3|w_2) … P(w_k |w_{k-1})$$\nAlternative: Use Viterbi algorithm (Dynamic Programming) to find the optimal way of splitting. Every directed graph path corresponds to a way to split a sentence.\n Spell Correction   Way 1: Go through the vocabulary and then return the words with the smallest edit distance.\n  Way 2: Generate all strings with edit distance 1 or 2, then filter and return (faster than Way 1). We will discuss how to filter at the end of the summary and the filtering method we introduce is called Noisy Channel model.\n    Lowercasing Lowercasing text data is one of the simplest and most effective form of text preprocessing. It can help in cases where your dataset is not very large and thus solve the sparsity issue.\nStop-word removal The intuition behind using stop words is that, by removing low information words from text, we can focus on the important words instead. Think about it as a feature selection process. In general, we won’t do stop-word removal when dealing with machine translation.\nZipf’s law: Zipf’s law is an empirical law refers to the fact that many types of data studied can be approximated with a Zipfian distribution. It states that given some corpus, the frequency of any word is inversely proportional to its rank in the frequency table. Thus the most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word.\nNormalization Text normalization is the process of transforming text into a standard form. For example, the word “gooood” and “gud” can be transformed to “good”. Another example is mapping of near identical words such as “stopwords”, “stop-words” and “stop words” to just “stopwords”. Text normalization is important for noisy texts such as social media comments, text messages and comments to blog posts where abbreviations, misspellings and use of out-of-vocabulary words (oov) are prevalent.\nTwo popular ways to normalizr are Stemming \u0026 Lemmatization.\nThe goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.\nHowever, the two words differ in their flavor. Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.\nTF-IDF TF-IDF works by increasing proportionally to the number of times a word appears in a document, but is offset by the number of documents that contain the word. So, words that are common in every document, such as this, what, and if, rank low even though they may appear many times, since they don’t mean much to that document in particular. However, if a word appears many times in a document, while not appearing many times in others, it probably means that it’s very relevant.\nFormula There are multiple ways to caluculate. Here is one: the TF-IDF score for each word $w$ in the document $d$ from the document set $D$ is calculated as follows:\n$$ tf\\text{-}idf , (w,d,D) = tf , (w,d) \\cdot idf , (w,D)$$\nwhere\n$$ tf , (w,d) = \\text{freq} , (w,d) \\ idf ,(w, D) = \\log \\frac{|D|}{N(w)} $$\nwhere $\\text{freq} , (w,d)$ is the frequency of word $w$ in the document $d$. $|D|$ is the number of document and $N(w)$ is the number of documents having word $w$. Therefore we have a TF-IDF representation for all words in all documents. Note that in every document, the number of word is equal to the number of TF-IDF representation.\nWays to compute similarities between sentences Common ways are using Dot Product, Cosine Similarity, Minkowski distance, and Euclidean distance.\nDot Product: $$d(x,y) = \\langle x, y\\rangle $$\nCosine Similarity: $$ d(x, y) = \\frac{\\langle x, y\\rangle}{|x||y|}$$\nEuclidean distance (squared): $$ d(x, y) = |x-y|^2 = |x|^2 +|y|^2 - 2\\langle x , y\\rangle$$\nMinkowski distance: $$ d(x, y) = (\\sum_{i=1}^n |x_i - y_i|^p ) ^ \\frac{1}{p}$$\nComparision between Cosine Similarity and Euclidean distance:\n  In general, the Cosine Similarity removes the effect of document length. For example, a postcard and a full-length book may be about the same topic, but will likely be quite far apart in pure “term frequency” space using the Euclidean distance. However,tThey will be right on top of each other in cosine similarity.\n  Euclidean distance mainly measures the numeric difference between $x$ and $y$. Cosine Similarity mainly measures the difference of direction between $x$ and $y$.\n  However, if we normalize x and y, the two calculations are equivalent. If we assume $x$ and $y$ are normalized, then Cosine Similarity is $\\langle x , y\\rangle$, and Euclidean distance (squared) is $2(1 - \\langle x , y\\rangle)$. As you can see, minimizing (square) euclidean distance is equivalent to maximizing cosine similarity if the vectors are normalized.\n  Noisy Channel model ( for spell correction) Intuition:\nThe intuition of the noisy channel model is to treat the misspelled word as if a correctly spelled word had been “distorted” by being passed through a noisy communication channel. This channel introduces noise, making it hard to recognize the true word. Our goal is to find the true word by passing every word of the language through our model of the noisy channel and seeing which one comes the closest to the misspelled word.\nProcess:\nWe see an observation $x$ (a misspelled word) and our job is to find the word $w$ that generated this misspelled word. Out of all possible words in the vocabulary $V$ we want to find the word $w$ such that $P(w|x)$ is highest. So our objective function is $$\\mathop{\\rm argmax}\\limits_{w , \\in , V} P(w|x)$$\nWe can re-write our objective function as $$\\mathop{\\rm argmax}\\limits_{w , \\in , V} \\frac{P(x|w)P(w)}{p(x)} $$\nSince $P(x)$ doesn’t change for each choice of word $w$, we can drop it and simplify the objective function as\n$$ \\mathop{\\rm argmax}\\limits_{w , \\in , V} P(x|w)P(w)$$\nThe channel model (or likelihood) of the noisy channel producing any particular observation sequence x is modeled by $P(x|w)$. The prior probability of a hidden word is modeled by $P(w)$. We can compute the most probable word $\\hat w$ given that we’ve seen some observed misspelling $x$ by multiplying the prior $P(w)$ and the likelihood $P(x|w)$ and choosing the word for which this product is greatest.\nWe apply the noisy channel approach to correcting non-word spelling errors by taking any word not in our spelling dictionary, generating a list of candidate words, ranking them according to the objective function defined above and then picking the highest-ranked one. In fact, we can modify the objective function to refer to this list of candidate words instead of the full vocabulary $V$ as follows:\nTo find this list of candidates we’ll use the Minimum Edit Distance algorithm. Note that the types of edits are:\n Insertion Deletion Substitution Transposition of two adjacent letters (perticular in tasks like spell correction)  why we prefer not to compute $p(w|x)$ directly?   Two distributions $p(x|w)$ and $p(w)$ (the language model) can be estimated seperately.\n  If we compute $p(w|x)$ directly, that means we just find a word that maximize the probability $p(w|x)$ but do not put the word in the context (surrounding words). Thus the accuracy of the spell correction is pretty low.\n  On the other hand, it’s worth noting that the surrounding words will make the choice of word clearer. If we maximize $P(x|w)P(w)$, there is a prior term $P(w)$ which we can use bigram, trigram, etc, to compute. Usually, bigram, trigram are better than unigram since they take surrounding words into account.\n  How to compute channel model $P(x|w)$ A perfect model of the probability that a word will be mistyped would condition on all sorts of factors: who the typist was, whether the typist was left-handed or right-handed, and so on. Luckily, we can get a pretty reasonable estimate of $P(x|w)$ just by looking at local context: the identity of the correct letter itself, the misspelling, and the surrounding letters. For example, the letters $m$ and $n$ are often substituted for each other; this is partly a fact about their identity (these two letters are pronounced similarly and they are next to each other on the keyboard) and partly a fact about context (because they are pronounced similarly and they occur in similar contexts). For more detail about how to compute $P(x|w)$, check out https://web.stanford.edu/~jurafsky/slp3/B.pdf.\n Reference:\n https://en.wikipedia.org/wiki/Natural-language_generation https://towardsdatascience.com/a-quick-introduction-to-text-summarization-in-machine-learning-3d27ccf18a9f https://en.wikipedia.org/wiki/Information_extraction https://kavita-ganesan.com/text-preprocessing-tutorial/ https://web.stanford.edu/~jurafsky/slp3/B.pdf https://en.wikipedia.org/wiki/Zipf%27s_law https://stackoverflow.com/questions/1787110/what-is-the-difference-between-lemmatization-vs-stemming https://monkeylearn.com/blog/what-is-tf-idf/ https://www.jianshu.com/p/4f0ee6d023a5 https://www.reddit.com/r/MachineLearning/comments/493exs/why_do_they_use_cosine_distance_over_euclidean/ https://stats.stackexchange.com/questions/72978/vector-space-model-cosine-similarity-vs-euclidean-distance https://datascience.stackexchange.com/questions/6506/shall-i-use-the-euclidean-distance-or-the-cosine-similarity-to-compute-the-seman https://www.quora.com/Natural-Language-Processing-What-is-a-noisy-channel-model  ","wordCount":"1781","inLanguage":"en","datePublished":"2020-04-10T00:00:00Z","dateModified":"2020-04-10T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://tangliyan.com/blog/posts/nlp_basic/"},"publisher":{"@type":"Organization","name":"Liyan Tang","logo":{"@type":"ImageObject","url":"https://tangliyan.com/blog/favicon.ico"}}}</script><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']]},svg:{fontCache:'global'}}</script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://tangliyan.com/blog/ accesskey=h title="Liyan Tang (Alt + H)">Liyan Tang</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
<li>
<a href=https://tangliyan.com/blog/archives title=Archive>
<span>Archive</span>
</a>
</li>
<li>
<a href=https://tangliyan.com/blog/tags/ title=Tags>
<span>Tags</span>
</a>
</li>
<li>
<a href=https://tangliyan.com/blog/search/ title="Search (Alt + /)" accesskey=/>
<span>Search</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<div class=breadcrumbs><a href=https://tangliyan.com/blog/>Home</a>&nbsp;»&nbsp;<a href=https://tangliyan.com/blog/posts/>Posts</a></div>
<h1 class=post-title>
NLP Basics, Spell Correction with Noisy Channel
</h1>
<div class=post-meta><span title="2020-04-10 00:00:00 +0000 UTC">April 10, 2020</span>&nbsp;·&nbsp;9 min
</div>
</header> <div class=toc>
<details>
<summary accesskey=c title="(Alt + C)">
<span class=details>Table of Contents</span>
</summary>
<div class=inner><ul>
<li>
<a href=#nlp--nlu--nlg aria-label="NLP = NLU + NLG">NLP = NLU + NLG</a></li>
<li>
<a href=#classical-applications-in-nlp aria-label="Classical applications in NLP">Classical applications in NLP</a></li>
<li>
<a href=#text-preprocessing aria-label="Text preprocessing">Text preprocessing</a><ul>
<li>
<a href=#tokenization aria-label=Tokenization>Tokenization</a></li>
<li>
<a href=#lowercasing aria-label=Lowercasing>Lowercasing</a></li>
<li>
<a href=#stop-word-removal aria-label="Stop-word removal">Stop-word removal</a></li>
<li>
<a href=#normalization aria-label=Normalization>Normalization</a></li></ul>
</li>
<li>
<a href=#tf-idf aria-label=TF-IDF>TF-IDF</a><ul>
<li>
<a href=#formula aria-label=Formula>Formula</a></li></ul>
</li>
<li>
<a href=#ways-to-compute-similarities-between-sentences aria-label="Ways to compute similarities between sentences">Ways to compute similarities between sentences</a></li>
<li>
<a href=#noisy-channel-model--for-spell-correction aria-label="Noisy Channel model ( for spell correction)">Noisy Channel model ( for spell correction)</a><ul>
<li>
<a href=#why-we-prefer-not-to-compute-pwx-directly aria-label="why we prefer not to compute $p(w|x)$ directly?">why we prefer not to compute $p(w|x)$ directly?</a></li>
<li>
<a href=#how-to-compute-channel-model-pxw aria-label="How to compute channel model $P(x|w)$">How to compute channel model $P(x|w)$</a>
</li>
</ul>
</li>
</ul>
</div>
</details>
</div>
<div class=post-content><h2 id=nlp--nlu--nlg>NLP = NLU + NLG<a hidden class=anchor aria-hidden=true href=#nlp--nlu--nlg>#</a></h2>
<ul>
<li>NLU: Natural Language Understanding</li>
<li>NLG: Natural Language Generation</li>
</ul>
<p>NLG may be viewed as the opposite of NLU: whereas in NLU, the system needs to disambiguate the input sentence to produce the machine representation language, in NLG the system needs to make decisions about how to put a concept into human understandable words.</p>
<h2 id=classical-applications-in-nlp>Classical applications in NLP<a hidden class=anchor aria-hidden=true href=#classical-applications-in-nlp>#</a></h2>
<ul>
<li>
<p><strong>Question Answering</strong></p>
</li>
<li>
<p><strong>Sentiment Analysis</strong></p>
</li>
<li>
<p><strong>Machine Translation</strong></p>
</li>
<li>
<p><strong>Text Summarization</strong>: Text summarization refers to the technique of shortening long pieces of text. The intention is to create a coherent and fluent summary having only the main points outlined in the document. It involves both NLU and NLG. It requires the machine to first understand human text and overcome the long distance dependence problems (NLU) and then generate human understandable text (NLG).</p>
<ul>
<li>Extraction-based summarization: The extractive text summarization technique involves pulling keyphrases from the source document and combining them to make a summary. The extraction is made according to the defined metric without making any changes to the texts. The grammar might not be right.</li>
<li>Abstraction-based summarization: The abstraction technique entails paraphrasing and shortening parts of the source document. The abstractive text summarization algorithms create new phrases and sentences that relay the most useful information from the original text — just like humans do.</li>
<li>Therefore, abstraction performs better than extraction. However, the text summarization algorithms required to do abstraction are more difficult to develop; that’s why the use of extraction is still popular.</li>
</ul>
</li>
<li>
<p><strong>Information Extraction</strong>: Information extraction is the task of automatically <em>extracting structured information</em> from unstructured and/or semi-structured machine-readable documents and other electronically represented sources. QA uses information extraction a lot.</p>
</li>
<li>
<p><strong>Dialogue System</strong></p>
<ul>
<li>task-oriented dialogue system.</li>
</ul>
</li>
</ul>
<h2 id=text-preprocessing>Text preprocessing<a hidden class=anchor aria-hidden=true href=#text-preprocessing>#</a></h2>
<h3 id=tokenization>Tokenization<a hidden class=anchor aria-hidden=true href=#tokenization>#</a></h3>
<p>For Chinese, classical methods are forward max-matching and backward max-matching.</p>
<img src=https://img-blog.csdnimg.cn/2020041012252776.png width=450>
<img src=https://img-blog.csdnimg.cn/2020041012261482.png width=450>
<p>Shortcoming: Do not take semantic meaning into account.</p>
<p>Tokenization based on Language Modeling. Given an input, generate all possible way to split the sentence and then find the one with the highest possibility.</p>
<p><strong>Unigram model</strong>:
$$ P(s) = P(w_1)P(w_2)&mldr;P(w_k) $$</p>
<p><strong>Bigram model</strong>:
$$ P(s) = P(w_1)P(w_2 | w_1)P(w_3|w_2) &mldr; P(w_k |w_{k-1})$$</p>
<p><em>Alternative</em>: Use <strong>Viterbi algorithm</strong> (Dynamic Programming) to find the optimal way of splitting. Every directed graph path corresponds to a way to split a sentence.</p>
<img src=https://img-blog.csdnimg.cn/20200410122720129.png width=700>
<ul>
<li><strong>Spell Correction</strong>
<ul>
<li>
<p>Way 1: Go through the vocabulary and then return the words with the smallest edit distance.</p>
</li>
<li>
<p>Way 2: Generate all strings with edit distance 1 or 2, then filter and return (faster than Way 1). We will discuss how to filter at the end of the summary and the filtering method we introduce is called <strong>Noisy Channel model</strong>.</p>
</li>
</ul>
</li>
</ul>
<h3 id=lowercasing>Lowercasing<a hidden class=anchor aria-hidden=true href=#lowercasing>#</a></h3>
<p>Lowercasing text data is one of the simplest and most effective form of text preprocessing. It can help in cases where your dataset is not very large and thus solve the sparsity issue.</p>
<h3 id=stop-word-removal>Stop-word removal<a hidden class=anchor aria-hidden=true href=#stop-word-removal>#</a></h3>
<p>The intuition behind using stop words is that, by removing low information words from text, we can focus on the important words instead. Think about it as a feature selection process. In general, we won&rsquo;t do stop-word removal when dealing with machine translation.</p>
<p><strong>Zipf&rsquo;s law</strong>: <em>Zipf&rsquo;s law</em> is an empirical law refers to the fact that many types of data studied can be approximated with a Zipfian distribution. It states that given some corpus, the frequency of any word is inversely proportional to its rank in the frequency table. Thus the most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word.</p>
<h3 id=normalization>Normalization<a hidden class=anchor aria-hidden=true href=#normalization>#</a></h3>
<p>Text normalization is the process of transforming text into a standard form. For example, the word “gooood” and “gud” can be transformed to “good”. Another example is mapping of near identical words such as “stopwords”, “stop-words” and “stop words” to just “stopwords”. Text normalization is important for noisy texts such as social media comments, text messages and comments to blog posts where abbreviations, misspellings and use of out-of-vocabulary words (oov) are prevalent.</p>
<p>Two popular ways to normalizr are <strong>Stemming & Lemmatization</strong>.</p>
<p>The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.</p>
<p>However, the two words differ in their flavor. Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.</p>
<h2 id=tf-idf>TF-IDF<a hidden class=anchor aria-hidden=true href=#tf-idf>#</a></h2>
<p>TF-IDF works by increasing proportionally to the number of times a word appears in a document, but is offset by the number of documents that contain the word. So, words that are common in every document, such as this, what, and if, rank low even though they may appear many times, since they don’t mean much to that document in particular. However, if a word appears many times in a document, while not appearing many times in others, it probably means that it’s very relevant.</p>
<h3 id=formula>Formula<a hidden class=anchor aria-hidden=true href=#formula>#</a></h3>
<p>There are multiple ways to caluculate. Here is one: the TF-IDF score for <em>each word</em> $w$ in the document $d$ from the document set $D$ is calculated as follows:</p>
<p>$$ tf\text{-}idf , (w,d,D) = tf , (w,d) \cdot idf , (w,D)$$</p>
<p>where</p>
<p>$$ tf , (w,d) = \text{freq} , (w,d) \ idf ,(w, D) = \log \frac{|D|}{N(w)} $$</p>
<p>where $\text{freq} , (w,d)$ is the frequency of word $w$ in the document $d$. $|D|$ is the number of document and $N(w)$ is the number of documents having word $w$. Therefore we have a TF-IDF representation for all words in all documents. Note that in every document, the number of word is equal to the number of TF-IDF representation.</p>
<h2 id=ways-to-compute-similarities-between-sentences>Ways to compute similarities between sentences<a hidden class=anchor aria-hidden=true href=#ways-to-compute-similarities-between-sentences>#</a></h2>
<p>Common ways are using <em>Dot Product</em>, <em>Cosine Similarity</em>, <em>Minkowski distance</em>, and <em>Euclidean distance</em>.</p>
<p><em>Dot Product</em>: $$d(x,y) = \langle x, y\rangle $$</p>
<p><em>Cosine Similarity</em>: $$ d(x, y) = \frac{\langle x, y\rangle}{|x||y|}$$</p>
<p><em>Euclidean distance</em> (squared): $$ d(x, y) = |x-y|^2 = |x|^2 +|y|^2 - 2\langle x , y\rangle$$</p>
<p><em>Minkowski distance</em>: $$ d(x, y) = (\sum_{i=1}^n |x_i - y_i|^p ) ^ \frac{1}{p}$$</p>
<p><strong>Comparision between <em>Cosine Similarity</em> and <em>Euclidean distance</em>:</strong></p>
<ul>
<li>
<p>In general, the <em>Cosine Similarity</em> removes the effect of document length. For example, a postcard and a full-length book may be about the same topic, but will likely be quite far apart in pure &ldquo;term frequency&rdquo; space using the <em>Euclidean distance</em>. However,tThey will be right on top of each other in cosine similarity.</p>
</li>
<li>
<p><em>Euclidean distance</em> mainly measures the numeric difference between $x$ and $y$. <em>Cosine Similarity</em> mainly measures the difference of direction between $x$ and $y$.</p>
</li>
<li>
<p>However, if we normalize <em>x</em> and <em>y</em>, the two calculations are equivalent. If we assume $x$ and $y$ are normalized, then <em>Cosine Similarity</em> is $\langle x , y\rangle$, and <em>Euclidean distance</em> (squared) is $2(1 - \langle x , y\rangle)$. As you can see, minimizing (square) euclidean distance is equivalent to maximizing cosine similarity if the vectors are normalized.</p>
</li>
</ul>
<h2 id=noisy-channel-model--for-spell-correction>Noisy Channel model ( for spell correction)<a hidden class=anchor aria-hidden=true href=#noisy-channel-model--for-spell-correction>#</a></h2>
<img src=https://img-blog.csdnimg.cn/20200410122848618.png width=700>
<p><em>Intuition:</em></p>
<p>The intuition of the noisy channel model is to treat the misspelled
word as if a correctly spelled word had been &ldquo;distorted&rdquo; by being passed through a
noisy communication channel. This channel introduces noise, making it hard to recognize the true word. Our goal is to find the true word by passing every word of the language through our model of the noisy channel and seeing which one comes the closest to the misspelled word.</p>
<p><em>Process:</em></p>
<p>We see an observation $x$ (a misspelled word) and our job is to find the word $w$ that generated this misspelled word. Out of all possible words in the vocabulary $V$ we want to find the word $w$ such that $P(w|x)$ is highest. So our objective function is $$\mathop{\rm argmax}\limits_{w , \in , V} P(w|x)$$</p>
<p>We can re-write our objective function as $$\mathop{\rm argmax}\limits_{w , \in , V} \frac{P(x|w)P(w)}{p(x)} $$</p>
<p>Since $P(x)$ doesn’t change for each choice of word $w$, we can drop it and simplify the objective function as</p>
<p>$$ \mathop{\rm argmax}\limits_{w , \in , V} P(x|w)P(w)$$</p>
<p>The <strong>channel model</strong> (or <strong>likelihood</strong>) of the noisy channel producing any particular observation sequence x is modeled by $P(x|w)$. The prior probability of a hidden word is modeled by $P(w)$. We can compute the most probable word $\hat w$ given that we’ve seen some observed misspelling $x$ by multiplying the prior $P(w)$ and the likelihood $P(x|w)$ and choosing the word for which this product is greatest.</p>
<p>We apply the noisy channel approach to correcting non-word spelling errors by
taking any word not in our spelling dictionary, generating a list of <em>candidate words</em>,
ranking them according to the objective function defined above and then picking the highest-ranked one. In fact, we can modify the objective function to refer to this list of candidate words instead of the full vocabulary $V$ as follows:</p>
<img src=https://img-blog.csdnimg.cn/20200410122932127.png width=300>
<p>To find this list of candidates we’ll use the <em>Minimum Edit Distance algorithm</em>. Note that the types of edits are:</p>
<ul>
<li>Insertion</li>
<li>Deletion</li>
<li>Substitution</li>
<li><em>Transposition of two adjacent letters</em> (perticular in tasks like spell correction)</li>
</ul>
<h3 id=why-we-prefer-not-to-compute-pwx-directly>why we prefer not to compute $p(w|x)$ directly?<a hidden class=anchor aria-hidden=true href=#why-we-prefer-not-to-compute-pwx-directly>#</a></h3>
<ul>
<li>
<p>Two distributions $p(x|w)$ and $p(w)$ (the language model) can be estimated seperately.</p>
</li>
<li>
<p>If we compute $p(w|x)$ directly, that means we just find a word that maximize the probability $p(w|x)$ but <em>do not put the word in the context (surrounding words)</em>. Thus the accuracy of the spell correction is pretty low.</p>
</li>
<li>
<p>On the other hand, it&rsquo;s worth noting that the surrounding words will make the choice of word clearer. If we maximize $P(x|w)P(w)$, there is a prior term $P(w)$ which we can use bigram, trigram, <em>etc</em>, to compute. Usually, bigram, trigram are better than unigram since they take surrounding words into account.</p>
</li>
</ul>
<h3 id=how-to-compute-channel-model-pxw>How to compute channel model $P(x|w)$<a hidden class=anchor aria-hidden=true href=#how-to-compute-channel-model-pxw>#</a></h3>
<p>A perfect model of the probability that a word will be mistyped would condition on all sorts of factors: who the typist was, whether the typist was left-handed or right-handed, and so on. Luckily, we can get a pretty reasonable estimate of $P(x|w)$ just by looking at local context: the identity of the correct letter itself, the misspelling, and the surrounding letters. For example, the letters $m$ and $n$ are often substituted for each other; this is partly a fact about their identity (these two letters are pronounced similarly and they are next to each other on the keyboard) and partly a fact about context (because they are pronounced similarly and they occur in similar contexts). For more detail about how to compute $P(x|w)$, check out <a href=https://web.stanford.edu/~jurafsky/slp3/B.pdf>https://web.stanford.edu/~jurafsky/slp3/B.pdf</a>.</p>
<hr>
<p>Reference:</p>
<ul>
<li><a href=https://en.wikipedia.org/wiki/Natural-language_generation>https://en.wikipedia.org/wiki/Natural-language_generation</a></li>
<li><a href=https://towardsdatascience.com/a-quick-introduction-to-text-summarization-in-machine-learning-3d27ccf18a9f>https://towardsdatascience.com/a-quick-introduction-to-text-summarization-in-machine-learning-3d27ccf18a9f</a></li>
<li><a href=https://en.wikipedia.org/wiki/Information_extraction>https://en.wikipedia.org/wiki/Information_extraction</a></li>
<li><a href=https://kavita-ganesan.com/text-preprocessing-tutorial/>https://kavita-ganesan.com/text-preprocessing-tutorial/</a></li>
<li><a href=https://web.stanford.edu/~jurafsky/slp3/B.pdf>https://web.stanford.edu/~jurafsky/slp3/B.pdf</a></li>
<li><a href=https://en.wikipedia.org/wiki/Zipf%27s_law>https://en.wikipedia.org/wiki/Zipf%27s_law</a></li>
<li><a href=https://stackoverflow.com/questions/1787110/what-is-the-difference-between-lemmatization-vs-stemming>https://stackoverflow.com/questions/1787110/what-is-the-difference-between-lemmatization-vs-stemming</a></li>
<li><a href=https://monkeylearn.com/blog/what-is-tf-idf/>https://monkeylearn.com/blog/what-is-tf-idf/</a></li>
<li><a href=https://www.jianshu.com/p/4f0ee6d023a5>https://www.jianshu.com/p/4f0ee6d023a5</a></li>
<li><a href=https://www.reddit.com/r/MachineLearning/comments/493exs/why_do_they_use_cosine_distance_over_euclidean/>https://www.reddit.com/r/MachineLearning/comments/493exs/why_do_they_use_cosine_distance_over_euclidean/</a></li>
<li><a href=https://stats.stackexchange.com/questions/72978/vector-space-model-cosine-similarity-vs-euclidean-distance>https://stats.stackexchange.com/questions/72978/vector-space-model-cosine-similarity-vs-euclidean-distance</a></li>
<li><a href=https://datascience.stackexchange.com/questions/6506/shall-i-use-the-euclidean-distance-or-the-cosine-similarity-to-compute-the-seman>https://datascience.stackexchange.com/questions/6506/shall-i-use-the-euclidean-distance-or-the-cosine-similarity-to-compute-the-seman</a></li>
<li><a href=https://www.quora.com/Natural-Language-Processing-What-is-a-noisy-channel-model>https://www.quora.com/Natural-Language-Processing-What-is-a-noisy-channel-model</a></li>
</ul>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=https://tangliyan.com/blog/tags/nlp/>NLP</a></li>
<li><a href=https://tangliyan.com/blog/tags/math/>MATH</a></li>
</ul>
<nav class=paginav>
<a class=prev href=https://tangliyan.com/blog/posts/representation/>
<span class=title>« Prev Page</span>
<br>
<span>Distributed representation, Hyperbolic Space, Gaussian/Graph Embedding</span>
</a>
<a class=next href=https://tangliyan.com/blog/posts/kaggle_google_quest/>
<span class=title>Next Page »</span>
<br>
<span>Kaggle: Google Quest Q&A Labeling - my solution</span>
</a>
</nav>
<div class=share-buttons>
<a target=_blank rel="noopener noreferrer" aria-label="share NLP Basics, Spell Correction with Noisy Channel on twitter" href="https://twitter.com/intent/tweet/?text=NLP%20Basics%2c%20Spell%20Correction%20with%20Noisy%20Channel&url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fnlp_basic%2f&hashtags=NLP%2cMATH"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share NLP Basics, Spell Correction with Noisy Channel on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fnlp_basic%2f&title=NLP%20Basics%2c%20Spell%20Correction%20with%20Noisy%20Channel&summary=NLP%20Basics%2c%20Spell%20Correction%20with%20Noisy%20Channel&source=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fnlp_basic%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share NLP Basics, Spell Correction with Noisy Channel on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fnlp_basic%2f&title=NLP%20Basics%2c%20Spell%20Correction%20with%20Noisy%20Channel"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share NLP Basics, Spell Correction with Noisy Channel on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fnlp_basic%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share NLP Basics, Spell Correction with Noisy Channel on whatsapp" href="https://api.whatsapp.com/send?text=NLP%20Basics%2c%20Spell%20Correction%20with%20Noisy%20Channel%20-%20https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fnlp_basic%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share NLP Basics, Spell Correction with Noisy Channel on telegram" href="https://telegram.me/share/url?text=NLP%20Basics%2c%20Spell%20Correction%20with%20Noisy%20Channel&url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fnlp_basic%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a>
</div>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2022 <a href=https://tangliyan.com/blog/>Liyan Tang</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
<script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerText='copy';function d(){a.innerText='copied!',setTimeout(()=>{a.innerText='copy'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script>
</body>
</html>