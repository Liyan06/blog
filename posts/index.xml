<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Liyan Tang</title>
    <link>https://tangliyan.com/blog/posts/</link>
    <description>Recent content in Posts on Liyan Tang</description>
    <image>
      <url>https://tangliyan.com/blog/papermod-cover.png</url>
      <link>https://tangliyan.com/blog/papermod-cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Mon, 25 Oct 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://tangliyan.com/blog/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Paper Review - Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing</title>
      <link>https://tangliyan.com/blog/posts/pretrain_prompt/</link>
      <pubDate>Mon, 25 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/pretrain_prompt/</guid>
      <description>Authors: Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig</description>
    </item>
    
    <item>
      <title>Paper Review - Knowledge Neurons in Pretrained Transformers</title>
      <link>https://tangliyan.com/blog/posts/knowledge_neurons/</link>
      <pubDate>Sun, 24 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/knowledge_neurons/</guid>
      <description>Authors: Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Furu Wei</description>
    </item>
    
    <item>
      <title>Paper Review - Learning to summarize from human feedback</title>
      <link>https://tangliyan.com/blog/posts/learning_to_summarize/</link>
      <pubDate>Mon, 18 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/learning_to_summarize/</guid>
      <description>Authors: Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, Paul Christiano</description>
    </item>
    
    <item>
      <title>Paper Review - Inspecting the Factuality of Hallucinated Entities in Abstractive Summarization</title>
      <link>https://tangliyan.com/blog/posts/inspecting_the_factuality/</link>
      <pubDate>Mon, 27 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/inspecting_the_factuality/</guid>
      <description>Authors: Meng Cao, Yue Dong, Jackie Chi Kit Cheung</description>
    </item>
    
    <item>
      <title>Paper Review - Decision-Focused Summarization</title>
      <link>https://tangliyan.com/blog/posts/decision_focused_sum/</link>
      <pubDate>Wed, 22 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/decision_focused_sum/</guid>
      <description>Authors: Chao-Chun Hsu, Chenhao Tan</description>
    </item>
    
    <item>
      <title>Paper Review - A Survey of the State of Explainable AI for Natural Language Processing</title>
      <link>https://tangliyan.com/blog/posts/a_survey_of/</link>
      <pubDate>Tue, 21 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/a_survey_of/</guid>
      <description>Authors: Marina Danilevsky, Kun Qian, Ranit Aharonov, Yannis Katsis, Ban Kawas, Prithviraj Sen</description>
    </item>
    
    <item>
      <title>Paper Review - An Exploratory Study on Long Dialogue Summarization: What Works and What’s Next</title>
      <link>https://tangliyan.com/blog/posts/an_exploratory_study/</link>
      <pubDate>Tue, 14 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/an_exploratory_study/</guid>
      <description>Authors: Yusen Zhang Ansong Ni Tao Yu Rui Zhang Chenguang Zhu Budhaditya Deb Asli Celikyilmaz Ahmed H. Awadallah Dragomir Radev</description>
    </item>
    
    <item>
      <title>Paper Review - GNNExplainer: Generating Explanations for Graph Neural Networks</title>
      <link>https://tangliyan.com/blog/posts/gnnexplainer/</link>
      <pubDate>Mon, 13 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/gnnexplainer/</guid>
      <description>Authors: Rex Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, Jure Leskovec</description>
    </item>
    
    <item>
      <title>Paper Review - Factual Consistency Evaluation for Text Summarization via Counterfactual Estimation</title>
      <link>https://tangliyan.com/blog/posts/factual_consistency_evaluation/</link>
      <pubDate>Fri, 10 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/factual_consistency_evaluation/</guid>
      <description>Authors: Yuexiang Xie, Fei Sun, Yang Deng, Yaliang Li, Bolin Ding</description>
    </item>
    
    <item>
      <title>Paper Review - Discretized Integrated Gradients for Explaining Language Models</title>
      <link>https://tangliyan.com/blog/posts/discretized_integrated_gradients/</link>
      <pubDate>Wed, 08 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/discretized_integrated_gradients/</guid>
      <description>Authors: Soumya Sanyal, Xiang Ren</description>
    </item>
    
    <item>
      <title>Paper Review - What’s in a Summary? Laying the Groundwork for Advances in Hospital-Course Summarization</title>
      <link>https://tangliyan.com/blog/posts/what_is_in/</link>
      <pubDate>Tue, 07 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/what_is_in/</guid>
      <description>Authors: Griffin Adams, Emily Alsentzer, Mert Ketenci, Jason Zucker, Noémie Elhadad</description>
    </item>
    
    <item>
      <title>Paper Review - Causal Inference in Natural Language Processing: Estimation, Prediction, Interpretation and Beyond</title>
      <link>https://tangliyan.com/blog/posts/causal_inference_for/</link>
      <pubDate>Mon, 06 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/causal_inference_for/</guid>
      <description>Authors: Amir Feder, Katherine A. Keith, Emaad Manzoor, Reid Pryzant, Dhanya Sridhar, Zach Wood-Doughty, Jacob Eisenstein, Justin Grimmer, Roi Reichart, Margaret E. Roberts, Brandon M. Stewart, Victor Veitch, Diyi Yang</description>
    </item>
    
    <item>
      <title>Paper Review - Flexible Operations for Natural Language Deduction</title>
      <link>https://tangliyan.com/blog/posts/flexible_operations_for/</link>
      <pubDate>Thu, 02 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/flexible_operations_for/</guid>
      <description>Authors: Kaj Bostrom, Xinyu Zhao, Swarat Chaudhuri, Greg Durrett</description>
    </item>
    
    <item>
      <title>Paper Review - Discourse-Aware Neural Extractive Text Summarization</title>
      <link>https://tangliyan.com/blog/posts/discourse_aware_neural/</link>
      <pubDate>Sat, 21 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/discourse_aware_neural/</guid>
      <description>Authors: Jiacheng Xu, Zhe Gan, Yu Cheng, Jingjing Liu</description>
    </item>
    
    <item>
      <title>Paper Review - Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval</title>
      <link>https://tangliyan.com/blog/posts/approximate_nearst_neighbor/</link>
      <pubDate>Thu, 19 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/approximate_nearst_neighbor/</guid>
      <description>Authors: Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, Arnold Overwijk</description>
    </item>
    
    <item>
      <title>Paper Review - REALM: Retrieval-Augmented Language Model Pre-Training</title>
      <link>https://tangliyan.com/blog/posts/realm/</link>
      <pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/realm/</guid>
      <description>Authors: Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang</description>
    </item>
    
    <item>
      <title>Paper Review - Joint Retrieval and Generation Training for Grounded Text Generation</title>
      <link>https://tangliyan.com/blog/posts/joint_retrieval_and/</link>
      <pubDate>Mon, 16 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/joint_retrieval_and/</guid>
      <description>Authors: Yizhe Zhang, Siqi Sun, Xiang Gao, Yuwei Fang, Chris Brockett, Michel Galley, Jianfeng Gao, Bill Dolan</description>
    </item>
    
    <item>
      <title>Paper Review - Self-Supervised Meta-Learning for Few-Shot Natural Language Classification Tasks</title>
      <link>https://tangliyan.com/blog/posts/self_supervised_meta/</link>
      <pubDate>Sat, 14 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/self_supervised_meta/</guid>
      <description>Authors: Trapit Bansal, Rishikesh Jha, Tsendsuren Munkhdalai, Andrew McCallum</description>
    </item>
    
    <item>
      <title>Paper Review - Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</title>
      <link>https://tangliyan.com/blog/posts/model_agnostic_meta/</link>
      <pubDate>Fri, 13 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/model_agnostic_meta/</guid>
      <description>Authors: Chelsea Finn, Pieter Abbeel, Sergey Levine</description>
    </item>
    
    <item>
      <title>Paper Review - Noisy Channel Language Model Prompting for Few-Shot Text Classification</title>
      <link>https://tangliyan.com/blog/posts/noisy_channel_language/</link>
      <pubDate>Thu, 12 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/noisy_channel_language/</guid>
      <description>Authors: Sewon Min, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer</description>
    </item>
    
    <item>
      <title>Paper Review - TWAG: A Topic-guided Wikipedia Abstract Generator</title>
      <link>https://tangliyan.com/blog/posts/twag/</link>
      <pubDate>Wed, 11 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/twag/</guid>
      <description>Authors: Fangwei Zhu, Shangqing Tu, Jiaxin Shi, Juanzi Li, Lei Hou, Tong Cui</description>
    </item>
    
    <item>
      <title>Paper Review - Improving Factual Consistency of Abstractive Summarization via Question Answering</title>
      <link>https://tangliyan.com/blog/posts/improving_factual_consistency/</link>
      <pubDate>Tue, 10 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/improving_factual_consistency/</guid>
      <description>Authors: Feng Nan, Cicero Nogueira dos Santos, Henghui Zhu, Patrick Ng, Kathleen McKeown, Ramesh Nallapati, Dejiao Zhang, Zhiguo Wang, Andrew O. Arnold, Bing Xiang</description>
    </item>
    
    <item>
      <title>Paper Review - Dissecting Generation Modes for Abstractive Summarization Models via Ablation and Attribution</title>
      <link>https://tangliyan.com/blog/posts/dissecting_generation_modes/</link>
      <pubDate>Mon, 09 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/dissecting_generation_modes/</guid>
      <description>Authors: Jiacheng Xu, Greg Durrett</description>
    </item>
    
    <item>
      <title>Paper Review - Generating Query Focused Summaries from Query-Free Resources</title>
      <link>https://tangliyan.com/blog/posts/generating_query_focused/</link>
      <pubDate>Sun, 08 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/generating_query_focused/</guid>
      <description>Authors: Yumo Xu, Mirella Lapata</description>
    </item>
    
    <item>
      <title>Paper Review - BLEURT: Learning Robust Metrics for Text Generation</title>
      <link>https://tangliyan.com/blog/posts/bleurt/</link>
      <pubDate>Sat, 07 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/bleurt/</guid>
      <description>Authors: Thibault Sellam, Dipanjan Das, Ankur P. Parikh</description>
    </item>
    
    <item>
      <title>Paper Review - Big Bird: Transformers for Longer Sequences</title>
      <link>https://tangliyan.com/blog/posts/bigbird/</link>
      <pubDate>Fri, 06 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/bigbird/</guid>
      <description>Authors: Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed</description>
    </item>
    
    <item>
      <title>Paper Review - LongSumm 2021: Session based automatic summarization model for scientific document</title>
      <link>https://tangliyan.com/blog/posts/long_summ_2021/</link>
      <pubDate>Thu, 05 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/long_summ_2021/</guid>
      <description>Authors: Senci Ying, Yanzhao Zheng, Wuhe Zou</description>
    </item>
    
    <item>
      <title>Paper Review - Annotating and Modeling Fine-grained Factuality in Summarization</title>
      <link>https://tangliyan.com/blog/posts/annotating_and_modeling/</link>
      <pubDate>Wed, 04 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/annotating_and_modeling/</guid>
      <description>Authors: Tanya Goyal, Greg Durrett</description>
    </item>
    
    <item>
      <title>Paper Review - GSum: A General Framework for Guided Neural Abstractive Summarization</title>
      <link>https://tangliyan.com/blog/posts/gsum/</link>
      <pubDate>Tue, 03 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/gsum/</guid>
      <description>Authors: Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao Jiang, Graham Neubig</description>
    </item>
    
    <item>
      <title>Paper Review - Nutri-bullets Hybrid: Consensual Multi-document Summarization</title>
      <link>https://tangliyan.com/blog/posts/nutri_bullets/</link>
      <pubDate>Mon, 02 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/nutri_bullets/</guid>
      <description>Authors: Darsh Shah, Lili Yu, Tao Lei, Regina Barzilay</description>
    </item>
    
    <item>
      <title>Paper Review - CLIP: A Dataset for Extracting Action Items for Physicians from Hospital Discharge Notes</title>
      <link>https://tangliyan.com/blog/posts/clip/</link>
      <pubDate>Sun, 01 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/clip/</guid>
      <description>Authors: James Mullenbach, Yada Pruksachatkun, Sean Adler, Jennifer Seale, Jordan Swartz, T. Greg McKelvey, Hui Dai, Yi Yang, David Sontag</description>
    </item>
    
    <item>
      <title>Paper Review - Noisy Self-Knowledge Distillation for Text Summarization</title>
      <link>https://tangliyan.com/blog/posts/noisy_sele_knowledge/</link>
      <pubDate>Sat, 31 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/noisy_sele_knowledge/</guid>
      <description>Authors: Yang Liu, Sheng Shen, Mirella Lapata</description>
    </item>
    
    <item>
      <title>Paper Review - PAIR: Planning and Iterative Refinement in Pre-trained Transformers for Long Text Generation</title>
      <link>https://tangliyan.com/blog/posts/pair/</link>
      <pubDate>Fri, 30 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/pair/</guid>
      <description>Authors: Xinyu Hua, Lu Wang</description>
    </item>
    
    <item>
      <title>Paper Review - Improving Faithfulness in Abstractive Summarization with Contrast Candidate Generation and Selection</title>
      <link>https://tangliyan.com/blog/posts/improving_faithfulness_in/</link>
      <pubDate>Thu, 29 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/improving_faithfulness_in/</guid>
      <description>Authors: Sihao Chen, Fan Zhang, Kazoo Sone, Dan Roth</description>
    </item>
    
    <item>
      <title>Paper Review - Scoring Sentence Singletons and Pairs for Abstractive Summarization</title>
      <link>https://tangliyan.com/blog/posts/scoring_sentence_singletons/</link>
      <pubDate>Thu, 29 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/scoring_sentence_singletons/</guid>
      <description>Authors: Logan Lebanoff, Kaiqiang Song, Franck Dernoncourt, Doo Soon Kim, Seokhwan Kim, Walter Chang, Fei Liu</description>
    </item>
    
    <item>
      <title>Paper Review - Efficient Attentions for Long Document Summarization</title>
      <link>https://tangliyan.com/blog/posts/efficient_attentions_for/</link>
      <pubDate>Wed, 28 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/efficient_attentions_for/</guid>
      <description>Authors: Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, Lu Wang</description>
    </item>
    
    <item>
      <title>Paper Review - AdaptSum: Towards Low-Resource Domain Adaptation for Abstractive Summarization</title>
      <link>https://tangliyan.com/blog/posts/adaptsum/</link>
      <pubDate>Tue, 27 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/adaptsum/</guid>
      <description>Authors: Tiezheng Yu, Zihan Liu, Pascale Fung</description>
    </item>
    
    <item>
      <title>Paper Review - FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization</title>
      <link>https://tangliyan.com/blog/posts/feqa/</link>
      <pubDate>Tue, 27 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/feqa/</guid>
      <description>Authors: Esin Durmus, He He, Mona Diab</description>
    </item>
    
    <item>
      <title>Paper Review - Neural Text Summarization: A Critical Evaluation</title>
      <link>https://tangliyan.com/blog/posts/neural_text_summarization/</link>
      <pubDate>Mon, 26 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/neural_text_summarization/</guid>
      <description>Authors: Wojciech Kryściński, Nitish Shirish Keskar, Bryan McCann, Caiming Xiong, Richard Socher</description>
    </item>
    
    <item>
      <title>Paper Review - On Faithfulness and Factuality in Abstractive Summarization</title>
      <link>https://tangliyan.com/blog/posts/on_faithfulness_and/</link>
      <pubDate>Mon, 26 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/on_faithfulness_and/</guid>
      <description>Authors: Joshua Maynez, Shashi Narayan, Bernd Bohnet, Ryan McDonald</description>
    </item>
    
    <item>
      <title>Paper Review - Learning Neural Templates for Text Generation</title>
      <link>https://tangliyan.com/blog/posts/learning_neural_templates/</link>
      <pubDate>Sun, 25 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/learning_neural_templates/</guid>
      <description>Authors: Sam Wiseman, Stuart M. Shieber, Alexander M. Rush</description>
    </item>
    
    <item>
      <title>Paper Review - Bottom-Up Abstractive Summarization</title>
      <link>https://tangliyan.com/blog/posts/bottom_up_sum/</link>
      <pubDate>Fri, 23 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/bottom_up_sum/</guid>
      <description>Authors: Sebastian Gehrmann, Yuntian Deng, Alexander M. Rush</description>
    </item>
    
    <item>
      <title>Paper Review - Improving Zero and Few-Shot Abstractive Summarization with Intermediate Fine-tuning and Data Augmentation</title>
      <link>https://tangliyan.com/blog/posts/improving_zero_and/</link>
      <pubDate>Fri, 23 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/improving_zero_and/</guid>
      <description>Authors: Alexander R. Fabbri, Simeng Han, Haoyuan Li, Haoran Li, Marjan Ghazvininejad, Shafiq Joty, Dragomir Radev, Yashar Mehdad</description>
    </item>
    
    <item>
      <title>Paper Review - Decoupling Representation and Classifier for Long-Tailed Recognition</title>
      <link>https://tangliyan.com/blog/posts/decoupling_representation/</link>
      <pubDate>Fri, 08 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/decoupling_representation/</guid>
      <description>Authors: Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, Yannis Kalantidis</description>
    </item>
    
    <item>
      <title>Overlook of Relation Extraction</title>
      <link>https://tangliyan.com/blog/posts/relation_extraction/</link>
      <pubDate>Sun, 03 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/relation_extraction/</guid>
      <description>Information Extraction v.s. Relation Extraction Information Extraction: Information extraction is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents and other electronically represented sources.
Relation extraction (RE) is an important task in IE. It focuses on extracting relations between entities. A complete relation RE system consists of
 a named entity recognizer to identify named entities from text. an entity linker to link entities to existing knowledge graphs.</description>
    </item>
    
    <item>
      <title>Paper Review - Connecting the Dots: Document-level Neural Relation Extraction with Edge-oriented Graphs</title>
      <link>https://tangliyan.com/blog/posts/connecting_the_dots/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/connecting_the_dots/</guid>
      <description>Authors: Fenia Christopoulou, Makoto Miwa, Sophia Ananiadou</description>
    </item>
    
    <item>
      <title>Cross-Lingual Learning</title>
      <link>https://tangliyan.com/blog/posts/corss_lingual/</link>
      <pubDate>Mon, 28 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/corss_lingual/</guid>
      <description>Cross-lingual learning Most languages do not have training data available to create state-of-the-art models and thus our ability to create intelligent systems for these languages is limited as well.
Cross-lingual learning (CLL) is one possible remedy to solve the lack of data for low-resource languages. In essence, it is an effort to utilize annotated data from other languages when building new NLP models. When CLL is considered, target languages usually lack resources, while source languages are resource-rich and they can be used to improve the results for the former.</description>
    </item>
    
    <item>
      <title>BERT and RoBERTa </title>
      <link>https://tangliyan.com/blog/posts/bert_roberta/</link>
      <pubDate>Fri, 18 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/bert_roberta/</guid>
      <description>BERT Recap Overview  Bert (Bidirectional Encoder Representations from Transformers) uses a &amp;ldquo;masked language model&amp;rdquo; to randomly mask some tokens from the input and predict the original vocabulary id of the masked token. Bert shows that &amp;ldquo;pre-trained representations reduce the need for many heavily-engineered task-specific architectures&amp;rdquo;.  BERT Specifics There are two steps to the BERT framework: pre-training and fine-tuning   During pre training, the model is trained on unlabeled data over different pre-training tasks.</description>
    </item>
    
    <item>
      <title>Paper Review - What Does BERT Look At? An Analysis of BERT’s Attention</title>
      <link>https://tangliyan.com/blog/posts/bert_attn/</link>
      <pubDate>Mon, 14 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/bert_attn/</guid>
      <description>Authors: Kevin Clark, Urvashi Khandelwal, Omer Levy, Christopher D. Manning</description>
    </item>
    
    <item>
      <title>Paper Review - The More You Know: Using Knowledge Graphs for Image Classification</title>
      <link>https://tangliyan.com/blog/posts/the_more_you_know/</link>
      <pubDate>Wed, 02 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/the_more_you_know/</guid>
      <description>Authors: Kenneth Marino, Ruslan Salakhutdinov, Abhinav Gupta</description>
    </item>
    
    <item>
      <title>Graph Convolutional Neural Network -  Spectral Convolution</title>
      <link>https://tangliyan.com/blog/posts/spectral_conv/</link>
      <pubDate>Mon, 24 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/spectral_conv/</guid>
      <description>Fourier Transform Virtually everything in the world can be described via a waveform - a function of time, space or some other variable. For instance, sound waves, the price of a stock, etc. The Fourier Transform gives us a unique and powerful way of viewing these waveforms: All waveforms, no matter what you scribble or observe in the universe, are actually just the sum of simple sinusoids of different frequencies.</description>
    </item>
    
    <item>
      <title>Graph Convolutional Neural Network - Spatial Convolution</title>
      <link>https://tangliyan.com/blog/posts/spatial_conv/</link>
      <pubDate>Thu, 20 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/spatial_conv/</guid>
      <description>Note This is the second post of the Graph Neural Networks (GNNs) series.
Convolutional graph neural networks (ConvGNNs) Convolutional graph neural networks (ConvGNNs) generalize the operation of convolution from grid data to graph data. The main idea is to generate a node $v$’s representation by aggregating its own features $\mathbf{x}_{v}$ and neighbors’ features $\mathbf{x}_{u}$, where $u \in N(v)$. Different from RecGNNs, ConvGNNs stack fixed number of multiple graph convolutional layers with different weights to extract high-level node representations.</description>
    </item>
    
    <item>
      <title>Introduction to Graph Neural Network (GNN)</title>
      <link>https://tangliyan.com/blog/posts/gnn/</link>
      <pubDate>Mon, 17 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/gnn/</guid>
      <description>Note This is the first post of the Graph Neural Networks (GNNs) series.
Background and Intuition There is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. For examples, in e-commence, a graph-based learning system can exploit the interactions between users and products to make highly accurate recommendations. In chemistry, molecules are modeled as graphs.</description>
    </item>
    
    <item>
      <title>Kaggle: Jigsaw Multilingual Toxic Comment Classification - top solutions</title>
      <link>https://tangliyan.com/blog/posts/kaggle_jigsaw/</link>
      <pubDate>Tue, 11 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/kaggle_jigsaw/</guid>
      <description>Before we start Two of my previous post might be helpful in getting a general understanding of the top solutions of this competition. Please feel free to check them out.
 Knowledge Distillation clearly explained Common Multilingual Language Modeling methods (M-Bert, LASER, MultiFiT, XLM)  Jigsaw Multilingual Toxic Comment Classification  Use TPUs to identify toxicity comments across multiple languages.
 Overview of the competition Jigsaw Multilingual Toxic Comment Classification is the 3rd annual competition organized by the Jigsaw team.</description>
    </item>
    
    <item>
      <title>Multi-lingual: M-Bert, LASER, MultiFiT, XLM</title>
      <link>https://tangliyan.com/blog/posts/multilingual/</link>
      <pubDate>Sat, 08 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/multilingual/</guid>
      <description>Multilingual Models are a type of Machine Learning model that can understand different languages. In this post, I&amp;rsquo;m going to discuss four common multi-lingual language models Multilingual-Bert (M-Bert), Language-Agnostic SEntence Representations (LASER Embeddings), Efficient multi-lingual language model fine-tuning (MultiFiT) and Cross-lingual Language Model (XLM).
Ways of tokenization Word-based tokenization Word-based tokenization works well for the morphologically poor English, but results in very large and sparse vocabularies for morphologically rich languages, such as Polish and Turkish.</description>
    </item>
    
    <item>
      <title>Knowledge Distillation</title>
      <link>https://tangliyan.com/blog/posts/distillation/</link>
      <pubDate>Wed, 05 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/distillation/</guid>
      <description>Currently, especially in NLP, very large scale models are being trained. A large portion of those can’t even fit on an average person’s hardware. We can train a small network that can run on the limited computational resource of our mobile device. But small models can’t extract many complex features that can be handy in generating predictions unless you devise some elegant algorithm to do so. Plus, due to the Law of diminishing returns, a great increase in the size of model barely maps to a small increase in the accuracy.</description>
    </item>
    
    <item>
      <title>Kaggle: Tweet Sentiment Extraction - top solutions</title>
      <link>https://tangliyan.com/blog/posts/kaggle_tweet_sent2/</link>
      <pubDate>Thu, 02 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/kaggle_tweet_sent2/</guid>
      <description>Note This post is the second part of overall summarization of the competition. The first half is here.
Noteworthy ideas in 1st place solution Idea First step:
Use transformers to extract token level start and end probabilities.
Second step:
Feed these probabilities to a character level model. This step gives the team a huge improve on the final score since it handled the &amp;ldquo;noise&amp;rdquo; in the data properly.
Last step:</description>
    </item>
    
    <item>
      <title>Kaggle: Tweet Sentiment Extraction - common methods</title>
      <link>https://tangliyan.com/blog/posts/kaggle_tweet_sent1/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/kaggle_tweet_sent1/</guid>
      <description>Note This post is the first part of overall summarization of the competition. The second half is here.
Before we start I attended two NLP competition in June, Tweet Sentiment Extraction and Jigsaw Multilingual Toxic Comment Classification, and I&amp;rsquo;m happy to be a Kaggle Expert from now on :)
Tweet Sentiment Extraction Goal:
The objective in this competition is to &amp;ldquo;Extract support phrases for sentiment labels&amp;rdquo;. More precisely, this competition asks kagglers to construct a model that can figure out what word or phrase best supports the given tweet from the labeled sentiment.</description>
    </item>
    
    <item>
      <title>Recurrent Neural Network (RNN) and Long Short Term Memory (LSTM)</title>
      <link>https://tangliyan.com/blog/posts/lstm/</link>
      <pubDate>Thu, 04 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/lstm/</guid>
      <description>Sequence Data There are many sequence data in applications. Here are some examples
  Machine translation
 from text sequence to text sequence.    Text Summarization
 from text sequence to text sequence.    Sentiment classification
 from text sequence to categories.    Music Generation
 from nothing or some simple stuff (character, integer, etc) to wave sequence.    Name entity recognition (NER)</description>
    </item>
    
    <item>
      <title>Intro to Deep Learning and Backpropagation</title>
      <link>https://tangliyan.com/blog/posts/dl/</link>
      <pubDate>Tue, 26 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/dl/</guid>
      <description>Deep Learning v.s. Machine Learning The major difference between Deep Learning and Machine Learning technique is the problem solving approach. Deep Learning techniques tend to solve the problem end to end, where as Machine learning techniques need the problem statements to break down to different parts to be solved first and then their results to be combine at final stage.
Forward Propagation The general procedure is the following:
$$ \begin{aligned} a^{(1)}(x) &amp;amp;= w^{(1)^T} \cdot x + b^{(1)} \\ h^{(1)}(x) &amp;amp;= g_1(a^{(1)}(x)) \\ a^{(2)}(x) &amp;amp;= w^{(2)^T} \cdot h^{(1)}(x) + b^{(2)} \\ h^{(2)}(x) &amp;amp;= g_2(a^{(2)}(x)) \\ &amp;amp;&amp;hellip;&amp;hellip; \\ a^{(L+1)}(x) &amp;amp;= w^{(L+1)^T} \cdot h^{(L)}(x) + b^{(L+1)} \\ h^{(L+1)}(x) &amp;amp;= g_{L+1}(a^{(L+1)}(x)) \end{aligned} $$</description>
    </item>
    
    <item>
      <title>Log-Linear Model, Conditional Random Field(CRF)</title>
      <link>https://tangliyan.com/blog/posts/crf/</link>
      <pubDate>Tue, 19 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/crf/</guid>
      <description>Log-Linear model Let $x$ be an example, and let $y$ be a possible label for it. A log-linear model assumes that
$$ p(y | x ; w)=\frac{\exp [\sum_{j=1}^J w_{j} F_{j}(x, y)]}{Z(x, w)} $$
where the partition function
$$ Z(x, w)=\sum_{y^{\prime}} \exp [\sum_{j=1}^J w_{j} F_{j}\left(x, y^{\prime}\right)] $$
Note that in $\sum_{y^{\prime}}$, we make a summation over all possible $y$. Therefore, given $x$, the label predicted by the model is
$$ \hat{y}=\underset{y}{\operatorname{argmax}} p(y | x ; w)=\underset{y}{\operatorname{argmax}} \sum_{j=1}^J w_{j} F_{j}(x, y) $$</description>
    </item>
    
    <item>
      <title>Gaussian mixture model (GMM), k-means</title>
      <link>https://tangliyan.com/blog/posts/gmm/</link>
      <pubDate>Sat, 16 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/gmm/</guid>
      <description>Gaussian mixture model (GMM) A Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters.
Interpretation from geometry $p(x)$ is a weighted sum of multiple Gaussian distribution.
$$p(x)=\sum_{k=1}^{K} \alpha_{k} \cdot \mathcal{N}\left(x | \mu_{k}, \Sigma_{k}\right) $$
Interpretation from mixture model setup:
  The total number of Gaussian distribution $K$.
  $x$, a sample (observed variable).</description>
    </item>
    
    <item>
      <title>Probabilistic Graphical Model (PGM)</title>
      <link>https://tangliyan.com/blog/posts/pgm/</link>
      <pubDate>Mon, 11 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/pgm/</guid>
      <description>Probabilistic Graphical Model (PGM) Definition: A probabilistic graphical model is a probabilistic model for which a graph expresses the conditional dependence structure between random variables.
In general, PGM obeys following rules: $$ \begin{aligned} &amp;amp;\text {Sum Rule : } p\left(x_{1}\right)=\int p\left(x_{1}, x_{2}\right) d x_{2}\\ &amp;amp;\text {Product Rule : } p\left(x_{1}, x_{2}\right)=p\left(x_{1} | x_{2}\right) p\left(x_{2}\right)\\ &amp;amp;\text {Chain Rule: } p\left(x_{1}, x_{2}, \cdots, x_{p}\right)=\prod_{i=1}^{p} p\left(x_{i} | x_{i+1, x_{i+2}} \ldots x_{p}\right)\\ &amp;amp;\text {Bayesian Rule: } p\left(x_{1} | x_{2}\right)=\frac{p\left(x_{2} | x_{1}\right) p\left(x_{1}\right)}{p\left(x_{2}\right)} \end{aligned} $$</description>
    </item>
    
    <item>
      <title>Hidden Markov Model (HMM)</title>
      <link>https://tangliyan.com/blog/posts/hmm/</link>
      <pubDate>Sun, 03 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/hmm/</guid>
      <description>Before reading this post, make sure you are familiar with the EM Algorithm and decent among of knowledge of convex optimization. If not, please check out my previous post
  EM Algorithm
  convex optimization primal and dual problem
  Let&amp;rsquo;s get started!
Conditional independence $A$ and $B$ are conditionally independent given $C$ if and only if, given knowledge that $C$ occurs, knowledge of whether $A$ occurs provides no information on the likelihood of $B$ occurring, and knowledge of whether $B$ occurs provides no information on the likelihood of $A$ occurring.</description>
    </item>
    
    <item>
      <title>EM (Expectation–Maximization) Algorithm</title>
      <link>https://tangliyan.com/blog/posts/em/</link>
      <pubDate>Fri, 24 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/em/</guid>
      <description>Jensen’s inequality  Theorem: Let $f$ be a convex function, and let $X$ be a random variable. Then:  $$E[f(X)] \geq f(E[X])$$
$\quad$ Moreover, if $f$ is strictly convex, then $E[f(X)] = f(E[X])$ holds true if and only if $X$ is a constant.
 Later in the post we are going to use the following fact from the Jensen&amp;rsquo;s inequality: Suppose $\lambda_j \geq 0$ for all $j$ and $\sum_j \lambda_j = 1$, then  $$ \log \sum_j \lambda_j y_j \geq \sum_j \lambda_j , log , y_j$$</description>
    </item>
    
    <item>
      <title>Skip-gram</title>
      <link>https://tangliyan.com/blog/posts/skipgram/</link>
      <pubDate>Sat, 18 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/skipgram/</guid>
      <description>Comparison between CBOW and Skip-gram The major difference is that skip-gram is better for infrequent words than CBOW in word2vec. For simplicity, suppose there is a sentence &amp;ldquo;$w_1w_2w_3w_4$&amp;rdquo;, and the window size is $1$.
For CBOW, it learns to predict the word given a context, or to maximize the following probability
$$ p(w_2|w_1,w_3) \cdot P(w_3|w_2,w_4)$$
This is an issue for infrequent words, since they don’t appear very often in a given context.</description>
    </item>
    
    <item>
      <title>Distributed representation, Hyperbolic Space, Gaussian/Graph Embedding</title>
      <link>https://tangliyan.com/blog/posts/representation/</link>
      <pubDate>Fri, 17 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/representation/</guid>
      <description>Overview of various word representation and Embedding methods Local Representation v.s. Distributed Representation  One-hot encoding is local representation and is good for local generalization; distributed representation is good for global generalization.  Comparison between local generalization and global generalization:
Here is an example for better understanding this pair of concepts. Suppose now you have a bunch of ingredients and you&amp;rsquo;re able to cook 100 different meals with these ingredients.</description>
    </item>
    
    <item>
      <title>NLP Basics, Spell Correction with Noisy Channel</title>
      <link>https://tangliyan.com/blog/posts/nlp_basic/</link>
      <pubDate>Fri, 10 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/nlp_basic/</guid>
      <description>NLP = NLU + NLG  NLU: Natural Language Understanding NLG: Natural Language Generation  NLG may be viewed as the opposite of NLU: whereas in NLU, the system needs to disambiguate the input sentence to produce the machine representation language, in NLG the system needs to make decisions about how to put a concept into human understandable words.
Classical applications in NLP   Question Answering
  Sentiment Analysis</description>
    </item>
    
    <item>
      <title>Kaggle: Google Quest Q&amp;A Labeling - my solution</title>
      <link>https://tangliyan.com/blog/posts/kaggle_google_quest/</link>
      <pubDate>Sat, 04 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/kaggle_google_quest/</guid>
      <description>Kaggle: Google Quest Q&amp;amp;A Labeling summary General Part Congratulations to all winners of this competition. Your hard work paid off!
First, I have to say thanks to the authors of the following three published notebooks:
https://www.kaggle.com/akensert/bert-base-tf2-0-now-huggingface-transformer, https://www.kaggle.com/abhishek/distilbert-use-features-oof, https://www.kaggle.com/codename007/start-from-here-quest-complete-eda-fe.
These notebooks showed awesome ways to build models, visualize the dataset and extract features from non-text data.
Our initial plan was to take question title, question body and answer all into a Bert based model.</description>
    </item>
    
    <item>
      <title>SVM, Dual SVM, Non-linear SVM</title>
      <link>https://tangliyan.com/blog/posts/svm/</link>
      <pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/svm/</guid>
      <description>Linear SVM Idea We want to find a hyper-plane $w^\top x + b = 0$ that maximizes the margin.
Set up We first show that the vector $w$ is orthogonal to this hyper-plane. Let $x_1$, $x_2$ be any element on the hyper-plane. So we have $w^\top x_1 + b = 0$ and $w^\top x_2 + b = 0$. Then $w^\top (x_1 - x_2) = 0$, which implies $w$ is orthogonal to the hyper-plane.</description>
    </item>
    
    <item>
      <title>Introduction to Convex Optimization - Primal problem to Dual problem</title>
      <link>https://tangliyan.com/blog/posts/convex2/</link>
      <pubDate>Sun, 29 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/convex2/</guid>
      <description>Consider an optimization problem in the standard form (we call this a primal problem):
We denote the optimal value of this as $p^\star$. We don&amp;rsquo;t assume the problem is convex.
The Lagrange dual function We define the Lagrangian $L$ associated with the problem as $$ L(x,\lambda, v) = f_0(x) + \sum^m_{i=1}\lambda_if_i(x) + \sum^p_{i=1}v_ih_i(x)$$ We call vectors $\lambda$ and $v$ the dual variables or Lagrange multiplier vectors associated with the problem (1).</description>
    </item>
    
    <item>
      <title>Introduction to Convex Optimization - Basic Concepts</title>
      <link>https://tangliyan.com/blog/posts/convex1/</link>
      <pubDate>Sat, 28 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/convex1/</guid>
      <description>Optimization problem All optimization problems can be written as:
Optimization Categories   convex v.s. non-convex Deep Neural Network is non-convex
  continuous v.s.discrete Most are continuous variable; tree structure is discrete
  constrained v.s. non-constrained We add prior to make it a constrained problem
  smooth v.s.non-smooth Most are smooth optimization
  Different initialization brings different optimum (if not convex) Idea: Give up global optimal and find a good local optimal.</description>
    </item>
    
    <item>
      <title>XGBoost</title>
      <link>https://tangliyan.com/blog/posts/xgboost/</link>
      <pubDate>Fri, 27 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/xgboost/</guid>
      <description>Bagging v.s. Boosting: Bagging: Leverages unstable base learners that are weak because of overfitting.
Boosting: Leverages stable base learners that are weak because of underfitting.
XGBoost Learning Process through XGBoost:
 How to set a objective function. Hard to solve directly, how to approximate. How to add tree structures into the objective function. Still hard to optimize, then have to use greedy algorithms.  Step 1. How to set an objective function Suppose we trained $K$ trees, then the prediction for the ith sample is $$\hat{y}_i = \sum_{k=1}^K f_k(x_i), f_k \in \mathcal{F}$$ where $K$ is the number of trees, $f$ is a function in the functional space $\mathcal{F}$, and $\mathcal{F}$ is the set of all possible CARTs.</description>
    </item>
    
    <item>
      <title>Maximum Likelihood Estimation (MLE) and Maximum a Posteriori (MAP)</title>
      <link>https://tangliyan.com/blog/posts/mle/</link>
      <pubDate>Thu, 26 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/mle/</guid>
      <description>MLE v.s. MAP  MLE: learn parameters from data. MAP: add a prior (experience) into the model; more reliable if data is limited. As we have more and more data, the prior becomes less useful. As data increase, MAP $\rightarrow$ MLE.  Notation: $D = {(x_1, y_1), (x_2, y_2), &amp;hellip;, (x_n, y_n)}$
Framework:
 MLE: $\mathop{\rm arg\ max} P(D \ |\ \theta)$ MAP: $\mathop{\rm arg\ max} P(\theta \ |\ D)$  Note that taking a product of some numbers less than 1 would approaching 0 as the number of those numbers goes to infinity, it would be not practical to compute, because of computation underflow.</description>
    </item>
    
    <item>
      <title>Logistic Regression, L1, L2 regularization, Gradient/Coordinate descent</title>
      <link>https://tangliyan.com/blog/posts/logistic_regression/</link>
      <pubDate>Wed, 25 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>https://tangliyan.com/blog/posts/logistic_regression/</guid>
      <description>Generative model v.s. Discriminative model： Examples:
 Generative model: Naive Bayes, HMM, VAE, GAN. Discriminative model：Logistic Regression, CRF.  Objective function:
 Generative model: $max \ p \ (x,y)$ Discriminative model：$max \ p \ (y \ |\ x)$  Difference:
 Generative model: We first assume a distribution of the data in the consideration of computation efficiency and features of the data. Next, the model will learn the parameters of distribution of the data.</description>
    </item>
    
  </channel>
</rss>
