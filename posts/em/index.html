<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>EM (Expectation–Maximization) Algorithm | Liyan Tang</title>
<meta name=keywords content="ML,MATH">
<meta name=description content="Jensen’s inequality  Theorem: Let $f$ be a convex function, and let $X$ be a random variable. Then:  $$E[f(X)] \geq f(E[X])$$
$\quad$ Moreover, if $f$ is strictly convex, then $E[f(X)] = f(E[X])$ holds true if and only if $X$ is a constant.
 Later in the post we are going to use the following fact from the Jensen&rsquo;s inequality: Suppose $\lambda_j \geq 0$ for all $j$ and $\sum_j \lambda_j = 1$, then  $$ \log \sum_j \lambda_j y_j \geq \sum_j \lambda_j , log , y_j$$">
<meta name=author content>
<link rel=canonical href=https://tangliyan.com/blog/posts/em/>
<link crossorigin=anonymous href=/blog/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css integrity="sha256-yIlj/i15RiAA/Q+xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/blog/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://tangliyan.com/blog/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://tangliyan.com/blog/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://tangliyan.com/blog/favicon-32x32.png>
<link rel=apple-touch-icon href=https://tangliyan.com/blog/apple-touch-icon.png>
<link rel=mask-icon href=https://tangliyan.com/blog/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.92.0">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-202974782-1','auto'),ga('send','pageview'))</script><meta property="og:title" content="EM (Expectation–Maximization) Algorithm">
<meta property="og:description" content="Jensen’s inequality  Theorem: Let $f$ be a convex function, and let $X$ be a random variable. Then:  $$E[f(X)] \geq f(E[X])$$
$\quad$ Moreover, if $f$ is strictly convex, then $E[f(X)] = f(E[X])$ holds true if and only if $X$ is a constant.
 Later in the post we are going to use the following fact from the Jensen&rsquo;s inequality: Suppose $\lambda_j \geq 0$ for all $j$ and $\sum_j \lambda_j = 1$, then  $$ \log \sum_j \lambda_j y_j \geq \sum_j \lambda_j , log , y_j$$">
<meta property="og:type" content="article">
<meta property="og:url" content="https://tangliyan.com/blog/posts/em/"><meta property="og:image" content="https://tangliyan.com/blog/papermod-cover.png"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2020-04-24T00:00:00+00:00">
<meta property="article:modified_time" content="2020-04-24T00:00:00+00:00">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://tangliyan.com/blog/papermod-cover.png">
<meta name=twitter:title content="EM (Expectation–Maximization) Algorithm">
<meta name=twitter:description content="Jensen’s inequality  Theorem: Let $f$ be a convex function, and let $X$ be a random variable. Then:  $$E[f(X)] \geq f(E[X])$$
$\quad$ Moreover, if $f$ is strictly convex, then $E[f(X)] = f(E[X])$ holds true if and only if $X$ is a constant.
 Later in the post we are going to use the following fact from the Jensen&rsquo;s inequality: Suppose $\lambda_j \geq 0$ for all $j$ and $\sum_j \lambda_j = 1$, then  $$ \log \sum_j \lambda_j y_j \geq \sum_j \lambda_j , log , y_j$$">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://tangliyan.com/blog/posts/"},{"@type":"ListItem","position":2,"name":"EM (Expectation–Maximization) Algorithm","item":"https://tangliyan.com/blog/posts/em/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"EM (Expectation–Maximization) Algorithm","name":"EM (Expectation–Maximization) Algorithm","description":"Jensen’s inequality  Theorem: Let $f$ be a convex function, and let $X$ be a random variable. Then:  $$E[f(X)] \\geq f(E[X])$$\n$\\quad$ Moreover, if $f$ is strictly convex, then $E[f(X)] = f(E[X])$ holds true if and only if $X$ is a constant.\n Later in the post we are going to use the following fact from the Jensen\u0026rsquo;s inequality: Suppose $\\lambda_j \\geq 0$ for all $j$ and $\\sum_j \\lambda_j = 1$, then  $$ \\log \\sum_j \\lambda_j y_j \\geq \\sum_j \\lambda_j , log , y_j$$","keywords":["ML","MATH"],"articleBody":"Jensen’s inequality  Theorem: Let $f$ be a convex function, and let $X$ be a random variable. Then:  $$E[f(X)] \\geq f(E[X])$$\n$\\quad$ Moreover, if $f$ is strictly convex, then $E[f(X)] = f(E[X])$ holds true if and only if $X$ is a constant.\n Later in the post we are going to use the following fact from the Jensen’s inequality: Suppose $\\lambda_j \\geq 0$ for all $j$ and $\\sum_j \\lambda_j = 1$, then  $$ \\log \\sum_j \\lambda_j y_j \\geq \\sum_j \\lambda_j , log , y_j$$\n$\\quad$ where the $\\log$ function is concave.\nOverview of Expectation–Maximization (EM) algorithm In this post, let $Y$ be a set of observed data, $Z$ a set of unobserved latent data, and $\\theta$ the unknown parameters.\n(After this post, you will be comfortable with the following description about the EM algorithm.)\nExpectation–Maximization (EM) algorithm is an iterative method to find (local) maximum likelihood estimation (MLE) of $L(\\theta) = p(Y|\\theta)$, where the model depends on unobserved latent variables $Z$.\nAlgorithm:\n Initialize peremeters $\\theta_0$.  Iterate between steps 2 and 3 until convergence:\nan expectation (E) step, which creates a function $Q(\\theta, \\theta_i)$ for the expectation of the log-likelihood $\\log p(Y,Z|\\theta)$ evaluated using the current conditional distribution of $Z$ given $Y$ and the current estimate of the parameters $\\theta_i$, where  $$ \\begin{aligned} Q(\\theta, \\theta_i) \u0026= \\sum_Z P(Z|Y,\\theta_i) \\cdot \\log p(Y,Z|\\theta) \\\\ \u0026= E_{Z \\sim P(Z|Y,\\theta_i)}[\\log p(Y,Z|\\theta)] \\end{aligned} $$\nA maximization (M) step, which computes parameters maximizing the expected log-likelihood $Q(\\theta, \\theta_i)$ found on the $E$ step and then update parameters to $\\theta_{i+1}$.  These parameter-estimates are then used to determine the distribution of the latent variables in the next $E$ step. We say it converges if the increase in successive iterations is smaller than some tolerance parameter.\nIn general, multiple maxima may occur, with no guarantee that the global maximum will be found.\nIntuition: Why we need EM algorithm Sometimes maximizing the likelihood $\\ell(\\theta)$ explicitly might be difficult since there are some unknown latent variables. In such a setting, the EM algorithm gives an efficient method for maximum likelihood estimation.\nComplete Case v.s. Incomplete Case Complete case\n$(Y, Z)$ is observable, and the log likelihood can be written as\n$$ \\begin{aligned} \\ell(\\theta) \u0026= \\log p(Y, Z | \\theta) \\\\ \u0026= \\log p(Z|\\theta) \\cdot p(Y|Z, \\theta) \\\\ \u0026= \\log p(Z|\\theta) + \\log p(Y|Z, \\theta) \\ \\end{aligned} $$\nWe subdivide our task of maximizing $\\ell(\\theta)$ into two sub-tasks. Note that in both $\\log p(Z|\\theta)$ and $\\log p(Y|Z, \\theta)$, the only unknown parameter is $\\theta$. They are just two standard MLE problems which could be easily solved by methods such as gradient descent.\nIncomplete case\n$(Y)$ is observable, but $(Z)$ is unknown. We need to introduce the marginal distribution of variable $Z$:\n$$ \\begin{aligned} \\ell(\\theta) \u0026= \\log p(Y | \\theta) \\\\ \u0026= \\log \\sum_Z p(Y, Z | \\theta) \\\\ \u0026= \\log \\sum_Z p(Z|\\theta) \\cdot p(Y|Z, \\theta) \\ \\end{aligned} $$\nHere we have a summation inside the log, so it’s hard to use optimization methods or take derivatives. This is the case where EM algorithm comes into aid.\nEM Algorithm Derivation (Using MLE) Given the observed data $Y$, we want to maximize the likelihood $\\ell(\\theta) = p(Y|\\theta)$, and it’s the same as maximizing the log-likelihood $\\log p(Y|\\theta)$. Therefore, from now on we will try to maximize the likelihood\n$$\\ell(\\theta) = \\log p(Y|\\theta)$$\nby taking the unknown variable $Z$ into account, we rewrite the objective function as\n$$ \\begin{aligned} \\ell(\\theta) \u0026= \\log p(Y|\\theta) \\\\ \u0026= \\log \\sum_Z p(Y, Z | \\theta) \\\\ \u0026= \\log \\sum_Z p(Y|Z,\\theta) \\cdot p(Z|\\theta) \\ \\end{aligned} $$\nNote that in the last step, the $\\log$ is outside of the $\\sum$, which is hard to compute and optimize. Check out my previous post to know why we prefer to have $\\log$ inside of $\\sum$, instead of outside. So later we would find a way to approximate it (Jensen’s inequality).\nSuppose we follow the iteration step 2 (E) and 3 (M) repeatedly, and have updated parameters to $\\theta_i$, then the difference between $\\ell(\\theta)$ and our estimate $\\ell(\\theta_i)$ is $\\ell(\\theta) - \\ell(\\theta_i)$. You can think of this difference as the improvement that later estimate of $\\theta$ tries to achieve. So our next step is to find $\\theta_{i+1}$ such that it improves the difference the most. That is, to make the difference $\\ell(\\theta) - \\ell(\\theta_i)$ as large as possible. So we want our next estimate $\\theta_{i+1}$ to be\n$$\\theta_{i+1} = \\mathop{\\rm arg,max}\\limits_{\\theta} ,, \\ell(\\theta) - \\ell(\\theta_i)$$\nNote that we know the value of $\\theta_i$, so as $\\ell(\\theta_i)$. And\n$$ \\begin{aligned} \\ell(\\theta) - \\ell(\\theta_i)\u0026= \\log p(Y|\\theta) \\\\ \u0026= \\log \\sum_Z p(Y|Z,\\theta) \\cdot p(Z|\\theta) - \\log p(Y|\\theta_i) \\\\ \u0026= \\log \\sum_Z P(Z|Y,\\theta_i) \\frac{p(Y|Z,\\theta) \\cdot p(Z|\\theta)}{P(Z|Y,\\theta_i)} - \\log p(Y|\\theta_i) \u0026 \u0026 \u0026(1)\\ \\end{aligned} $$\nSince $P(Z|Y,\\theta_i) \\geq 0$ for all $z\\in Z$ and $\\sum_Z P(Z|Y,\\theta_i) = 1$, we can use the Jensen’s inequality and then re-write $(1)$ as\n$$ \\begin{aligned} \\ell(\\theta) - \\ell(\\theta_i) \u0026\\geq \\sum_Z P(Z|Y,\\theta_i) \\log \\frac{p(Y|Z,\\theta) \\cdot p(Z|\\theta)}{P(Z|Y,\\theta_i)} - \\sum_Z P(Z|Y,\\theta_i)) \\cdot \\log p(Y|\\theta_i)\\\\ \u0026= \\sum_Z P(Z|Y,\\theta_i) \\left( \\log \\frac{p(Y|Z,\\theta) \\cdot p(Z|\\theta)}{P(Z|Y,\\theta_i)} - \\log p(Y|\\theta_i) \\right) \\\\ \\ell(\\theta) \u0026\\geq \\ell(\\theta_i) + \\sum_Z P(Z|Y,\\theta_i) \\log \\frac{p(Y|Z,\\theta) \\cdot p(Z|\\theta)}{P(Z|Y,\\theta_i) \\cdot p(Y|\\theta_i)} \\end{aligned} $$\nNow we define\n$$ B(\\theta, \\theta_i) \\triangleq \\ell(\\theta_i) + \\sum_Z P(Z|Y,\\theta_i) \\log \\frac{p(Y|Z,\\theta) \\cdot p(Z|\\theta)}{P(Z|Y,\\theta_i) \\cdot p(Y|\\theta_i)} $$\nSo we see that\n$$ \\ell(\\theta) \\geq B(\\theta, \\theta_i)$$\nwhich implies that $B(\\theta, \\theta_i)$ is a lower bound of $\\ell(\\theta)$ for all $i$. Therefore our next step is to maximize the lower bound $B(\\theta, \\theta_i)$ and make it as tight as possible. In the $M$ step, we define\n$$ \\begin{aligned} \\theta_{i+1} \u0026= \\mathop{\\rm arg,max}\\limits_{\\theta} B(\\theta, \\theta_i)\\\\ \u0026= \\mathop{\\rm arg,max}\\limits_{\\theta} , \\left(\\ell(\\theta_i) + \\sum_Z P(Z|Y,\\theta_i) \\cdot \\log \\frac{p(Y|Z,\\theta) \\cdot p(Z|\\theta)}{P(Z|Y,\\theta_i) \\cdot p(Y|\\theta_i)} \\right) \u0026\u0026 (2)\\\\ \u0026= \\mathop{\\rm arg,max}\\limits_{\\theta} , \\left(\\sum_Z P(Z|Y,\\theta_i) \\cdot \\log \\frac{p(Y|Z,\\theta) \\cdot p(Z|\\theta)}{P(Z|Y,\\theta_i) \\cdot p(Y|\\theta_i)} \\right) \u0026\u0026 (3)\\\\ \u0026= \\mathop{\\rm arg,max}\\limits_{\\theta} , \\left(\\sum_Z P(Z|Y,\\theta_i) \\cdot [\\log p(Y|Z,\\theta) \\cdot p(Z|\\theta) - \\log P(Z|Y,\\theta_i) \\cdot p(Y|\\theta_i)] \\right) \u0026\u0026 (4)\\\\ \u0026= \\mathop{\\rm arg,max}\\limits_{\\theta} , \\left(\\sum_Z P(Z|Y,\\theta_i) \\cdot \\log p(Y|Z,\\theta) \\cdot p(Z|\\theta) \\right) \u0026\u0026 (5)\\\\ \u0026= \\mathop{\\rm arg,max}\\limits_{\\theta} , \\left(\\sum_Z P(Z|Y,\\theta_i) \\cdot \\log p(Y,Z|\\theta) \\right) \u0026\u0026(6)\\\\ \u0026= \\mathop{\\rm arg,max}\\limits_{\\theta} , Q(\\theta, \\theta_i) \u0026\u0026(7)\\ \\end{aligned} $$\nRemark:\n  $(2) \\to (3)$ since $\\ell(\\theta_i)$ does not contain $\\theta$.\n  $(3) \\to (4)$ we used $\\log \\frac{A}{B} = \\log A - \\log B$.\n  $(4) \\to (5)$ since $\\log P(Z|Y,\\theta_i) \\cdot p(Y|\\theta_i)$ does not contain $\\theta$.\n  $(6) \\to (7)$ we define $Q(\\theta, \\theta_i) = \\sum_Z P(Z|Y,\\theta_i) \\cdot \\log p(Y,Z|\\theta)$.\n  since both $Y$ and $\\theta_i$ are known, we have the distribution of $Z \\sim p(Z|Y,\\theta_i)$. Therefore, the only unknown parameter is $\\theta$, which means this is now a complete case I mentioned early in the post. So this is now a MLE problem.\n  summary   $\\theta_{i+1} = \\mathop{\\rm arg,max}\\limits_{\\theta} \\ell(\\theta) - \\ell(\\theta_i) = \\mathop{\\rm arg,max}\\limits_{\\theta} , Q(\\theta, \\theta_i)$. This implies that maximizing $\\ell(\\theta) - \\ell(\\theta_i)$ is the same as maximizing $Q(\\theta, \\theta_i)$.\n  Note that $Q(\\theta, \\theta_i) = \\sum_Z P(Z|Y,\\theta_i) \\cdot \\log p(Y,Z|\\theta)$ is just the expectation of $\\log p(Y,Z|\\theta)$, where $Z$ is drawn from the current conditional distribution $P(Z|Y,\\theta_i)$. Therefore we have $$ Q(\\theta, \\theta_i) = E_{Z \\sim p(Z|Y,\\theta_i)}[\\log p(Y,Z|\\theta)]$$ That’s why it’s called the Expectation step. In E step, we are actually trying to calculate the expectation of the term $\\log p(Y,Z|\\theta)$, where unknown variable $Z$ follows the current conditional distribution given by $Y$ and $\\theta_i$. Then in the Maximazation step, we are trying to maximize this expectation. That’s why it’s called the M step.\n  Coordinate Ascent/ descent - view EM from a different prospect In the Expectation Step, we actually fixed $\\theta_i$, and tried to optimize $Q(\\theta, \\theta_i)$.\nIn the Maximization step, we actually fixed $Q(\\theta, \\theta_i)$, and tried to optimize $\\theta$ to get $\\theta_{i+1}$.\nEvery time we only optimize one variable and fix the rest. Therefore, from the graph above we see that in every iteration the gradient changes either vertically or horizontally.\nTo see how to perform the coordinate descent, check out my previous post.\nConvergence of EM algorithm By following the algorithm, we keep updating parameter $\\theta_i$ and calculating approximated log-likelihood $\\ell (\\theta_i)$. But do we actually keep $\\ell (\\theta_i)$ getting closer to $l(\\theta)$ as we do more iterations? Keep in mind that in MLE our final goal is to maximize $\\ell (\\theta)$.\nSuppose $\\theta_i$ and $\\theta_{i+1}$ are the parameters from two successive iterations of EM. We will now prove that $\\ell(\\theta_i) \\leq \\ell(\\theta_{i+1})$, which shows EM always monotonically improves the log-likelihood.\n$$ \\begin{aligned} \\ell(\\theta) \u0026= \\log p(Y|\\theta) \\\\ \u0026= \\log \\frac {p(Y,Z|\\theta)}{p(Z|Y, \\theta)} \\\\ \u0026= \\log p(Y,Z|\\theta) - \\log p(Z|Y, \\theta) \\\\ \u0026= \\sum_Z p(Z|Y, \\theta_i)\\cdot \\log p(Y,Z|\\theta) - \\sum_Z p(Z|Y, \\theta_i)\\cdot \\log p(Z|Y, \\theta) \\\\ \u0026= Q(\\theta, \\theta_i) + H(\\theta, \\theta_i) \u0026\u0026 (8)\\ \\end{aligned} $$\nwhere $H(\\theta, \\theta_i) = - \\sum_Z p(Z|Y, \\theta_i)\\cdot \\log p(Z|Y, \\theta)$. This last equation $(8)$ holds for every value of $\\theta$, including $\\theta=\\theta_i$, which means\n$$ \\ell(\\theta_i) = Q(\\theta_i, \\theta_i) + H(\\theta_i, \\theta_i)$$\nTherefore subtracting $\\ell(\\theta_i)$ from $\\ell(\\theta_{i+1})$ gives\n$$ \\begin{aligned} \\ell(\\theta_{i+1}) - \\ell(\\theta_i) \u0026= [Q(\\theta_{i+1}, \\theta_{i}) + H(\\theta_{i+1}, \\theta_{i})] - [Q(\\theta_i, \\theta_i) + H(\\theta_i, \\theta_i)]\\\\ \u0026= [Q(\\theta_{i+1}, \\theta_{i}) - Q(\\theta_i, \\theta_{i})] + [H(\\theta_{i+1}, \\theta_{i}) - H(\\theta_i, \\theta_{i})] \\ \\end{aligned} $$\nSince $\\theta_{i+1} = \\mathop{\\rm arg,max}\\limits_{\\theta} , Q(\\theta, \\theta_i)$, we have $Q(\\theta_{i+1}, \\theta_i) \\geq Q(\\theta_i, \\theta_i)$. The second parenthesis gives\n$$ \\begin{aligned} H(\\theta_{i+1}, \\theta_{i}) - H(\\theta_i, \\theta_{i}) \u0026= - \\sum_Z p(Z|Y, \\theta_i)\\cdot \\log p(Z|Y, \\theta_{i+1}) + \\sum_Z p(Z|Y, \\theta_i)\\cdot \\log p(Z|Y, \\theta_i) \\\\ \u0026= - \\left( \\sum_Z p(Z|Y, \\theta_i)\\cdot \\log p(Z|Y, \\theta_{i+1}) - \\sum_Z p(Z|Y, \\theta_i)\\cdot \\log p(Z|Y, \\theta_i)\\right) \\\\ \u0026= - \\left( \\sum_Z p(Z|Y, \\theta_i) \\cdot \\log \\frac{p(Z|Y, \\theta_{i+1})}{p(Z|Y, \\theta_{i})} \\right) \u0026\u0026 \\ \\ \\ (9)\\\\ \u0026\\geq - \\log \\left( \\sum_Z p(Z|Y, \\theta_i) \\cdot \\frac{p(Z|Y, \\theta_{i+1})}{p(Z|Y, \\theta_{i})} \\right) \u0026\u0026 (10)\\\\ \u0026= - \\log \\sum_Z p(Z|Y, \\theta_{i+1}) = - \\log 1 = 0 \\end{aligned} $$\nFrom $(9)$ to $(10)$ we use the Jensen’s inequality (note that there is a negative sign in the front so we reverse the inequality).\nSince $Q(\\theta_{i+1}, \\theta_{i}) \\geq Q(\\theta_i, \\theta_{i})$ and $H(\\theta_{i+1}, \\theta_{i}) \\geq H(\\theta_i, \\theta_{i})$, we have\n$$ \\ell(\\theta_{i+1}) \\geq \\ell(\\theta_i) \\qquad\\qquad \\text{for all} , i$$\nSince it’s monotonically increasing and bounded above, we say $\\ell (\\theta_i)$ converges.\nWhat’s next? I’m going to write a post to discuss how EM algorithm is applied in K-means and GMM in the future. Stay tuned!\n Reference:\n https://en.wikipedia.org/wiki/Expectation–maximization_algorithm Part IX: The EM algorithm from CS229 Lecture notes by Andrew Ng http://cs229.stanford.edu/notes/cs229-notes8.pdf http://guillefix.me/cosmos/static/Jensen%27s%2520inequality.html https://en.wikipedia.org/wiki/Jensen%27s_inequality http://www.adeveloperdiary.com/data-science/machine-learning/introduction-to-coordinate-descent-using-least-squares-regression/  ","wordCount":"1698","inLanguage":"en","datePublished":"2020-04-24T00:00:00Z","dateModified":"2020-04-24T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://tangliyan.com/blog/posts/em/"},"publisher":{"@type":"Organization","name":"Liyan Tang","logo":{"@type":"ImageObject","url":"https://tangliyan.com/blog/favicon.ico"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://tangliyan.com/blog/ accesskey=h title="Liyan Tang (Alt + H)">Liyan Tang</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
<li>
<a href=https://tangliyan.com/blog/archives title=Archive>
<span>Archive</span>
</a>
</li>
<li>
<a href=https://tangliyan.com/blog/tags/ title=Tags>
<span>Tags</span>
</a>
</li>
<li>
<a href=https://tangliyan.com/blog/search/ title="Search (Alt + /)" accesskey=/>
<span>Search</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<div class=breadcrumbs><a href=https://tangliyan.com/blog/>Home</a>&nbsp;»&nbsp;<a href=https://tangliyan.com/blog/posts/>Posts</a></div>
<h1 class=post-title>
EM (Expectation–Maximization) Algorithm
</h1>
<div class=post-meta><span title="2020-04-24 00:00:00 +0000 UTC">April 24, 2020</span>&nbsp;·&nbsp;8 min
</div>
</header> <div class=toc>
<details>
<summary accesskey=c title="(Alt + C)">
<span class=details>Table of Contents</span>
</summary>
<div class=inner><ul>
<li>
<a href=#jensens-inequality aria-label="Jensen’s inequality">Jensen’s inequality</a></li>
<li>
<a href=#overview-of-expectationmaximization-em-algorithm aria-label="Overview of Expectation–Maximization (EM) algorithm">Overview of Expectation–Maximization (EM) algorithm</a></li>
<li>
<a href=#intuition-why-we-need-em-algorithm aria-label="Intuition: Why we need EM algorithm">Intuition: Why we need EM algorithm</a><ul>
<li>
<a href=#complete-case-vs-incomplete-case aria-label="Complete Case v.s. Incomplete Case">Complete Case v.s. Incomplete Case</a></li></ul>
</li>
<li>
<a href=#em-algorithm-derivation-using-mle aria-label="EM Algorithm Derivation (Using MLE)">EM Algorithm Derivation (Using <em>MLE</em>)</a><ul>
<li>
<a href=#summary aria-label=summary>summary</a></li>
<li>
<a href=#coordinate-ascent-descent---view-em-from-a-different-prospect aria-label="Coordinate Ascent/ descent - view EM from a different prospect">Coordinate Ascent/ descent - view EM from a different prospect</a></li></ul>
</li>
<li>
<a href=#convergence-of-em-algorithm aria-label="Convergence of EM algorithm">Convergence of EM algorithm</a></li>
<li>
<a href=#whats-next aria-label="What&amp;rsquo;s next?">What&rsquo;s next?</a>
</li>
</ul>
</div>
</details>
</div>
<div class=post-content><h2 id=jensens-inequality>Jensen’s inequality<a hidden class=anchor aria-hidden=true href=#jensens-inequality>#</a></h2>
<img src=https://img-blog.csdnimg.cn/20200424052805135.png width=500>
<ul>
<li><em>Theorem</em>: Let $f$ be a convex function, and let $X$ be a random variable. Then:</li>
</ul>
<p>$$E[f(X)] \geq f(E[X])$$</p>
<p>$\quad$ Moreover, if $f$ is strictly convex, then $E[f(X)] = f(E[X])$ holds true if and
only if $X$ is a constant.</p>
<ul>
<li>Later in the post we are going to use the following fact from the Jensen&rsquo;s inequality:
Suppose $\lambda_j \geq 0$ for all $j$ and $\sum_j \lambda_j = 1$, then</li>
</ul>
<p>$$ \log \sum_j \lambda_j y_j \geq \sum_j \lambda_j , log , y_j$$</p>
<p>$\quad$ where the $\log$ function is concave.</p>
<h2 id=overview-of-expectationmaximization-em-algorithm>Overview of Expectation–Maximization (EM) algorithm<a hidden class=anchor aria-hidden=true href=#overview-of-expectationmaximization-em-algorithm>#</a></h2>
<p><strong>In this post, let $Y$ be a set of observed data, $Z$ a set of unobserved latent data, and $\theta$ the unknown parameters.</strong></p>
<p>(After this post, you will be comfortable with the following description about the EM algorithm.)</p>
<p>Expectation–Maximization (EM) algorithm is <em>an iterative method</em> to find <em>(local)</em> maximum likelihood estimation (MLE) of $L(\theta) = p(Y|\theta)$, where the model depends on <em>unobserved latent variables</em> $Z$.</p>
<p><em>Algorithm:</em></p>
<ol>
<li>Initialize peremeters $\theta_0$.</li>
</ol>
<p>Iterate between steps 2 and 3 until convergence:</p>
<ol start=2>
<li>an <em>expectation (E) step</em>, which creates a function $Q(\theta, \theta_i)$ for the expectation of the log-likelihood $\log p(Y,Z|\theta)$ evaluated using the current conditional distribution of
$Z$ given $Y$ and the current estimate of the parameters $\theta_i$, where</li>
</ol>
<p>$$
\begin{aligned}
Q(\theta, \theta_i) &= \sum_Z P(Z|Y,\theta_i) \cdot \log p(Y,Z|\theta) \\ &= E_{Z \sim P(Z|Y,\theta_i)}[\log p(Y,Z|\theta)]
\end{aligned}
$$</p>
<ol start=3>
<li>A <em>maximization (M) step</em>, which computes parameters maximizing the expected log-likelihood $Q(\theta, \theta_i)$ found on the $E$ step and then update parameters to $\theta_{i+1}$.</li>
</ol>
<p>These parameter-estimates are then used to determine the distribution of the latent variables in the next $E$ step. We say it converges if the increase in successive iterations is smaller than some tolerance parameter.</p>
<p><em>In general, multiple maxima may occur, with no guarantee that the global maximum will be found.</em></p>
<h2 id=intuition-why-we-need-em-algorithm>Intuition: Why we need EM algorithm<a hidden class=anchor aria-hidden=true href=#intuition-why-we-need-em-algorithm>#</a></h2>
<p>Sometimes maximizing the likelihood $\ell(\theta)$ explicitly might be difficult since there are some unknown latent variables. In such a setting, the EM algorithm gives an efficient method for maximum likelihood estimation.</p>
<h3 id=complete-case-vs-incomplete-case>Complete Case v.s. Incomplete Case<a hidden class=anchor aria-hidden=true href=#complete-case-vs-incomplete-case>#</a></h3>
<p><strong>Complete case</strong></p>
<p>$(Y, Z)$ is observable, and the log likelihood can be written as</p>
<p>$$
\begin{aligned}
\ell(\theta) &= \log p(Y, Z | \theta) \\ &= \log p(Z|\theta) \cdot p(Y|Z, \theta) \\ &= \log p(Z|\theta) + \log p(Y|Z, \theta) \
\end{aligned}
$$</p>
<p>We subdivide our task of maximizing $\ell(\theta)$ into two sub-tasks. <strong>Note that in both $\log p(Z|\theta)$ and $\log p(Y|Z, \theta)$, the only unknown parameter is $\theta$.</strong> They are just two standard <em>MLE</em> problems which could be easily solved by methods such as gradient descent.</p>
<p><strong>Incomplete case</strong></p>
<p>$(Y)$ is observable, but $(Z)$ is unknown. We need to introduce the marginal distribution of variable $Z$:</p>
<p>$$
\begin{aligned}
\ell(\theta) &= \log p(Y | \theta) \\ &= \log \sum_Z p(Y, Z | \theta) \\ &= \log \sum_Z p(Z|\theta) \cdot p(Y|Z, \theta) \
\end{aligned}
$$</p>
<p>Here we have a summation inside the log, so it&rsquo;s hard to use optimization methods or take derivatives. This is the case where EM algorithm comes into aid.</p>
<h2 id=em-algorithm-derivation-using-mle>EM Algorithm Derivation (Using <em>MLE</em>)<a hidden class=anchor aria-hidden=true href=#em-algorithm-derivation-using-mle>#</a></h2>
<p>Given the observed data $Y$, we want to maximize the likelihood $\ell(\theta) = p(Y|\theta)$, and it&rsquo;s the same as maximizing the log-likelihood $\log p(Y|\theta)$. Therefore, from now on we will try to maximize the likelihood</p>
<p>$$\ell(\theta) = \log p(Y|\theta)$$</p>
<p>by taking the unknown variable $Z$ into account, we rewrite the objective function as</p>
<p>$$
\begin{aligned}
\ell(\theta) &= \log p(Y|\theta) \\ &= \log \sum_Z p(Y, Z | \theta) \\ &= \log \sum_Z p(Y|Z,\theta) \cdot p(Z|\theta) \
\end{aligned}
$$</p>
<p>Note that in the last step, the $\log$ is outside of the $\sum$, which is hard to compute and optimize. Check out my <a href=https://blog.csdn.net/Jay_Tang/article/details/105577295>previous post</a> to know why we prefer to have $\log$ inside of $\sum$, instead of outside. So later we would find a way to approximate it (Jensen’s inequality).</p>
<p>Suppose we follow the iteration step 2 <em>(E)</em> and 3 <em>(M)</em> repeatedly, and have updated parameters to $\theta_i$, then the difference between $\ell(\theta)$ and our estimate $\ell(\theta_i)$ is $\ell(\theta) - \ell(\theta_i)$. You can think of this difference as the improvement that later estimate of $\theta$ tries to achieve. So our next step is to find $\theta_{i+1}$ such that it improves the difference the most. That is, to make the difference $\ell(\theta) - \ell(\theta_i)$ as large as possible. So we want our next estimate $\theta_{i+1}$ to be</p>
<p>$$\theta_{i+1} = \mathop{\rm arg,max}\limits_{\theta} ,, \ell(\theta) - \ell(\theta_i)$$</p>
<p>Note that we know the value of $\theta_i$, so as $\ell(\theta_i)$. And</p>
<p>$$
\begin{aligned}
\ell(\theta) - \ell(\theta_i)&= \log p(Y|\theta) \\ &= \log \sum_Z p(Y|Z,\theta) \cdot p(Z|\theta) - \log p(Y|\theta_i) \\ &= \log \sum_Z P(Z|Y,\theta_i) \frac{p(Y|Z,\theta) \cdot p(Z|\theta)}{P(Z|Y,\theta_i)} - \log p(Y|\theta_i) & & &(1)\
\end{aligned}
$$</p>
<p>Since $P(Z|Y,\theta_i) \geq 0$ for all $z\in Z$ and $\sum_Z P(Z|Y,\theta_i) = 1$, we can use the Jensen’s inequality and then re-write $(1)$ as</p>
<p>$$
\begin{aligned}
\ell(\theta) - \ell(\theta_i)
&\geq \sum_Z P(Z|Y,\theta_i) \log \frac{p(Y|Z,\theta) \cdot p(Z|\theta)}{P(Z|Y,\theta_i)} - \sum_Z P(Z|Y,\theta_i)) \cdot \log p(Y|\theta_i)\\ &= \sum_Z P(Z|Y,\theta_i) \left( \log \frac{p(Y|Z,\theta) \cdot p(Z|\theta)}{P(Z|Y,\theta_i)} - \log p(Y|\theta_i) \right) \\ \ell(\theta) &\geq \ell(\theta_i) + \sum_Z P(Z|Y,\theta_i) \log \frac{p(Y|Z,\theta) \cdot p(Z|\theta)}{P(Z|Y,\theta_i) \cdot p(Y|\theta_i)}
\end{aligned}
$$</p>
<p>Now we define</p>
<p>$$ B(\theta, \theta_i) \triangleq \ell(\theta_i) + \sum_Z P(Z|Y,\theta_i) \log \frac{p(Y|Z,\theta) \cdot p(Z|\theta)}{P(Z|Y,\theta_i) \cdot p(Y|\theta_i)} $$</p>
<p>So we see that</p>
<p>$$ \ell(\theta) \geq B(\theta, \theta_i)$$</p>
<p>which implies that $B(\theta, \theta_i)$ is a lower bound of $\ell(\theta)$ for all $i$. Therefore our next step is to maximize the lower bound $B(\theta, \theta_i)$ and make it as tight as possible. In the $M$ step, we define</p>
<p>$$
\begin{aligned}
\theta_{i+1} &= \mathop{\rm arg,max}\limits_{\theta} B(\theta, \theta_i)\\ &= \mathop{\rm arg,max}\limits_{\theta} , \left(\ell(\theta_i) + \sum_Z P(Z|Y,\theta_i) \cdot \log \frac{p(Y|Z,\theta) \cdot p(Z|\theta)}{P(Z|Y,\theta_i) \cdot p(Y|\theta_i)} \right) && (2)\\ &= \mathop{\rm arg,max}\limits_{\theta} , \left(\sum_Z P(Z|Y,\theta_i) \cdot \log \frac{p(Y|Z,\theta) \cdot p(Z|\theta)}{P(Z|Y,\theta_i) \cdot p(Y|\theta_i)} \right) && (3)\\ &= \mathop{\rm arg,max}\limits_{\theta} , \left(\sum_Z P(Z|Y,\theta_i) \cdot [\log p(Y|Z,\theta) \cdot p(Z|\theta) - \log P(Z|Y,\theta_i) \cdot p(Y|\theta_i)] \right) && (4)\\ &= \mathop{\rm arg,max}\limits_{\theta} , \left(\sum_Z P(Z|Y,\theta_i) \cdot \log p(Y|Z,\theta) \cdot p(Z|\theta) \right) && (5)\\ &= \mathop{\rm arg,max}\limits_{\theta} , \left(\sum_Z P(Z|Y,\theta_i) \cdot \log p(Y,Z|\theta) \right) &&(6)\\ &= \mathop{\rm arg,max}\limits_{\theta} , Q(\theta, \theta_i) &&(7)\
\end{aligned}
$$</p>
<p><em>Remark:</em></p>
<ul>
<li>
<p>$(2) \to (3)$ since $\ell(\theta_i)$ does not contain $\theta$.</p>
</li>
<li>
<p>$(3) \to (4)$ we used $\log \frac{A}{B} = \log A - \log B$.</p>
</li>
<li>
<p>$(4) \to (5)$ since $\log P(Z|Y,\theta_i) \cdot p(Y|\theta_i)$ does not contain $\theta$.</p>
</li>
<li>
<p>$(6) \to (7)$ we define $Q(\theta, \theta_i) = \sum_Z P(Z|Y,\theta_i) \cdot \log p(Y,Z|\theta)$.</p>
</li>
<li>
<p><strong>since both $Y$ and $\theta_i$ are known, we have the distribution of $Z \sim p(Z|Y,\theta_i)$. Therefore, the only unknown parameter is $\theta$, which means this is now a complete case I mentioned early in the post. So this is now a <em>MLE</em> problem.</strong></p>
</li>
</ul>
<h3 id=summary>summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h3>
<ol>
<li>
<p><strong>$\theta_{i+1} = \mathop{\rm arg,max}\limits_{\theta} \ell(\theta) - \ell(\theta_i) = \mathop{\rm arg,max}\limits_{\theta} , Q(\theta, \theta_i)$. This implies that maximizing $\ell(\theta) - \ell(\theta_i)$ is the same as maximizing $Q(\theta, \theta_i)$.</strong></p>
</li>
<li>
<p><strong>Note that $Q(\theta, \theta_i) = \sum_Z P(Z|Y,\theta_i) \cdot \log p(Y,Z|\theta)$ is just the expectation of $\log p(Y,Z|\theta)$, where $Z$ is drawn from the current conditional distribution $P(Z|Y,\theta_i)$. Therefore we have</strong>
$$ Q(\theta, \theta_i) = E_{Z \sim p(Z|Y,\theta_i)}[\log p(Y,Z|\theta)]$$
<strong>That&rsquo;s why it&rsquo;s called the <em>Expectation step</em>. In <em>E</em> step, we are actually trying to calculate the expectation of the term $\log p(Y,Z|\theta)$, where unknown variable $Z$ follows the current conditional distribution given by $Y$ and $\theta_i$. Then in the <em>Maximazation step</em>, we are trying to maximize this expectation. That&rsquo;s why it&rsquo;s called the <em>M</em> step.</strong></p>
</li>
</ol>
<h3 id=coordinate-ascent-descent---view-em-from-a-different-prospect>Coordinate Ascent/ descent - view EM from a different prospect<a hidden class=anchor aria-hidden=true href=#coordinate-ascent-descent---view-em-from-a-different-prospect>#</a></h3>
<img src=https://img-blog.csdnimg.cn/20200424061745818.jpg width=350>
<p>In the Expectation Step, we actually fixed $\theta_i$, and tried to optimize $Q(\theta, \theta_i)$.</p>
<p>In the Maximization step, we actually fixed $Q(\theta, \theta_i)$, and tried to optimize $\theta$ to get $\theta_{i+1}$.</p>
<p>Every time we only optimize one variable and fix the rest. Therefore, from the graph above we see that in every iteration the gradient changes either vertically or horizontally.</p>
<p>To see how to perform the coordinate descent, check out my <a href=https://blog.csdn.net/Jay_Tang/article/details/105138425>previous post</a>.</p>
<h2 id=convergence-of-em-algorithm>Convergence of EM algorithm<a hidden class=anchor aria-hidden=true href=#convergence-of-em-algorithm>#</a></h2>
<p>By following the algorithm, we keep updating parameter $\theta_i$ and calculating approximated log-likelihood $\ell (\theta_i)$. But do we actually keep $\ell (\theta_i)$ getting closer to $l(\theta)$ as we do more iterations? Keep in mind that in <em>MLE</em> our final goal is to maximize $\ell (\theta)$.</p>
<p>Suppose $\theta_i$ and $\theta_{i+1}$ are the parameters from two successive iterations of <em>EM</em>. We will now prove that $\ell(\theta_i) \leq \ell(\theta_{i+1})$, which shows <em>EM</em> always monotonically improves the log-likelihood.</p>
<p>$$
\begin{aligned}
\ell(\theta) &= \log p(Y|\theta) \\ &= \log \frac {p(Y,Z|\theta)}{p(Z|Y, \theta)} \\ &= \log p(Y,Z|\theta) - \log p(Z|Y, \theta) \\ &= \sum_Z p(Z|Y, \theta_i)\cdot \log p(Y,Z|\theta) - \sum_Z p(Z|Y, \theta_i)\cdot \log p(Z|Y, \theta) \\ &= Q(\theta, \theta_i) + H(\theta, \theta_i) && (8)\
\end{aligned}
$$</p>
<p>where $H(\theta, \theta_i) = - \sum_Z p(Z|Y, \theta_i)\cdot \log p(Z|Y, \theta)$. This last equation $(8)$ holds for every value of $\theta$, including $\theta=\theta_i$, which means</p>
<p>$$ \ell(\theta_i) = Q(\theta_i, \theta_i) + H(\theta_i, \theta_i)$$</p>
<p>Therefore subtracting $\ell(\theta_i)$ from $\ell(\theta_{i+1})$ gives</p>
<p>$$
\begin{aligned}
\ell(\theta_{i+1}) - \ell(\theta_i) &= [Q(\theta_{i+1}, \theta_{i}) + H(\theta_{i+1}, \theta_{i})] - [Q(\theta_i, \theta_i) + H(\theta_i, \theta_i)]\\ &= [Q(\theta_{i+1}, \theta_{i}) - Q(\theta_i, \theta_{i})] + [H(\theta_{i+1}, \theta_{i}) - H(\theta_i, \theta_{i})] \
\end{aligned}
$$</p>
<p>Since $\theta_{i+1} = \mathop{\rm arg,max}\limits_{\theta} , Q(\theta, \theta_i)$, we have $Q(\theta_{i+1}, \theta_i) \geq Q(\theta_i, \theta_i)$. The second parenthesis gives</p>
<p>$$
\begin{aligned}
H(\theta_{i+1}, \theta_{i}) - H(\theta_i, \theta_{i})
&= - \sum_Z p(Z|Y, \theta_i)\cdot \log p(Z|Y, \theta_{i+1}) + \sum_Z p(Z|Y, \theta_i)\cdot \log p(Z|Y, \theta_i) \\ &= - \left( \sum_Z p(Z|Y, \theta_i)\cdot \log p(Z|Y, \theta_{i+1}) - \sum_Z p(Z|Y, \theta_i)\cdot \log p(Z|Y, \theta_i)\right) \\ &= - \left( \sum_Z p(Z|Y, \theta_i) \cdot \log \frac{p(Z|Y, \theta_{i+1})}{p(Z|Y, \theta_{i})} \right) && \ \ \ (9)\\ &\geq - \log \left( \sum_Z p(Z|Y, \theta_i) \cdot \frac{p(Z|Y, \theta_{i+1})}{p(Z|Y, \theta_{i})} \right) && (10)\\ &= - \log \sum_Z p(Z|Y, \theta_{i+1}) = - \log 1 = 0
\end{aligned}
$$</p>
<p>From $(9)$ to $(10)$ we use the Jensen’s inequality (note that there is a negative sign in the front so we reverse the inequality).</p>
<p>Since $Q(\theta_{i+1}, \theta_{i}) \geq Q(\theta_i, \theta_{i})$ and $H(\theta_{i+1}, \theta_{i}) \geq H(\theta_i, \theta_{i})$, we have</p>
<p>$$ \ell(\theta_{i+1}) \geq \ell(\theta_i) \qquad\qquad \text{for all} , i$$</p>
<p>Since it&rsquo;s monotonically increasing and bounded above, we say $\ell (\theta_i)$ converges.</p>
<h2 id=whats-next>What&rsquo;s next?<a hidden class=anchor aria-hidden=true href=#whats-next>#</a></h2>
<p>I&rsquo;m going to write a post to discuss how EM algorithm is applied in <a href=https://liyantang.blog.csdn.net/article/details/106153760>K-means and GMM</a> in the future. Stay tuned!</p>
<hr>
<p>Reference:</p>
<ul>
<li><a href=https://en.wikipedia.org/wiki/Expectation>https://en.wikipedia.org/wiki/Expectation</a>–maximization_algorithm</li>
<li>Part IX: The EM algorithm from CS229 Lecture notes by Andrew Ng <a href=http://cs229.stanford.edu/notes/cs229-notes8.pdf>http://cs229.stanford.edu/notes/cs229-notes8.pdf</a></li>
<li><a href=http://guillefix.me/cosmos/static/Jensen%27s%2520inequality.html>http://guillefix.me/cosmos/static/Jensen%27s%2520inequality.html</a></li>
<li><a href=https://en.wikipedia.org/wiki/Jensen%27s_inequality>https://en.wikipedia.org/wiki/Jensen%27s_inequality</a></li>
<li><a href=http://www.adeveloperdiary.com/data-science/machine-learning/introduction-to-coordinate-descent-using-least-squares-regression/>http://www.adeveloperdiary.com/data-science/machine-learning/introduction-to-coordinate-descent-using-least-squares-regression/</a></li>
</ul>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=https://tangliyan.com/blog/tags/ml/>ML</a></li>
<li><a href=https://tangliyan.com/blog/tags/math/>MATH</a></li>
</ul>
<nav class=paginav>
<a class=prev href=https://tangliyan.com/blog/posts/hmm/>
<span class=title>« Prev Page</span>
<br>
<span>Hidden Markov Model (HMM)</span>
</a>
<a class=next href=https://tangliyan.com/blog/posts/skipgram/>
<span class=title>Next Page »</span>
<br>
<span>Skip-gram</span>
</a>
</nav>
<div class=share-buttons>
<a target=_blank rel="noopener noreferrer" aria-label="share EM (Expectation–Maximization) Algorithm on twitter" href="https://twitter.com/intent/tweet/?text=EM%20%28Expectation%e2%80%93Maximization%29%20Algorithm&url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fem%2f&hashtags=ML%2cMATH"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share EM (Expectation–Maximization) Algorithm on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fem%2f&title=EM%20%28Expectation%e2%80%93Maximization%29%20Algorithm&summary=EM%20%28Expectation%e2%80%93Maximization%29%20Algorithm&source=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fem%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share EM (Expectation–Maximization) Algorithm on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fem%2f&title=EM%20%28Expectation%e2%80%93Maximization%29%20Algorithm"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share EM (Expectation–Maximization) Algorithm on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fem%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share EM (Expectation–Maximization) Algorithm on whatsapp" href="https://api.whatsapp.com/send?text=EM%20%28Expectation%e2%80%93Maximization%29%20Algorithm%20-%20https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fem%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share EM (Expectation–Maximization) Algorithm on telegram" href="https://telegram.me/share/url?text=EM%20%28Expectation%e2%80%93Maximization%29%20Algorithm&url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fem%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a>
</div>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2022 <a href=https://tangliyan.com/blog/>Liyan Tang</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
<script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerText='copy';function d(){a.innerText='copied!',setTimeout(()=>{a.innerText='copy'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script>
</body>
</html>