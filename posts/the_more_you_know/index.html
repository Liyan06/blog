<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Paper Review - The More You Know: Using Knowledge Graphs for Image Classification | Liyan Tang</title>
<meta name=keywords content="PAPER,KG,CV">
<meta name=description content="Authors: Kenneth Marino, Ruslan Salakhutdinov, Abhinav Gupta">
<meta name=author content>
<link rel=canonical href=https://tangliyan.com/blog/posts/the_more_you_know/>
<link crossorigin=anonymous href=/blog/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css integrity="sha256-yIlj/i15RiAA/Q+xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/blog/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://tangliyan.com/blog/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://tangliyan.com/blog/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://tangliyan.com/blog/favicon-32x32.png>
<link rel=apple-touch-icon href=https://tangliyan.com/blog/apple-touch-icon.png>
<link rel=mask-icon href=https://tangliyan.com/blog/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.92.0">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-202974782-1','auto'),ga('send','pageview'))</script><meta property="og:title" content="Paper Review - The More You Know: Using Knowledge Graphs for Image Classification">
<meta property="og:description" content="Authors: Kenneth Marino, Ruslan Salakhutdinov, Abhinav Gupta">
<meta property="og:type" content="article">
<meta property="og:url" content="https://tangliyan.com/blog/posts/the_more_you_know/"><meta property="og:image" content="https://tangliyan.com/blog/papermod-cover.png"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2020-09-02T00:00:00+00:00">
<meta property="article:modified_time" content="2020-09-02T00:00:00+00:00">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://tangliyan.com/blog/papermod-cover.png">
<meta name=twitter:title content="Paper Review - The More You Know: Using Knowledge Graphs for Image Classification">
<meta name=twitter:description content="Authors: Kenneth Marino, Ruslan Salakhutdinov, Abhinav Gupta">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://tangliyan.com/blog/posts/"},{"@type":"ListItem","position":2,"name":"Paper Review - The More You Know: Using Knowledge Graphs for Image Classification","item":"https://tangliyan.com/blog/posts/the_more_you_know/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Paper Review - The More You Know: Using Knowledge Graphs for Image Classification","name":"Paper Review - The More You Know: Using Knowledge Graphs for Image Classification","description":"Authors: Kenneth Marino, Ruslan Salakhutdinov, Abhinav Gupta","keywords":["PAPER","KG","CV"],"articleBody":"Authors: Kenneth Marino, Ruslan Salakhutdinov, Abhinav Gupta Paper reference: https://arxiv.org/pdf/1612.04844.pdf\nOverview Note: This previous post I wrote might be helpful for reading this paper summary:\n Introduction to Graph Neural Network (GNN)  This paper investigates the use of structured prior knowledge in the form of knowledge graphs and shows that using this knowledge improves performance on image classification.\nIt introduce the Graph Search Neural Network (GSNN) as a way of efficiently incorporating large knowledge graphs into a vision classification pipeline, which outperforms standard neural network baselines for multi-label classification.\nIntuition While modern learning-based approaches can recognize some categories with high accuracy, it usually requires thousands of labeled examples for each of these categories. This approach of building large datasets for every concept is unscalable. One way to solve this problem is to use structured knowledge and reasoning (prior knowledge, this is what human usually do but current approaches do not).\nFor example, when people try to identify the animal shown in the figure, they will first recognize the animal, then recall relevant knowledge, and finally reason about it. With this information, even if we have only seen one or two pictures of this animal, we would be able to classify it. So we hope a model could also have similar reasoning process.\nPrevious Work There has been a lot of work in end-to-end learning on graphs or neural network trained on graphs. Most of these approaches either extract features from the graph or they learn a propagation model that transfers evidence between nodes conditional on the type of edge. An example of this is the Gated Graph Neural Network which takes an arbitrary graph as input. Given some initialization specific to the task, it learns how to propagate information and predict the output for every node in the graph.\nPrevious works are focusing on building and then querying knowledge bases rather than using existing knowledge bases as side information for some vision task.\nThis work not only uses attribute relationships that appear in our knowledge graphs, but also uses relationships between objects and reasons directly on graphs rather than using object-attribute pairs directly.\nMajor Contribution   The introduction of the GSNN as a way of incorporating potentially large knowledge graphs into an end-to-end learning system that is computationally feasible for large graphs;\n  Provide a framework for using noisy knowledge graphs for image classification (In vision problems, graphs encode contextual and common-sense relationships and are significantly larger and noisier);\n  The ability to explain image classifications by using the propagation model (Interpretability).\n  Graph Search Neural Network (GSNN) GSNN Explanation The idea is that rather than performing the recurrent update over all of the nodes of the graph at once, it starts with some initial nodes based on the input and only choose to expand nodes which are useful for the final output. Thus, the model only compute the update steps over a subset of the graph.\nSteps in GSNN:\n Determine Initial Nodes  They determine initial nodes in the graph based on likelihood of the visual concept being present as determined by an object detector or classifier. For their experiments, they use Faster R-CNN for each of the 80 COCO categories. For scores over some chosen threshold, they choose the corresponding nodes in the graph as our initial set of active nodes. Once they have initial nodes, they also add the nodes adjacent to the initial nodes to the active set.\n Propagation  Given the initial nodes, they want to first propagate the beliefs about the initial nodes to all of the adjacent nodes (propagation network). This process is similar to GGNN.\n Decide which nodes to expand next  After the first time step, they need a way of deciding which nodes to expand next. Therefore, a per-node scoring function is learned to estimates how “important” that node is. After each propagation step, for every node in the current graph, the model predict an importance score\n$$ i_{v}^{(t)}=g_{i}\\left(h_{v}, x_{v}\\right) $$\nwhere $g_{i}$ is a learned network, the importance network. Once we have values of $i_{v}$, we take the top $P$ scoring nodes that have never been expanded and add them to the expanded set, and add all nodes adjacent to those nodes to our active set.\nThe above two steps (Propagation, Decide which nodes to expand next) repeated $T$ times ($T$ is a hyper-parameter).\nLastly, at the final time step $T$, the model computes the per-node-output (output network) and re-order and zero-pad the outputs into the final classification net. Re-order it so that nodes always appear in the same order into the final network, and zero pad any nodes that were not expanded.\nThe entire process is shown in the figure above.\nThree networks   Propagation network: normal Graph Gated Neural Network (GGNN). GGNN is a fully end-to-end network that takes as input a directed graph and outputs either a classification over the entire graph or an output for each node. Check my previous post to know more details about GGNN and how propagation works.\n  Output network: After $T$ time steps, we have our final hidden states. The node level outputs can then just be computed as $$ o_{v}=g\\left(h_{v}^{(T)}, x_{v}\\right) $$ where $g$ is a fully connected network, the output network, and $x_{v}$ is the original annotation for the node.\n  Importance network: learn a per-node scoring function that estimates how “important” that node is. To train the importance net, they assign target importance value to each node in the graph for a given image. Nodes corresponding to ground-truth concepts in an image are assigned an importance value of 1. The neighbors of these nodes are assigned a value of $\\gamma$. Nodes which are two-hop away have value $\\gamma^2$ and so on. The idea is that nodes closest to the final output are the most important to expand. After each propagation step, for each node in the current graph, they predict an importance score $$ i_{v}^{(t)}=g_{i}\\left(h_{v}, x_{v}\\right) $$\n  Diagram visualization First $x_{init}$, the detection confidences initialize $h_{i n i t}^{(1)}$, the hidden states of the initially detected nodes (Each visual concept (e.x., person, horse, cat, etc) in the knowledge graph is represented in a hidden state). We then initialize $h_{a d j 1}^{(1)}$, the hidden states of the adjacent nodes, with $0$. We then update the hidden states using the propagation net. The values of $h^{(2)}$ are then used to predict the importance scores $i^{(1)}$ which are used to pick the next nodes to add $adj2$. These nodes are then initialized with $h_{a d j 2}^{(2)}=0$ and the hidden states are updated again through the propagation net. After $T$ steps, we then take all of the accumulated hidden states $h^{T}$ to predict the GSNN outputs for all the active nodes. During backpropagation, the binary cross entropy (BCE) loss is fed backward through the output layer, and the importance losses are fed through the importance networks to update the network parameters.\nOne final detail is the addition of a “node bias” into GSNN. In GGNN, the per-node output function $g\\left(h_{v}^{(T)}, x_{v}\\right)$, takes in the hidden state and initial annotation of the node $v$ to compute its output. Our output equations are now $g\\left(h_{v}^{(T)}, x_{v}, n_{v}\\right)$ where $n_{v}$ is a bias term that is tied to a particular node $v$ in the overall graph. This value is stored in a table and its value are updated by backpropagation.\nAdvantage   This new architecture mitigates the computational issues with the Gated Graph Neural Networks for large graphs which allows our model to be efficiently trained for image tasks using large knowledge graphs.\n  Importantly, the GSNN model is also able to provide explanations on classifications by following how the information is propagated in the graph.\n  Incorporate the graph network into an image pipeline We take the output of the graph network, re-order it so that nodes always appear in the same order into the final network, and zero pad any nodes that were not expanded. Therefore, if we have a graph with 316 node outputs, and each node predicts a 5-dim hidden variable, we create a 1580-dim feature vector from the graph. We also concatenate this feature vector with fc7 layer (4096-dim) of a fine-tuned VGG-16 network and top-score for each COCO category predicted by Faster R-CNN (80-dim). This 5756-dim feature vector is then fed into 1-layer final classification network trained with dropout.\nYou can think of this as a way of feature engineering where you concatenate the output from models with different structures.\nDataset COCO: COCO is a large-scale object detection, segmentation, and captioning dataset, which includes 80 object categories.\nVisual Genome: a dataset that represents the complex, noisy visual world with its many different kinds of objects, where labels are potentially ambiguous and overlapping, and categories fall into a long-tail distribution (skew to the right).\nVisual Genome contains over 100,000 natural images from the Internet. Each image is labeled with objects, attributes and relationships between objects entered by human annotators. They create a subset from Visual Genome which they call Visual Genome multi-label dataset or VGML (They took 316 visual concepts from the subset).\nUsing only the train split, we build a knowledge graph connecting the concepts using the most common object-attribute and object-object relationships in the dataset. Specifically, we counted how often an object/object relationship or object/attribute pair occurred in the training set, and pruned any edges that had fewer than 200 instances. This leaves us with a graph over all of the images with each edge being a common relationship.\nVisual Genome + WordNet: including the outside semantic knowledge from WordNet. The Visual Genome graphs do not contain useful semantic relationships. For instance, it might be helpful to know that dog is an animal if our visual system sees a dog and one of our labels is animal. Check the original paper to know how to add WordNet into the graph.\nConclusion In this paper, they present the Graph Search Neural Network (GSNN) as a way of efficiently using knowledge graphs as extra information to improve image classification.\nNext Step:\nThe GSNN and the framework they use for vision problems is completely general. The next steps will be to apply the GSNN to other vision tasks, such as detection, Visual Question Answering, and image captioning.\nAnother interesting direction would be to combine the procedure of this work with a system such as NEIL to create a system which builds knowledge graphs and then prunes them to get a more accurate, useful graph for image tasks.\n Reference:\n Visual Genome: https://visualgenome.org The More You Know: Using Knowledge Graphs for Image Classification: https://arxiv.org/abs/1612.04844  ","wordCount":"1750","inLanguage":"en","datePublished":"2020-09-02T00:00:00Z","dateModified":"2020-09-02T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://tangliyan.com/blog/posts/the_more_you_know/"},"publisher":{"@type":"Organization","name":"Liyan Tang","logo":{"@type":"ImageObject","url":"https://tangliyan.com/blog/favicon.ico"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://tangliyan.com/blog/ accesskey=h title="Liyan Tang (Alt + H)">Liyan Tang</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
<li>
<a href=https://tangliyan.com/blog/archives title=Archive>
<span>Archive</span>
</a>
</li>
<li>
<a href=https://tangliyan.com/blog/tags/ title=Tags>
<span>Tags</span>
</a>
</li>
<li>
<a href=https://tangliyan.com/blog/search/ title="Search (Alt + /)" accesskey=/>
<span>Search</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<div class=breadcrumbs><a href=https://tangliyan.com/blog/>Home</a>&nbsp;»&nbsp;<a href=https://tangliyan.com/blog/posts/>Posts</a></div>
<h1 class=post-title>
Paper Review - The More You Know: Using Knowledge Graphs for Image Classification
</h1>
<div class=post-meta><span title="2020-09-02 00:00:00 +0000 UTC">September 2, 2020</span>&nbsp;·&nbsp;9 min
</div>
</header> <div class=toc>
<details>
<summary accesskey=c title="(Alt + C)">
<span class=details>Table of Contents</span>
</summary>
<div class=inner><ul>
<li>
<a href=#overview aria-label=Overview>Overview</a></li>
<li>
<a href=#intuition aria-label=Intuition>Intuition</a></li>
<li>
<a href=#previous-work aria-label="Previous Work">Previous Work</a></li>
<li>
<a href=#major-contribution aria-label="Major Contribution">Major Contribution</a></li>
<li>
<a href=#graph-search-neural-network-gsnn aria-label="Graph Search Neural Network (GSNN)">Graph Search Neural Network (GSNN)</a><ul>
<li>
<a href=#gsnn-explanation aria-label="GSNN Explanation">GSNN Explanation</a></li>
<li>
<a href=#three-networks aria-label="Three networks">Three networks</a></li>
<li>
<a href=#diagram-visualization aria-label="Diagram visualization">Diagram visualization</a></li>
<li>
<a href=#advantage aria-label=Advantage>Advantage</a></li></ul>
</li>
<li>
<a href=#incorporate-the-graph-network-into-an-image-pipeline aria-label="Incorporate the graph network into an image pipeline">Incorporate the graph network into an image pipeline</a></li>
<li>
<a href=#dataset aria-label=Dataset>Dataset</a></li>
<li>
<a href=#conclusion aria-label=Conclusion>Conclusion</a>
</li>
</ul>
</div>
</details>
</div>
<div class=post-content><p>Authors: Kenneth Marino, Ruslan Salakhutdinov, Abhinav Gupta <br>
Paper reference: <a href=https://arxiv.org/pdf/1612.04844.pdf>https://arxiv.org/pdf/1612.04844.pdf</a></p>
<h2 id=overview>Overview<a hidden class=anchor aria-hidden=true href=#overview>#</a></h2>
<p>Note: This previous post I wrote might be helpful for reading this paper summary:</p>
<ul>
<li><a href=./GNN.md>Introduction to Graph Neural Network (GNN)</a></li>
</ul>
<p>This paper investigates <strong>the use of structured prior knowledge in the form of knowledge graphs</strong> and shows that using this knowledge improves performance on image classification.</p>
<p>It introduce the <em>Graph Search Neural Network (GSNN)</em> as a way of efficiently incorporating large knowledge graphs into a vision classification pipeline, which outperforms standard neural network baselines for multi-label classification.</p>
<h2 id=intuition>Intuition<a hidden class=anchor aria-hidden=true href=#intuition>#</a></h2>
<p>While modern learning-based approaches can recognize some categories with high accuracy, it usually requires thousands of labeled examples for each of these categories. This approach of building large datasets for every concept is unscalable. One way to solve this problem is to use structured knowledge and reasoning (prior knowledge, this is what human usually do but current approaches do not).</p>
<img src="https://img-blog.csdnimg.cn/20200901235037534.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_1,color_FFFFFF,t_70#pic_center" width=550>
<p>For example, when people try to identify the animal shown in the figure, they will first <em>recognize</em> the animal, then <em>recall relevant knowledge</em>, and finally <em>reason</em> about it. With this information, even if we have only seen one or two pictures of this animal, we would be able to classify it. So we hope a model could also have similar reasoning process.</p>
<h2 id=previous-work>Previous Work<a hidden class=anchor aria-hidden=true href=#previous-work>#</a></h2>
<p>There has been a lot of work in end-to-end learning on graphs or neural network trained on graphs. Most of these approaches either extract features from the graph or they learn a propagation model that transfers evidence between nodes conditional on the type of edge. An example of this is the Gated Graph Neural Network which takes an arbitrary graph as input. Given some initialization specific to the task, it learns how to propagate information and predict the output for every node in the graph.</p>
<img src="https://img-blog.csdnimg.cn/20200901235137987.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_1,color_FFFFFF,t_70#pic_center" width=550>
<p>Previous works are focusing on building and then querying knowledge bases <strong>rather than using existing knowledge bases as side information</strong> for some vision task.</p>
<p><strong>This work not only uses attribute relationships that appear in our knowledge graphs, but also uses relationships between objects and reasons directly on graphs rather than using object-attribute pairs directly.</strong></p>
<h2 id=major-contribution>Major Contribution<a hidden class=anchor aria-hidden=true href=#major-contribution>#</a></h2>
<ol>
<li>
<p>The introduction of the GSNN as a way of incorporating potentially large knowledge graphs into an end-to-end learning system that is computationally feasible for large graphs;</p>
</li>
<li>
<p>Provide a framework for using noisy knowledge graphs for image classification (In vision problems, graphs encode contextual and common-sense relationships and are significantly larger and noisier);</p>
</li>
<li>
<p>The ability to explain image classifications by using the propagation model (Interpretability).</p>
</li>
</ol>
<h2 id=graph-search-neural-network-gsnn>Graph Search Neural Network (GSNN)<a hidden class=anchor aria-hidden=true href=#graph-search-neural-network-gsnn>#</a></h2>
<h3 id=gsnn-explanation>GSNN Explanation<a hidden class=anchor aria-hidden=true href=#gsnn-explanation>#</a></h3>
<p>The idea is that rather than performing the recurrent update over all of the nodes of the graph at once, it starts with some initial nodes based on the input and only choose to expand nodes which are useful for the final output. Thus, the model only compute the update steps over a subset of the graph.</p>
<img src="https://img-blog.csdnimg.cn/20200901235256289.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_1,color_FFFFFF,t_70#pic_center" width=770>
<p><strong>Steps in GSNN:</strong></p>
<ul>
<li>Determine Initial Nodes</li>
</ul>
<p>They determine initial nodes in the graph based on likelihood of the visual concept being present as determined by an object detector or classifier. For their experiments, they use Faster R-CNN for each of the 80 COCO categories. For scores over some chosen threshold, they choose the corresponding nodes in the graph as our initial set of active nodes. Once they have initial nodes, they also add the nodes adjacent to the initial nodes to the active set.</p>
<ul>
<li><em>Propagation</em></li>
</ul>
<p>Given the initial nodes, they want to first propagate the beliefs about the initial nodes to all of the adjacent nodes (<strong>propagation network</strong>). This process is similar to GGNN.</p>
<ul>
<li><em>Decide which nodes to expand next</em></li>
</ul>
<p>After the first time step, they need a way of deciding which nodes to expand next. Therefore, a per-node scoring function is learned to estimates how &ldquo;important&rdquo; that node is. After each propagation step, for every node in the current graph, the model predict an importance score</p>
<p>$$
i_{v}^{(t)}=g_{i}\left(h_{v}, x_{v}\right)
$$</p>
<p>where $g_{i}$ is a learned network, the <strong>importance network</strong>. Once we have values of $i_{v}$, we take the top $P$ scoring nodes that have never been expanded and add them to the expanded set, and add all nodes adjacent to those nodes to our active set.</p>
<p>The above two steps (Propagation, Decide which nodes to expand next) repeated $T$ times ($T$ is a hyper-parameter).</p>
<p>Lastly, at the final time step $T$, the model computes the per-node-output (<strong>output network</strong>) and re-order and zero-pad the outputs into the final classification net. <strong>Re-order it so that nodes always appear in the same order into the final network, and zero pad any nodes that were not expanded.</strong></p>
<p>The entire process is shown in the figure above.</p>
<h3 id=three-networks>Three networks<a hidden class=anchor aria-hidden=true href=#three-networks>#</a></h3>
<ul>
<li>
<p><strong>Propagation network</strong>: normal Graph Gated Neural Network (GGNN). GGNN is a fully end-to-end network that takes as input a directed graph and outputs either a classification over the entire graph or an output for each node. Check my previous <a href=https://liyantang.blog.csdn.net/article/details/108047975>post</a> to know more details about GGNN and how propagation works.</p>
</li>
<li>
<p><strong>Output network</strong>: After $T$ time steps, we have our final hidden states. The node level outputs can then just be computed as
$$
o_{v}=g\left(h_{v}^{(T)}, x_{v}\right)
$$
where $g$ is a fully connected network, the output network, and $x_{v}$ is the original annotation for the node.</p>
</li>
<li>
<p><strong>Importance network</strong>: learn a per-node scoring function that estimates how &ldquo;important&rdquo; that node is. To train the importance net, they <em>assign target importance value to each node in the graph for a given image</em>. Nodes corresponding to ground-truth concepts in an image are assigned an importance value of 1. The neighbors of these nodes are assigned a value of $\gamma$. Nodes which are two-hop away have value $\gamma^2$ and so on. The idea is that nodes closest to the final output are the most important to expand. After each propagation step, for each node in the current graph, they predict an importance score
$$
i_{v}^{(t)}=g_{i}\left(h_{v}, x_{v}\right)
$$</p>
</li>
</ul>
<h3 id=diagram-visualization>Diagram visualization<a hidden class=anchor aria-hidden=true href=#diagram-visualization>#</a></h3>
<img src="https://img-blog.csdnimg.cn/20200901235357743.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_16,color_FFFFFF,t_70#pic_center" width=600>
<p>First $x_{init}$, the detection confidences initialize $h_{i n i t}^{(1)}$, the hidden states of the initially detected nodes (Each visual concept (<em>e.x.</em>, person, horse, cat, <em>etc</em>) in the knowledge graph is represented in a hidden state). We then initialize $h_{a d j 1}^{(1)}$, the hidden states of the adjacent nodes, with $0$. We then update the hidden states using the propagation net. The values of $h^{(2)}$ are then used to predict the importance scores $i^{(1)}$ which are used to pick the next nodes to add $adj2$. These nodes are then initialized with $h_{a d j 2}^{(2)}=0$ and the hidden states are updated again through the propagation net. After $T$ steps, we then take all of the accumulated hidden states $h^{T}$ to predict the GSNN outputs for all the active nodes. During backpropagation, the binary cross entropy (BCE) loss is fed backward through the output layer, and the importance losses are fed through the importance networks to update the network parameters.</p>
<p>One final detail is the addition of a &ldquo;node bias&rdquo; into GSNN. In GGNN, the per-node output function $g\left(h_{v}^{(T)}, x_{v}\right)$, takes in the hidden state and initial annotation of the node $v$ to compute its output. Our output equations are now $g\left(h_{v}^{(T)}, x_{v}, n_{v}\right)$ where $n_{v}$ is a bias term that is tied to a particular node $v$ in the overall graph. This value is stored in a table and its value are updated by backpropagation.</p>
<h3 id=advantage>Advantage<a hidden class=anchor aria-hidden=true href=#advantage>#</a></h3>
<ul>
<li>
<p>This new architecture mitigates the computational issues with the Gated Graph Neural Networks for large graphs which allows our model to be efficiently trained for image tasks using large knowledge graphs.</p>
</li>
<li>
<p>Importantly, the GSNN model is also able to <strong>provide explanations on classifications</strong> by following how the information is propagated in the graph.</p>
</li>
</ul>
<h2 id=incorporate-the-graph-network-into-an-image-pipeline>Incorporate the graph network into an image pipeline<a hidden class=anchor aria-hidden=true href=#incorporate-the-graph-network-into-an-image-pipeline>#</a></h2>
<p>We take the output of the graph network, <strong>re-order it so that nodes always appear in the same order into the final network, and zero pad any nodes that were not expanded.</strong> Therefore, if we have a graph with 316 node outputs, and each node predicts a 5-dim hidden variable, we create a 1580-dim feature vector from the graph. We also concatenate this feature vector with fc7 layer (4096-dim) of a fine-tuned VGG-16 network and top-score for each COCO category predicted by Faster R-CNN (80-dim). This 5756-dim feature vector is then fed into 1-layer final classification network trained with dropout.</p>
<p>You can think of this as a way of feature engineering where you concatenate the output from models with different structures.</p>
<img src="https://img-blog.csdnimg.cn/20200901235445426.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_1,color_FFFFFF,t_70#pic_center" width=650>
<h2 id=dataset>Dataset<a hidden class=anchor aria-hidden=true href=#dataset>#</a></h2>
<p>COCO: COCO is a large-scale object detection, segmentation, and captioning dataset, which includes 80 object categories.</p>
<p>Visual Genome: a dataset that represents the complex, noisy visual world with its many different kinds of objects, where labels are potentially ambiguous and overlapping, and categories fall into a long-tail distribution (skew to the right).</p>
<img src="https://img-blog.csdnimg.cn/20200902060912766.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_1,color_FFFFFF,t_70#pic_center" width=650>
<p>Visual Genome contains over 100,000 natural images from the Internet. Each image is labeled with objects, attributes and relationships between objects entered by human annotators. They create a subset from Visual Genome which they call Visual Genome multi-label dataset or VGML (They took 316 visual concepts from the subset).</p>
<p><em>Using only the train split, we build a knowledge graph connecting the concepts using the most common object-attribute and object-object relationships in the dataset. Specifically, we counted how often an object/object relationship or object/attribute pair occurred in the training set, and pruned any edges that had fewer than 200 instances. This leaves us with a graph over all of the images with each edge being a common relationship.</em></p>
<p>Visual Genome + WordNet: including the <em>outside semantic knowledge</em> from WordNet. The Visual Genome graphs do not contain useful semantic relationships. For instance, it might be helpful to know that dog is an animal if our visual system sees a dog and one of our labels is animal. Check the original paper to know how to add WordNet into the graph.</p>
<img src="https://img-blog.csdnimg.cn/20200901235652262.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_1,color_FFFFFF,t_70#pic_center" width=600>
<h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2>
<p>In this paper, they present the Graph Search Neural Network (GSNN) as a way of efficiently using knowledge graphs as extra information to improve image classification.</p>
<p>Next Step:</p>
<p>The GSNN and the framework they use for vision problems is completely general. The next steps will be to apply the GSNN to other vision tasks, such as detection, Visual Question Answering, and image captioning.</p>
<p>Another interesting direction would be to combine the procedure of this work with a system such as NEIL <em>to create a system which builds knowledge graphs and then prunes them to get a more accurate, useful graph for image tasks</em>.</p>
<hr>
<p>Reference:</p>
<ul>
<li>Visual Genome: <a href=https://visualgenome.org>https://visualgenome.org</a></li>
<li>The More You Know: Using Knowledge Graphs for Image Classification: <a href=https://arxiv.org/abs/1612.04844>https://arxiv.org/abs/1612.04844</a></li>
</ul>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=https://tangliyan.com/blog/tags/paper/>PAPER</a></li>
<li><a href=https://tangliyan.com/blog/tags/kg/>KG</a></li>
<li><a href=https://tangliyan.com/blog/tags/cv/>CV</a></li>
</ul>
<nav class=paginav>
<a class=prev href=https://tangliyan.com/blog/posts/bert_attn/>
<span class=title>« Prev Page</span>
<br>
<span>Paper Review - What Does BERT Look At? An Analysis of BERT’s Attention</span>
</a>
<a class=next href=https://tangliyan.com/blog/posts/spectral_conv/>
<span class=title>Next Page »</span>
<br>
<span>Graph Convolutional Neural Network - Spectral Convolution</span>
</a>
</nav>
<div class=share-buttons>
<a target=_blank rel="noopener noreferrer" aria-label="share Paper Review - The More You Know: Using Knowledge Graphs for Image Classification on twitter" href="https://twitter.com/intent/tweet/?text=Paper%20Review%20-%20The%20More%20You%20Know%3a%20Using%20Knowledge%20Graphs%20for%20Image%20Classification&url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fthe_more_you_know%2f&hashtags=PAPER%2cKG%2cCV"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Paper Review - The More You Know: Using Knowledge Graphs for Image Classification on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fthe_more_you_know%2f&title=Paper%20Review%20-%20The%20More%20You%20Know%3a%20Using%20Knowledge%20Graphs%20for%20Image%20Classification&summary=Paper%20Review%20-%20The%20More%20You%20Know%3a%20Using%20Knowledge%20Graphs%20for%20Image%20Classification&source=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fthe_more_you_know%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Paper Review - The More You Know: Using Knowledge Graphs for Image Classification on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fthe_more_you_know%2f&title=Paper%20Review%20-%20The%20More%20You%20Know%3a%20Using%20Knowledge%20Graphs%20for%20Image%20Classification"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Paper Review - The More You Know: Using Knowledge Graphs for Image Classification on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fthe_more_you_know%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Paper Review - The More You Know: Using Knowledge Graphs for Image Classification on whatsapp" href="https://api.whatsapp.com/send?text=Paper%20Review%20-%20The%20More%20You%20Know%3a%20Using%20Knowledge%20Graphs%20for%20Image%20Classification%20-%20https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fthe_more_you_know%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Paper Review - The More You Know: Using Knowledge Graphs for Image Classification on telegram" href="https://telegram.me/share/url?text=Paper%20Review%20-%20The%20More%20You%20Know%3a%20Using%20Knowledge%20Graphs%20for%20Image%20Classification&url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fthe_more_you_know%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a>
</div>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2022 <a href=https://tangliyan.com/blog/>Liyan Tang</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
<script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerText='copy';function d(){a.innerText='copied!',setTimeout(()=>{a.innerText='copy'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script>
</body>
</html>