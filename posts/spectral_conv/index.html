<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Graph Convolutional Neural Network - Spectral Convolution | Liyan Tang</title>
<meta name=keywords content="GRAPH,MATH">
<meta name=description content="Fourier Transform Virtually everything in the world can be described via a waveform - a function of time, space or some other variable. For instance, sound waves, the price of a stock, etc. The Fourier Transform gives us a unique and powerful way of viewing these waveforms: All waveforms, no matter what you scribble or observe in the universe, are actually just the sum of simple sinusoids of different frequencies.">
<meta name=author content>
<link rel=canonical href=https://tangliyan.com/blog/posts/spectral_conv/>
<link crossorigin=anonymous href=/blog/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css integrity="sha256-yIlj/i15RiAA/Q+xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/blog/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://tangliyan.com/blog/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://tangliyan.com/blog/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://tangliyan.com/blog/favicon-32x32.png>
<link rel=apple-touch-icon href=https://tangliyan.com/blog/apple-touch-icon.png>
<link rel=mask-icon href=https://tangliyan.com/blog/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.92.0">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-202974782-1','auto'),ga('send','pageview'))</script><meta property="og:title" content="Graph Convolutional Neural Network -  Spectral Convolution">
<meta property="og:description" content="Fourier Transform Virtually everything in the world can be described via a waveform - a function of time, space or some other variable. For instance, sound waves, the price of a stock, etc. The Fourier Transform gives us a unique and powerful way of viewing these waveforms: All waveforms, no matter what you scribble or observe in the universe, are actually just the sum of simple sinusoids of different frequencies.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://tangliyan.com/blog/posts/spectral_conv/"><meta property="og:image" content="https://tangliyan.com/blog/papermod-cover.png"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2020-08-24T00:00:00+00:00">
<meta property="article:modified_time" content="2020-08-24T00:00:00+00:00">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://tangliyan.com/blog/papermod-cover.png">
<meta name=twitter:title content="Graph Convolutional Neural Network -  Spectral Convolution">
<meta name=twitter:description content="Fourier Transform Virtually everything in the world can be described via a waveform - a function of time, space or some other variable. For instance, sound waves, the price of a stock, etc. The Fourier Transform gives us a unique and powerful way of viewing these waveforms: All waveforms, no matter what you scribble or observe in the universe, are actually just the sum of simple sinusoids of different frequencies.">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://tangliyan.com/blog/posts/"},{"@type":"ListItem","position":2,"name":"Graph Convolutional Neural Network -  Spectral Convolution","item":"https://tangliyan.com/blog/posts/spectral_conv/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Graph Convolutional Neural Network -  Spectral Convolution","name":"Graph Convolutional Neural Network -  Spectral Convolution","description":"Fourier Transform Virtually everything in the world can be described via a waveform - a function of time, space or some other variable. For instance, sound waves, the price of a stock, etc. The Fourier Transform gives us a unique and powerful way of viewing these waveforms: All waveforms, no matter what you scribble or observe in the universe, are actually just the sum of simple sinusoids of different frequencies.","keywords":["GRAPH","MATH"],"articleBody":"Fourier Transform Virtually everything in the world can be described via a waveform - a function of time, space or some other variable. For instance, sound waves, the price of a stock, etc. The Fourier Transform gives us a unique and powerful way of viewing these waveforms: All waveforms, no matter what you scribble or observe in the universe, are actually just the sum of simple sinusoids of different frequencies.\nHere is the mathematical definition of Fourier Transform $\\mathcal{F}$:\n$$ \\mathcal{F}[f(t)]=\\int f(t) e^{-i \\omega t} d t \\tag 1 $$\nFourier Transform decomposes a function defined in the space/time domain into several components in the frequency domain. In other words, the Fourier transform can change a function from the spatial domain to the frequency domain. Check the Graph Fourier Transform section for more details.\nConvolution Theorem Convolution Theorem: The convolution of two functions in real space is the same as the product of their respective Fourier transforms in Fourier space.\nEquivalent statement:\n  Convolution in time domain equals multiplication in frequency domain.\n  Multiplication in time equals convolution in the frequency domain.\n  $$ \\mathscr{F}[(f * g)(t)] = \\mathscr{F}(f(t)) \\odot \\mathscr{F}(g(t)) \\tag 2$$\nIn other words, one can calculate the convolution of two functions $f$ and $g$ by first transforming them into the frequency domain through Fourier transform, multiplying the two functions in the frequency domain, and then transforming them back through inverse Fourier transform. The mathematical expression of this idea is\n$$(f * g)(t) = \\mathscr{F}^{-1}[\\mathscr{F}(f(t)) \\odot \\mathscr{F}(g(t))] \\tag 3$$\nwhere $\\odot$ is the element-wise product. we denote the Fourier transform of a function $f$ as $\\hat{f}$.\nGraph Fourier Transform A few definitions Spectral-based methods have a solid mathematical foundation in graph signal processing. They assume graphs to be undirected.\n  Adjacency $\\operatorname{matrix} \\mathbf{A}$: The adjacency $\\operatorname{matrix} \\mathbf{A}$ is a $n \\times n$ matrix with $A_{i j}=1$ if $e_{i j} \\in E$ and $A_{i j}=0$ if $e_{i j} \\notin E$.\n  Degree $\\operatorname{matrix} \\mathbf{D}$: The degree matrix is a diagonal matrix which contains information about the degree of each vertex (the number of edges attached to each vertex). It is used together with the adjacency matrix to construct the Laplacian matrix of a graph.\n  Laplacian $\\operatorname{matrix} \\mathbf{L}$: The Laplacian matrix is a matrix representation of a graph, which is defined by $$\\mathbf{L} = \\mathbf{D} - \\mathbf{A} \\tag 4$$\n  Symmetric normalized Laplacian $\\operatorname{matrix} \\mathbf{L}^{sys}$: The normalized graph Laplacian matrix is a mathematical representation of an undirected graph $$ \\mathbf{L}^{sys} = \\mathbf{D}^{-\\frac{1}{2}} \\mathbf{L} \\mathbf{D}^{-\\frac{1}{2}} \\tag 5$$\n  The symmetric normalized graph Laplacian matrix possesses the property of being real symmetric positive semidefinite. With this property, the normalized Laplacian matrix can be factored as\n$$\\mathbf{L}^{sys}=\\mathbf{U} \\mathbf{\\Lambda} \\mathbf{U}^{T} \\tag 6$$\nwhere\n$$\\mathbf{U}=\\left[\\mathbf{u}_{\\mathbf{0}}, \\mathbf{u}_{\\mathbf{1}}, \\cdots, \\mathbf{u}_{\\mathbf{n}-1}\\right] \\in \\mathbf{R}^{n \\times n} \\tag 7$$\nis the matrix of eigenvectors ordered by eigenvalues and $\\mathbf{\\Lambda}$ is the diagonal matrix of eigenvalues (spectrum), $\\Lambda_{i i}=\\lambda_{i}$. The eigenvectors of the normalized Laplacian matrix form an orthonormal space, in mathematical words $\\mathbf{U}^{T} \\mathbf{U}=\\mathbf{I}$.\nGraph Fourier transform In graph signal processing, a graph signal $\\mathbf{x} \\in \\mathbf{R}^{n}$ is a feature vector of all nodes of a graph where $x_{i}$ is the value of the $i^{t h}$ node.\nThe graph Fourier transform to a signal $\\mathbf{\\hat{f}}$ is defined as\n$$\\mathscr{F}(\\mathbf{f})=\\mathbf{U}^{T} \\mathbf{f} \\tag 8$$\nand the inverse graph Fourier transform is defined as\n$$\\mathscr{F}^{-1}(\\mathbf{\\hat{f}})=\\mathbf{U} \\mathbf{\\hat{f}} \\tag 9$$\nwhere $\\mathbf{\\hat{f}}$ represents the resulted signal from the graph Fourier transform.\nNote:\n  $$ \\mathbf{\\hat{f}} = \\left(\\begin{array}{c} \\hat{f}\\left(\\lambda_{1}\\right) \\\\ \\hat{f}\\left(\\lambda_{2}\\right) \\\\ \\vdots \\\\ \\hat{f}\\left(\\lambda_{n}\\right) \\end{array}\\right)= \\mathbf{U}^{T} \\mathbf{f} = \\left(\\begin{array}{cccc} u_{1}(1) \u0026 u_{1}(2) \u0026 \\ldots \u0026 u_{1}(n) \\\\ u_{2}(1) \u0026 u_{2}(2) \u0026 \\ldots \u0026 u_{2}(n) \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ u_{n}(1) \u0026 u_{n}(2) \u0026 \\ldots \u0026 u_{n}(n) \\end{array}\\right)\\left(\\begin{array}{c} f(1) \\\\ f(2) \\\\ \\vdots \\\\ f(n) \\end{array}\\right) \\tag {10} $$ where $\\lambda_i$ are ordered eigenvalues (biggest to smallest), $N$ is the number of nodes.\n  $$\\mathbf{\\hat{f}}(\\lambda_{l})=\\sum_{i=1}^{n} \\mathbf{f}(i) u_{l}(i) \\tag {11}$$\n  $$ \\mathbf{f} = \\left(\\begin{array}{c} f(1) \\\\ f(2) \\\\ \\vdots \\\\ f(n) \\end{array}\\right)= \\mathbf{U} \\mathbf{\\hat{f}} = \\left(\\begin{array}{cccc} u_{1}(1) \u0026 u_{2}(1) \u0026 \\ldots \u0026 u_{n}(1) \\\\ u_{1}(2) \u0026 u_{2}(2) \u0026 \\ldots \u0026 u_{n}(2) \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ u_{1}(n) \u0026 u_{2}(n) \u0026 \\ldots \u0026 u_{n}(n) \\end{array}\\right)\\left(\\begin{array}{c} \\hat{f}\\left(\\lambda_{1}\\right) \\\\ \\hat{f}\\left(\\lambda_{2}\\right) \\\\ \\vdots \\\\ \\hat{f}\\left(\\lambda_{n}\\right) \\end{array}\\right) \\tag {12} $$\n  The graph Fourier transform projects the input graph signal to the orthonormal space where the basis is formed by independent eigenvectors ($\\mathbf{u}_{\\mathbf{0}}, \\mathbf{u}_{\\mathbf{1}}, \\cdots, \\mathbf{u}_{\\mathbf{n}-1}$) of the normalized graph Laplacian. Elements of the transformed signal $\\mathbf{\\hat{f}}$ are the coordinates of the graph signal in the new space so that the input signal can be represented as $\\mathbf{f}=\\sum_{i} \\mathbf{\\hat{f}}(\\lambda_{i}) \\mathbf{u}_{i}$ (linear combination of independent eigenvectors, coefficients are entries in vactor $\\mathbf{\\hat{f}}$), which is exactly the inverse graph Fourier transform.\nNow the graph convolution of the input signal $\\mathbf{f}$ with a filter $\\mathbf{g} \\in R^{n}$ is defined as\n$$ \\begin{aligned} \\mathbf{f} *_G \\mathbf{g} \u0026=\\mathscr{F}^{-1}(\\mathscr{F}(\\mathbf{f}) \\odot \\mathscr{F}(\\mathbf{g})) \\\\ \u0026=\\mathbf{U}\\left(\\mathbf{U}^{T} \\mathbf{f} \\odot \\mathbf{U}^{T} \\mathbf{g}\\right) \\\\ \u0026=\\mathbf{U}\\left(\\mathbf{\\hat f} \\odot \\mathbf{\\hat g}\\right) \\end{aligned} \\tag {13} $$\nNote that (13) uses formula from (3)(8)(9).\nWe know from (10) and (11) that\n$$ \\mathbf{\\hat{f}} = \\left(\\begin{array}{c} \\hat{f}\\left(\\lambda_{1}\\right) \\\\ \\hat{f}\\left(\\lambda_{2}\\right) \\\\ \\vdots \\\\ \\hat{f}\\left(\\lambda_{n}\\right) \\end{array}\\right) = \\left(\\begin{array}{c} \\mathbf{\\hat{f}}(\\lambda_{1})=\\sum_{i=1}^{n} \\mathbf{f}(i) u_{1}(i) \\\\ \\mathbf{\\hat{f}}(\\lambda_{2})=\\sum_{i=1}^{n} \\mathbf{f}(i) u_{2}(i) \\\\ \\vdots \\\\ \\mathbf{\\hat{f}}(\\lambda_{n})=\\sum_{i=1}^{n} \\mathbf{f}(i) u_{n}(i) \\end{array}\\right) \\tag {14} $$\nTherefore, we could write the element-wise product $\\mathbf{\\hat f} \\odot \\mathbf{\\hat g}$ as\n$$ \\mathbf{\\hat f} \\odot \\mathbf{\\hat g}= \\mathbf{\\hat g} \\odot \\mathbf{\\hat f}= \\mathbf{\\hat g} \\odot \\mathbf{U}^{T} \\mathbf{f}= \\left(\\begin{array}{ccc} \\mathbf{\\hat g}\\left(\\lambda_{1}\\right) \u0026 \u0026 \\\\ \u0026 \\ddots \u0026 \\ \u0026 \u0026 \\mathbf{\\hat g} \\left(\\lambda_{n}\\right) \\end{array}\\right) \\mathbf{U}^{T} \\mathbf{f} \\tag {15} $$\nIf we denote a filter as $\\mathbf{g}_{\\theta}(\\mathbf{\\Lambda})=\\operatorname{diag}\\left(\\mathbf{U}^{T} \\mathbf{g}\\right),$ then the spectral graph convolution is simplified as\n$$ \\mathbf{f} *_G \\mathbf{g} = \\mathbf{U g}_{\\theta}(\\mathbf{\\Lambda}) \\mathbf{U}^{T} \\mathbf{f} \\tag {16} $$\nSpectral-based ConvGNNs all follow this definition. The key difference lies in the choice of the filter $\\mathrm{g}_{\\theta}$. In the rest of the post, I list two designs of filter for making a sense of what Spectral Convolutional Neural Network is.\nSpectral-based ConvGNNs Definition:\n (the number of) Input/output channels: dimensionality of node feature vectors.  Spectral Convolutional Neural Network assumes the filter $\\mathbf{g}_{\\theta}=\\Theta_{i, j}^{(k)}$ is a set of learnable parameters and considers graph signals with multiple channels. The graph convolutional layer of Spectral CNN is defined as\n$$ \\mathbf{H}_{:, j}^{(k)}=\\sigma\\left(\\sum_{i=1}^{f_{k-1}} \\mathbf{U} \\Theta_{i, j}^{(k)} \\mathbf{U}^{T} \\mathbf{H}_{:, i}^{(k-1)}\\right) \\quad\\left(j=1,2, \\cdots, f_{k}\\right) \\tag {17} $$\nwhere $k$ is the layer index, $\\mathbf{H}^{(k-1)} \\in \\mathbf{R}^{n \\times f_{k-1}}$ is the input graph signal, $\\mathbf{H}^{(0)}=\\mathbf{X}$ (node feature matrix of a graph), $f_{k-1}$ is the number of input channels and $f_{k}$ is the number of output channels, $\\Theta_{i, j}^{(k)}$ is a diagonal matrix filled with learnable parameters.\nDue to the eigen-decomposition of the Laplacian matrix, Spectral CNN faces three limitations:\n  Any perturbation to a graph results in a change of eigenbasis.\n  The learned filters are domain dependent, meaning they cannot be applied to a graph with a different structure.\n  Eigen-decomposition requires $O\\left(n^{3}\\right)$ computational complexity ($n$: the number of nodes).\n  Naive Design of $\\mathbf{g}_{\\theta}(\\mathbf{\\Lambda})$ The naive way is to set $\\mathbf{\\hat g}(\\lambda_l) = \\theta_l$, that is,\n$$ g_{\\theta}(\\mathbf{\\Lambda})=\\left(\\begin{array}{ccc} \\theta_{1} \u0026 \u0026 \\\\ \u0026 \\ddots \u0026 \\\\ \u0026 \u0026 \\theta_{n} \\end{array}\\right) \\tag {18} $$\nLimitations:\n  In each forward-propagation step, we need to calculate the product of the three matrices The amount of calculation is too large especially for relatively large graphs.\n  Convolution kernel does not have Spatial Localization.\n  The number of $\\theta_l$ depends on the total number of nodes, which is unacceptable for large graph.\n  In follow-up works, ChebNet, for example, reduces the computational complexity by making several approximations and simplifications.\nChebyshev Spectral CNN (Recursive formulation for fast filtering) Polynomial parametrization for localized filters Limitations mentioned in the last section can be overcome with the use of a polynomial filter, where\n$$\\mathbf{\\hat g}(\\lambda_l) = \\sum_{i=0}^{K} \\theta_{l} \\lambda^{l} \\tag{19}$$\nWritten in the matrix format, we have\n$$ g_{\\theta}(\\mathbf{\\Lambda})=\\left(\\begin{array}{ccc} \\sum_{i=0}^{K} \\theta_{i} \\lambda_{1}^{i} \u0026 \u0026 \\\\ \u0026 \\ddots \u0026 \\ \u0026 \u0026 \\sum_{i=0}^{K} \\theta_{i} \\lambda_{n}^{i} \\end{array}\\right)=\\sum_{i=0}^{K} \\theta_{i} \\Lambda^{i} \\tag{20} $$\nThen, replacing new defined $g_{\\theta}(\\mathbf{\\Lambda})$ in (16), we have\n$$\\mathbf{f} *_G \\mathbf{g} = U (\\sum_{i=0}^{K} \\theta_{i} \\Lambda^{i}) U^{T} \\mathbf{f} =\\sum_{i=0}^{K} \\theta_{i} (U \\Lambda^{i} U^{T}) \\mathbf{f} =\\sum_{i=0}^{K} \\theta_{i} L^{i} \\mathbf{f} \\tag {21}$$\nBy using polynomial parametrization,\n  Parameters reduce from $n$ to $K+1$.\n  We no longer have to multiply three matrices, but instead we only need to calculate powers of matrix $L$.\n  It can be shown that this filter is localized in space (Spatial Localization). In other words, spectral filters represented by $K$th-order polynomials of the Laplacian are exactly K-localized (K-localized means the model is aggregating the information from neighbors within $K$th order, see the figure above).\n  Chebyshev Spectral CNN (ChebNet) Chebyshev Spectral CNN (ChebNet) approximates the filter $g_{\\theta}(\\mathbf{\\Lambda})$ by Chebyshev polynomials of the diagonal matrix of eigenvalues, i.e,\n$$\\mathrm{g}_{\\theta}(\\mathbf{\\Lambda})=\\sum_{i=0}^{K} \\theta_{i} T_{i}(\\tilde{\\boldsymbol{\\Lambda}}) \\tag{22}$$\nwhere\n  $\\tilde{\\boldsymbol{\\Lambda}}=2 \\mathbf{\\Lambda} / \\lambda_{\\max }- \\mathbf{I}_{\\mathbf{n}}$, a diagonal matrix of scaled eigenvalues that lie in $[−1, 1]$.\n  The Chebyshev polynomials are defined recursively by $$T_{i}(\\mathbf{f})=2 \\mathbf{f} T_{i-1}(\\mathbf{f})-T_{i-2}(\\mathbf{f}) \\tag{23}$$ with $T_{0}(\\mathbf{f})=1$ and $T_{1}(\\mathbf{f})=\\mathbf{f}$.\n  As a result, the convolution of a graph signal $\\mathbf{f}$ with the defined filter $\\mathrm{g}_{\\theta}(\\mathbf{\\Lambda})$ is $$ \\mathbf{f} *_{G} \\mathbf{g}_{\\theta}=\\mathbf{U}\\left(\\sum_{i=0}^{K} \\theta_{i} T_{i}(\\tilde{\\boldsymbol{\\Lambda}})\\right) \\mathbf{U}^{T} \\mathbf{f} \\tag{24} $$\nDenote scaled Laplacian $\\tilde{\\mathbf{L}}$ as\n$$\\tilde{\\mathbf{L}}=2 \\mathbf{L} / \\lambda_{\\max }-\\mathbf{I}_{\\mathbf{n}} \\tag {25}$$\nThen by induction on $i$ and using (21), the following equation holds:\n$$T_{i}(\\tilde{\\mathbf{L}})=\\mathbf{U} T_{i}(\\tilde{\\boldsymbol{\\Lambda}}) \\mathbf{U}^{T} \\tag {26}$$\nThen finally ChebNet takes the following form:\n$$ \\mathbf{f} *_{G} \\mathbf{g}_{\\theta}=\\sum_{i=0}^{K} \\theta_{i} T_{i}(\\tilde{\\mathbf{L}}) \\mathbf{f} \\tag {27} $$\nThe benefits of ChebNet is similar to those of polynomial parametrization mentioned above.\nComparison between spectral and spatial models Spectral models have a theoretical foundation in graph signal processing. By designing new graph signal filters, one can build new ConvGNNs. However, spatial models are preferred over spectral models due to efficiency, generality, and flexibility issues.\nFirst, spectral models are less efficient than spatial models. Spectral models either need to perform eigenvector computation or handle the whole graph at the same time. Spatial models are more scalable to large graphs as they directly perform convolutions in the graph domain via information propagation. The computation can be performed in a batch of nodes instead of the whole graph.\nSecond, spectral models which rely on a graph Fourier basis generalize poorly to new graphs. They assume a fixed graph. Any perturbations to a graph would result in a change of eigenbasis. Spatial-based models, on the other hand, perform graph convolutions locally on each node where weights can be easily shared across different locations and structures.\nThird, spectral-based models are limited to operate on undirected graphs. Spatial-based models are more flexible to handle multi-source graph inputs such as edge inputs, directed graphs, signed graphs, and heterogeneous graphs, because these graph inputs can be incorporated into the aggregation function easily.\n Reference:\n Introduction to the Fourier Transform: http://www.thefouriertransform.com/#introduction Degree Matrix: https://en.wikipedia.org/wiki/Degree_matrix Introduction to Graph Signal Processing: https://link.springer.com/chapter/10.1007/978-3-030-03574-7_1 Convolution Theorem: https://www.sciencedirect.com/topics/engineering/convolution-theorem The Convolution Theorem with Application Examples: https://dspillustrations.com/pages/posts/misc/the-convolution-theorem-and-application-examples.html  Paper:\n A Comprehensive Survey on Graph Neural Networks: https://arxiv.org/pdf/1901.00596.pdf Spectral Networks and Deep Locally Connected Networks on Graphs https://arxiv.org/pdf/1312.6203.pdf Spectral Networks and Deep Locally Connected Networks on Graphs: https://arxiv.org/pdf/1312.6203.pdf Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering: https://arxiv.org/pdf/1606.09375.pdf  ","wordCount":"1823","inLanguage":"en","datePublished":"2020-08-24T00:00:00Z","dateModified":"2020-08-24T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://tangliyan.com/blog/posts/spectral_conv/"},"publisher":{"@type":"Organization","name":"Liyan Tang","logo":{"@type":"ImageObject","url":"https://tangliyan.com/blog/favicon.ico"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://tangliyan.com/blog/ accesskey=h title="Liyan Tang (Alt + H)">Liyan Tang</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
<li>
<a href=https://tangliyan.com/blog/archives title=Archive>
<span>Archive</span>
</a>
</li>
<li>
<a href=https://tangliyan.com/blog/tags/ title=Tags>
<span>Tags</span>
</a>
</li>
<li>
<a href=https://tangliyan.com/blog/search/ title="Search (Alt + /)" accesskey=/>
<span>Search</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<div class=breadcrumbs><a href=https://tangliyan.com/blog/>Home</a>&nbsp;»&nbsp;<a href=https://tangliyan.com/blog/posts/>Posts</a></div>
<h1 class=post-title>
Graph Convolutional Neural Network - Spectral Convolution
</h1>
<div class=post-meta><span title="2020-08-24 00:00:00 +0000 UTC">August 24, 2020</span>&nbsp;·&nbsp;9 min
</div>
</header> <div class=toc>
<details>
<summary accesskey=c title="(Alt + C)">
<span class=details>Table of Contents</span>
</summary>
<div class=inner><ul>
<li>
<a href=#fourier-transform aria-label="Fourier Transform">Fourier Transform</a></li>
<li>
<a href=#convolution-theorem aria-label="Convolution Theorem">Convolution Theorem</a></li>
<li>
<a href=#graph-fourier-transform aria-label="Graph Fourier Transform">Graph Fourier Transform</a><ul>
<li>
<a href=#a-few-definitions aria-label="A few definitions">A few definitions</a></li>
<li>
<a href=#graph-fourier-transform-1 aria-label="Graph Fourier transform">Graph Fourier transform</a></li></ul>
</li>
<li>
<a href=#spectral-based-convgnns aria-label="Spectral-based ConvGNNs">Spectral-based ConvGNNs</a><ul>
<li>
<a href=#naive-design-of-mathbfg_thetamathbflambda aria-label="Naive Design of $\mathbf{g}_{\theta}(\mathbf{\Lambda})$">Naive Design of $\mathbf{g}_{\theta}(\mathbf{\Lambda})$</a></li>
<li>
<a href=#chebyshev-spectral-cnn-recursive-formulation-for-fast-filtering aria-label="Chebyshev Spectral CNN (Recursive formulation for fast filtering)">Chebyshev Spectral CNN (Recursive formulation for fast filtering)</a><ul>
<li>
<a href=#polynomial-parametrization-for-localized-filters aria-label="Polynomial parametrization for localized filters">Polynomial parametrization for localized filters</a></li>
<li>
<a href=#chebyshev-spectral-cnn-chebnet aria-label="Chebyshev Spectral CNN (ChebNet)">Chebyshev Spectral CNN (ChebNet)</a></li></ul>
</li></ul>
</li>
<li>
<a href=#comparison-between-spectral-and-spatial-models aria-label="Comparison between spectral and spatial models">Comparison between spectral and spatial models</a>
</li>
</ul>
</div>
</details>
</div>
<div class=post-content><h2 id=fourier-transform>Fourier Transform<a hidden class=anchor aria-hidden=true href=#fourier-transform>#</a></h2>
<p>Virtually everything in the world can be described via a waveform - a function of time, space or some other variable. For instance, sound waves, the price of a stock, <em>etc</em>. The Fourier Transform gives us a unique and powerful way of viewing these waveforms: <strong>All waveforms, no matter what you scribble or observe in the universe, are actually just the sum of simple sinusoids of different frequencies.</strong></p>
<p>Here is the mathematical definition of Fourier Transform $\mathcal{F}$:</p>
<p>$$
\mathcal{F}[f(t)]=\int f(t) e^{-i \omega t} d t \tag 1
$$</p>
<p>Fourier Transform decomposes a function defined in the space/time domain into several components in the frequency domain. In other words, the Fourier transform can change a function from the spatial domain to the frequency domain. Check the <em>Graph Fourier Transform section</em> for more details.</p>
<h2 id=convolution-theorem>Convolution Theorem<a hidden class=anchor aria-hidden=true href=#convolution-theorem>#</a></h2>
<img src=https://img-blog.csdnimg.cn/20200824030350303.gif#pic_center>
<p><strong>Convolution Theorem</strong>: The convolution of two functions in real space is the same as the product of their respective Fourier transforms in Fourier space.</p>
<p>Equivalent statement:</p>
<ul>
<li>
<p>Convolution in time domain equals multiplication in frequency domain.</p>
</li>
<li>
<p>Multiplication in time equals convolution in the frequency domain.</p>
</li>
</ul>
<p>$$ \mathscr{F}[(f * g)(t)] = \mathscr{F}(f(t)) \odot \mathscr{F}(g(t)) \tag 2$$</p>
<p>In other words, one can calculate the convolution of two functions $f$ and $g$ by first transforming them into the frequency domain through Fourier transform, multiplying the two functions in the frequency domain, and then transforming them back through inverse Fourier transform. The mathematical expression of this idea is</p>
<p>$$(f * g)(t) = \mathscr{F}^{-1}[\mathscr{F}(f(t)) \odot \mathscr{F}(g(t))] \tag 3$$</p>
<p>where $\odot$ is the element-wise product. we denote the Fourier transform of a function $f$ as $\hat{f}$.</p>
<h2 id=graph-fourier-transform>Graph Fourier Transform<a hidden class=anchor aria-hidden=true href=#graph-fourier-transform>#</a></h2>
<h3 id=a-few-definitions>A few definitions<a hidden class=anchor aria-hidden=true href=#a-few-definitions>#</a></h3>
<p>Spectral-based methods have a solid mathematical foundation in graph signal processing. <em>They assume graphs to be undirected</em>.</p>
<img src="https://img-blog.csdnimg.cn/20200824030424952.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_16,color_FFFFFF,t_70#pic_center">
<ul>
<li>
<p>Adjacency $\operatorname{matrix} \mathbf{A}$: The adjacency $\operatorname{matrix} \mathbf{A}$ is a $n \times n$ matrix with $A_{i j}=1$ if $e_{i j} \in E$ and $A_{i j}=0$ if $e_{i j} \notin E$.</p>
</li>
<li>
<p>Degree $\operatorname{matrix} \mathbf{D}$: The degree matrix is a diagonal matrix which contains information about the degree of each vertex (the number of edges attached to each vertex). It is used together with the adjacency matrix to construct the Laplacian matrix of a graph.</p>
</li>
<li>
<p>Laplacian $\operatorname{matrix} \mathbf{L}$: The Laplacian matrix is a matrix representation of a graph, which is defined by
$$\mathbf{L} = \mathbf{D} - \mathbf{A} \tag 4$$</p>
</li>
<li>
<p>Symmetric normalized Laplacian $\operatorname{matrix} \mathbf{L}^{sys}$: The normalized graph Laplacian matrix is a mathematical representation of an undirected graph
$$ \mathbf{L}^{sys} = \mathbf{D}^{-\frac{1}{2}} \mathbf{L} \mathbf{D}^{-\frac{1}{2}} \tag 5$$</p>
</li>
</ul>
<p>The symmetric normalized graph Laplacian matrix possesses the property of being <strong>real symmetric positive semidefinite</strong>. With this property, the normalized Laplacian matrix can be factored as</p>
<p>$$\mathbf{L}^{sys}=\mathbf{U} \mathbf{\Lambda} \mathbf{U}^{T} \tag 6$$</p>
<p>where</p>
<p>$$\mathbf{U}=\left[\mathbf{u}_{\mathbf{0}}, \mathbf{u}_{\mathbf{1}}, \cdots, \mathbf{u}_{\mathbf{n}-1}\right] \in \mathbf{R}^{n \times n} \tag 7$$</p>
<p>is the matrix of eigenvectors ordered by eigenvalues and $\mathbf{\Lambda}$ is the diagonal matrix of eigenvalues (spectrum), $\Lambda_{i i}=\lambda_{i}$. The eigenvectors of the normalized Laplacian matrix form an orthonormal space, in mathematical words $\mathbf{U}^{T} \mathbf{U}=\mathbf{I}$.</p>
<h3 id=graph-fourier-transform-1>Graph Fourier transform<a hidden class=anchor aria-hidden=true href=#graph-fourier-transform-1>#</a></h3>
<img src="https://img-blog.csdnimg.cn/20200824030515968.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_1,color_FFFFFF,t_70#pic_center" width=650>
<p>In <strong>graph signal processing</strong>, a graph signal $\mathbf{x} \in \mathbf{R}^{n}$ is a feature vector of all nodes of a graph where $x_{i}$ is the value of the $i^{t h}$ node.</p>
<p>The <strong>graph Fourier transform</strong> to a signal $\mathbf{\hat{f}}$ is defined as</p>
<p>$$\mathscr{F}(\mathbf{f})=\mathbf{U}^{T} \mathbf{f} \tag 8$$</p>
<p>and the <strong>inverse graph Fourier transform</strong> is defined as</p>
<p>$$\mathscr{F}^{-1}(\mathbf{\hat{f}})=\mathbf{U} \mathbf{\hat{f}} \tag 9$$</p>
<p>where $\mathbf{\hat{f}}$ represents the resulted signal from the graph Fourier transform.</p>
<p>Note:</p>
<ul>
<li>
<p>$$
\mathbf{\hat{f}} =
\left(\begin{array}{c}
\hat{f}\left(\lambda_{1}\right) \\ \hat{f}\left(\lambda_{2}\right) \\ \vdots \\ \hat{f}\left(\lambda_{n}\right)
\end{array}\right)=
\mathbf{U}^{T} \mathbf{f} =
\left(\begin{array}{cccc}
u_{1}(1) & u_{1}(2) & \ldots & u_{1}(n) \\ u_{2}(1) & u_{2}(2) & \ldots & u_{2}(n) \\ \vdots & \vdots & \ddots & \vdots \\ u_{n}(1) & u_{n}(2) & \ldots & u_{n}(n)
\end{array}\right)\left(\begin{array}{c}
f(1) \\ f(2) \\ \vdots \\ f(n)
\end{array}\right) \tag {10}
$$
where $\lambda_i$ are ordered eigenvalues (biggest to smallest), $N$ is the number of nodes.</p>
</li>
<li>
<p>$$\mathbf{\hat{f}}(\lambda_{l})=\sum_{i=1}^{n} \mathbf{f}(i) u_{l}(i) \tag {11}$$</p>
</li>
<li>
<p>$$ \mathbf{f} =
\left(\begin{array}{c}
f(1) \\ f(2) \\ \vdots \\ f(n)
\end{array}\right)=
\mathbf{U} \mathbf{\hat{f}} =
\left(\begin{array}{cccc}
u_{1}(1) & u_{2}(1) & \ldots & u_{n}(1) \\ u_{1}(2) & u_{2}(2) & \ldots & u_{n}(2) \\ \vdots & \vdots & \ddots & \vdots \\ u_{1}(n) & u_{2}(n) & \ldots & u_{n}(n)
\end{array}\right)\left(\begin{array}{c}
\hat{f}\left(\lambda_{1}\right) \\ \hat{f}\left(\lambda_{2}\right) \\ \vdots \\ \hat{f}\left(\lambda_{n}\right)
\end{array}\right) \tag {12}
$$</p>
</li>
</ul>
<p>The graph Fourier transform projects the input graph signal to the orthonormal space where the basis is formed by independent eigenvectors ($\mathbf{u}_{\mathbf{0}}, \mathbf{u}_{\mathbf{1}}, \cdots, \mathbf{u}_{\mathbf{n}-1}$) of the normalized graph Laplacian. Elements of the transformed signal $\mathbf{\hat{f}}$ are the coordinates of the graph signal in the new space so that the input signal can be represented as $\mathbf{f}=\sum_{i} \mathbf{\hat{f}}(\lambda_{i}) \mathbf{u}_{i}$ (linear combination of independent eigenvectors, coefficients are entries in vactor $\mathbf{\hat{f}}$), which is exactly the inverse graph Fourier transform.</p>
<p>Now the graph convolution of the input signal $\mathbf{f}$ with a filter $\mathbf{g} \in R^{n}$ is defined as</p>
<p>$$
\begin{aligned}
\mathbf{f} *_G \mathbf{g} &=\mathscr{F}^{-1}(\mathscr{F}(\mathbf{f}) \odot \mathscr{F}(\mathbf{g})) \\ &=\mathbf{U}\left(\mathbf{U}^{T} \mathbf{f} \odot \mathbf{U}^{T} \mathbf{g}\right) \\ &=\mathbf{U}\left(\mathbf{\hat f} \odot \mathbf{\hat g}\right)
\end{aligned} \tag {13}
$$</p>
<p>Note that (13) uses formula from (3)(8)(9).</p>
<p>We know from (10) and (11) that</p>
<p>$$
\mathbf{\hat{f}} =
\left(\begin{array}{c}
\hat{f}\left(\lambda_{1}\right) \\ \hat{f}\left(\lambda_{2}\right) \\ \vdots \\ \hat{f}\left(\lambda_{n}\right)
\end{array}\right) =
\left(\begin{array}{c}
\mathbf{\hat{f}}(\lambda_{1})=\sum_{i=1}^{n} \mathbf{f}(i) u_{1}(i) \\ \mathbf{\hat{f}}(\lambda_{2})=\sum_{i=1}^{n} \mathbf{f}(i) u_{2}(i) \\ \vdots \\ \mathbf{\hat{f}}(\lambda_{n})=\sum_{i=1}^{n} \mathbf{f}(i) u_{n}(i)
\end{array}\right) \tag {14}
$$</p>
<p>Therefore, we could write the element-wise product $\mathbf{\hat f} \odot \mathbf{\hat g}$ as</p>
<p>$$
\mathbf{\hat f} \odot \mathbf{\hat g}=
\mathbf{\hat g} \odot \mathbf{\hat f}=
\mathbf{\hat g} \odot \mathbf{U}^{T} \mathbf{f}=
\left(\begin{array}{ccc}
\mathbf{\hat g}\left(\lambda_{1}\right) & & \\ & \ddots & \
& & \mathbf{\hat g} \left(\lambda_{n}\right)
\end{array}\right) \mathbf{U}^{T} \mathbf{f} \tag {15}
$$</p>
<p>If we denote a filter as $\mathbf{g}_{\theta}(\mathbf{\Lambda})=\operatorname{diag}\left(\mathbf{U}^{T} \mathbf{g}\right),$ then the spectral graph convolution is simplified as</p>
<p>$$
\mathbf{f} *_G \mathbf{g} = \mathbf{U g}_{\theta}(\mathbf{\Lambda}) \mathbf{U}^{T} \mathbf{f} \tag {16}
$$</p>
<p><strong>Spectral-based ConvGNNs all follow this definition. The key difference lies in the choice of the filter $\mathrm{g}_{\theta}$.</strong> In the rest of the post, I list two designs of filter for making a sense of what Spectral Convolutional Neural Network is.</p>
<h2 id=spectral-based-convgnns>Spectral-based ConvGNNs<a hidden class=anchor aria-hidden=true href=#spectral-based-convgnns>#</a></h2>
<p>Definition:</p>
<ul>
<li><strong>(the number of) Input/output channels</strong>: dimensionality of node feature vectors.</li>
</ul>
<p>Spectral Convolutional Neural Network assumes the filter $\mathbf{g}_{\theta}=\Theta_{i, j}^{(k)}$ is a set of learnable parameters and considers graph signals with multiple channels. The graph convolutional layer of Spectral CNN is defined as</p>
<p>$$
\mathbf{H}_{:, j}^{(k)}=\sigma\left(\sum_{i=1}^{f_{k-1}} \mathbf{U} \Theta_{i, j}^{(k)} \mathbf{U}^{T} \mathbf{H}_{:, i}^{(k-1)}\right) \quad\left(j=1,2, \cdots, f_{k}\right) \tag {17}
$$</p>
<p>where $k$ is the layer index, $\mathbf{H}^{(k-1)} \in \mathbf{R}^{n \times f_{k-1}}$ is the input graph signal, $\mathbf{H}^{(0)}=\mathbf{X}$ (node feature matrix of a graph), $f_{k-1}$ is the number of input channels and $f_{k}$ is the number of output channels, $\Theta_{i, j}^{(k)}$ is a diagonal matrix filled with learnable parameters.</p>
<p>Due to the eigen-decomposition of the Laplacian matrix, Spectral CNN faces three limitations:</p>
<ol>
<li>
<p>Any perturbation to a graph results in a change of eigenbasis.</p>
</li>
<li>
<p>The learned filters are domain dependent, meaning they cannot be applied to a graph with a different structure.</p>
</li>
<li>
<p>Eigen-decomposition requires $O\left(n^{3}\right)$ computational complexity ($n$: the number of nodes).</p>
</li>
</ol>
<h3 id=naive-design-of-mathbfg_thetamathbflambda>Naive Design of $\mathbf{g}_{\theta}(\mathbf{\Lambda})$<a hidden class=anchor aria-hidden=true href=#naive-design-of-mathbfg_thetamathbflambda>#</a></h3>
<p>The naive way is to set $\mathbf{\hat g}(\lambda_l) = \theta_l$, that is,</p>
<p>$$
g_{\theta}(\mathbf{\Lambda})=\left(\begin{array}{ccc}
\theta_{1} & & \\ & \ddots & \\ & & \theta_{n}
\end{array}\right) \tag {18}
$$</p>
<p>Limitations:</p>
<ol>
<li>
<p>In each forward-propagation step, we need to calculate the product of the three matrices The amount of calculation is too large especially for relatively large graphs.</p>
</li>
<li>
<p>Convolution kernel does not have Spatial Localization.</p>
</li>
<li>
<p>The number of $\theta_l$ depends on the total number of nodes, which is unacceptable for large graph.</p>
</li>
</ol>
<p>In follow-up works, ChebNet, for example, reduces the computational complexity by making several approximations and simplifications.</p>
<h3 id=chebyshev-spectral-cnn-recursive-formulation-for-fast-filtering>Chebyshev Spectral CNN (Recursive formulation for fast filtering)<a hidden class=anchor aria-hidden=true href=#chebyshev-spectral-cnn-recursive-formulation-for-fast-filtering>#</a></h3>
<h4 id=polynomial-parametrization-for-localized-filters>Polynomial parametrization for localized filters<a hidden class=anchor aria-hidden=true href=#polynomial-parametrization-for-localized-filters>#</a></h4>
<p>Limitations mentioned in the last section can be overcome with the use of a polynomial filter, where</p>
<p>$$\mathbf{\hat g}(\lambda_l) = \sum_{i=0}^{K} \theta_{l} \lambda^{l} \tag{19}$$</p>
<p>Written in the matrix format, we have</p>
<p>$$
g_{\theta}(\mathbf{\Lambda})=\left(\begin{array}{ccc}
\sum_{i=0}^{K} \theta_{i} \lambda_{1}^{i} & & \\ & \ddots & \
& & \sum_{i=0}^{K} \theta_{i} \lambda_{n}^{i}
\end{array}\right)=\sum_{i=0}^{K} \theta_{i} \Lambda^{i} \tag{20}
$$</p>
<p>Then, replacing new defined $g_{\theta}(\mathbf{\Lambda})$ in (16), we have</p>
<p>$$\mathbf{f} *_G \mathbf{g} = U (\sum_{i=0}^{K} \theta_{i} \Lambda^{i}) U^{T} \mathbf{f} =\sum_{i=0}^{K} \theta_{i} (U \Lambda^{i} U^{T}) \mathbf{f} =\sum_{i=0}^{K} \theta_{i} L^{i} \mathbf{f} \tag {21}$$</p>
<img src="https://img-blog.csdnimg.cn/20200824030605785.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_1,color_FFFFFF,t_70#pic_center" width=550>
<p>By using polynomial parametrization,</p>
<ul>
<li>
<p>Parameters reduce from $n$ to $K+1$.</p>
</li>
<li>
<p>We no longer have to multiply three matrices, but instead we only need to calculate powers of matrix $L$.</p>
</li>
<li>
<p>It can be shown that this filter is localized in space (<em>Spatial Localization</em>). In other words, spectral filters represented by $K$th-order polynomials of the Laplacian are exactly <em>K-localized</em> (K-localized means the model is aggregating the information from neighbors within $K$th order, see the figure above).</p>
</li>
</ul>
<h4 id=chebyshev-spectral-cnn-chebnet>Chebyshev Spectral CNN (ChebNet)<a hidden class=anchor aria-hidden=true href=#chebyshev-spectral-cnn-chebnet>#</a></h4>
<p>Chebyshev Spectral CNN (ChebNet) approximates the filter $g_{\theta}(\mathbf{\Lambda})$ by Chebyshev polynomials of the diagonal matrix of eigenvalues, <em>i.e</em>,</p>
<p>$$\mathrm{g}_{\theta}(\mathbf{\Lambda})=\sum_{i=0}^{K} \theta_{i} T_{i}(\tilde{\boldsymbol{\Lambda}}) \tag{22}$$</p>
<p>where</p>
<ul>
<li>
<p>$\tilde{\boldsymbol{\Lambda}}=2 \mathbf{\Lambda} / \lambda_{\max }-
\mathbf{I}_{\mathbf{n}}$, a diagonal matrix of scaled eigenvalues that lie in $[−1, 1]$.</p>
</li>
<li>
<p>The Chebyshev polynomials are defined recursively by
$$T_{i}(\mathbf{f})=2 \mathbf{f} T_{i-1}(\mathbf{f})-T_{i-2}(\mathbf{f}) \tag{23}$$
with $T_{0}(\mathbf{f})=1$ and $T_{1}(\mathbf{f})=\mathbf{f}$.</p>
</li>
</ul>
<p>As a result, the convolution of a graph signal $\mathbf{f}$ with the defined filter $\mathrm{g}_{\theta}(\mathbf{\Lambda})$ is
$$
\mathbf{f} *_{G} \mathbf{g}_{\theta}=\mathbf{U}\left(\sum_{i=0}^{K} \theta_{i} T_{i}(\tilde{\boldsymbol{\Lambda}})\right) \mathbf{U}^{T} \mathbf{f} \tag{24}
$$</p>
<p>Denote scaled Laplacian $\tilde{\mathbf{L}}$ as</p>
<p>$$\tilde{\mathbf{L}}=2 \mathbf{L} / \lambda_{\max }-\mathbf{I}_{\mathbf{n}} \tag {25}$$</p>
<p>Then by induction on $i$ and using (21), the following equation holds:</p>
<p>$$T_{i}(\tilde{\mathbf{L}})=\mathbf{U} T_{i}(\tilde{\boldsymbol{\Lambda}}) \mathbf{U}^{T} \tag {26}$$</p>
<p>Then finally ChebNet takes the following form:</p>
<p>$$
\mathbf{f} *_{G} \mathbf{g}_{\theta}=\sum_{i=0}^{K} \theta_{i} T_{i}(\tilde{\mathbf{L}}) \mathbf{f} \tag {27}
$$</p>
<p>The benefits of ChebNet is similar to those of polynomial parametrization mentioned above.</p>
<h2 id=comparison-between-spectral-and-spatial-models>Comparison between spectral and spatial models<a hidden class=anchor aria-hidden=true href=#comparison-between-spectral-and-spatial-models>#</a></h2>
<p>Spectral models have a theoretical foundation in graph signal processing. <em>By designing new graph signal filters, one can build new ConvGNNs.</em> However, spatial models are preferred over spectral models due to efficiency, generality, and flexibility issues.</p>
<p>First, spectral models are less efficient than spatial models. Spectral models either need to perform eigenvector computation or handle the whole graph at the same time. Spatial models are more scalable to large graphs as they directly perform convolutions in the graph domain via information propagation. The computation can be performed in a batch of nodes instead of the whole graph.</p>
<p>Second, spectral models which rely on a graph Fourier basis generalize poorly to new graphs. They assume a fixed graph. Any perturbations to a graph would result in a change of eigenbasis. Spatial-based models, on the other hand, perform graph convolutions locally on each node where weights can be easily shared across different locations and structures.</p>
<p>Third, spectral-based models are limited to operate on undirected graphs. Spatial-based models are more flexible to handle multi-source graph inputs such as edge inputs, directed graphs, signed graphs, and heterogeneous graphs, because these graph inputs can be incorporated into the aggregation function easily.</p>
<hr>
<p>Reference:</p>
<ul>
<li>Introduction to the Fourier Transform: <a href=http://www.thefouriertransform.com/#introduction>http://www.thefouriertransform.com/#introduction</a></li>
<li>Degree Matrix: <a href=https://en.wikipedia.org/wiki/Degree_matrix>https://en.wikipedia.org/wiki/Degree_matrix</a></li>
<li>Introduction to Graph Signal Processing: <a href=https://link.springer.com/chapter/10.1007/978-3-030-03574-7_1>https://link.springer.com/chapter/10.1007/978-3-030-03574-7_1</a></li>
<li>Convolution Theorem: <a href=https://www.sciencedirect.com/topics/engineering/convolution-theorem>https://www.sciencedirect.com/topics/engineering/convolution-theorem</a></li>
<li>The Convolution Theorem with Application Examples: <a href=https://dspillustrations.com/pages/posts/misc/the-convolution-theorem-and-application-examples.html>https://dspillustrations.com/pages/posts/misc/the-convolution-theorem-and-application-examples.html</a></li>
</ul>
<p>Paper:</p>
<ul>
<li>A Comprehensive Survey on Graph Neural Networks: <a href=https://arxiv.org/pdf/1901.00596.pdf>https://arxiv.org/pdf/1901.00596.pdf</a></li>
<li>Spectral Networks and Deep Locally Connected Networks on Graphs <a href=https://arxiv.org/pdf/1312.6203.pdf>https://arxiv.org/pdf/1312.6203.pdf</a></li>
<li>Spectral Networks and Deep Locally Connected Networks on Graphs: <a href=https://arxiv.org/pdf/1312.6203.pdf>https://arxiv.org/pdf/1312.6203.pdf</a></li>
<li>Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering: <a href=https://arxiv.org/pdf/1606.09375.pdf>https://arxiv.org/pdf/1606.09375.pdf</a></li>
</ul>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=https://tangliyan.com/blog/tags/graph/>GRAPH</a></li>
<li><a href=https://tangliyan.com/blog/tags/math/>MATH</a></li>
</ul>
<nav class=paginav>
<a class=prev href=https://tangliyan.com/blog/posts/the_more_you_know/>
<span class=title>« Prev Page</span>
<br>
<span>Paper Review - The More You Know: Using Knowledge Graphs for Image Classification</span>
</a>
<a class=next href=https://tangliyan.com/blog/posts/spatial_conv/>
<span class=title>Next Page »</span>
<br>
<span>Graph Convolutional Neural Network - Spatial Convolution</span>
</a>
</nav>
<div class=share-buttons>
<a target=_blank rel="noopener noreferrer" aria-label="share Graph Convolutional Neural Network -  Spectral Convolution on twitter" href="https://twitter.com/intent/tweet/?text=Graph%20Convolutional%20Neural%20Network%20-%20%20Spectral%20Convolution&url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fspectral_conv%2f&hashtags=GRAPH%2cMATH"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Graph Convolutional Neural Network -  Spectral Convolution on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fspectral_conv%2f&title=Graph%20Convolutional%20Neural%20Network%20-%20%20Spectral%20Convolution&summary=Graph%20Convolutional%20Neural%20Network%20-%20%20Spectral%20Convolution&source=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fspectral_conv%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Graph Convolutional Neural Network -  Spectral Convolution on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fspectral_conv%2f&title=Graph%20Convolutional%20Neural%20Network%20-%20%20Spectral%20Convolution"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Graph Convolutional Neural Network -  Spectral Convolution on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fspectral_conv%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Graph Convolutional Neural Network -  Spectral Convolution on whatsapp" href="https://api.whatsapp.com/send?text=Graph%20Convolutional%20Neural%20Network%20-%20%20Spectral%20Convolution%20-%20https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fspectral_conv%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Graph Convolutional Neural Network -  Spectral Convolution on telegram" href="https://telegram.me/share/url?text=Graph%20Convolutional%20Neural%20Network%20-%20%20Spectral%20Convolution&url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fspectral_conv%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a>
</div>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2022 <a href=https://tangliyan.com/blog/>Liyan Tang</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
<script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerText='copy';function d(){a.innerText='copied!',setTimeout(()=>{a.innerText='copy'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script>
</body>
</html>