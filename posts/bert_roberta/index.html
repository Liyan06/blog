<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>BERT and RoBERTa | Liyan Tang</title>
<meta name=keywords content="NLP">
<meta name=description content="BERT Recap Overview  Bert (Bidirectional Encoder Representations from Transformers) uses a &ldquo;masked language model&rdquo; to randomly mask some tokens from the input and predict the original vocabulary id of the masked token. Bert shows that &ldquo;pre-trained representations reduce the need for many heavily-engineered task-specific architectures&rdquo;.  BERT Specifics There are two steps to the BERT framework: pre-training and fine-tuning   During pre training, the model is trained on unlabeled data over different pre-training tasks.">
<meta name=author content>
<link rel=canonical href=https://tangliyan.com/blog/posts/bert_roberta/>
<link crossorigin=anonymous href=/blog/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css integrity="sha256-yIlj/i15RiAA/Q+xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/blog/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://tangliyan.com/blog/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://tangliyan.com/blog/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://tangliyan.com/blog/favicon-32x32.png>
<link rel=apple-touch-icon href=https://tangliyan.com/blog/apple-touch-icon.png>
<link rel=mask-icon href=https://tangliyan.com/blog/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.92.0">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-202974782-1','auto'),ga('send','pageview'))</script><meta property="og:title" content="BERT and RoBERTa ">
<meta property="og:description" content="BERT Recap Overview  Bert (Bidirectional Encoder Representations from Transformers) uses a &ldquo;masked language model&rdquo; to randomly mask some tokens from the input and predict the original vocabulary id of the masked token. Bert shows that &ldquo;pre-trained representations reduce the need for many heavily-engineered task-specific architectures&rdquo;.  BERT Specifics There are two steps to the BERT framework: pre-training and fine-tuning   During pre training, the model is trained on unlabeled data over different pre-training tasks.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://tangliyan.com/blog/posts/bert_roberta/"><meta property="og:image" content="https://tangliyan.com/blog/papermod-cover.png"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2020-09-18T00:00:00+00:00">
<meta property="article:modified_time" content="2020-09-18T00:00:00+00:00">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://tangliyan.com/blog/papermod-cover.png">
<meta name=twitter:title content="BERT and RoBERTa ">
<meta name=twitter:description content="BERT Recap Overview  Bert (Bidirectional Encoder Representations from Transformers) uses a &ldquo;masked language model&rdquo; to randomly mask some tokens from the input and predict the original vocabulary id of the masked token. Bert shows that &ldquo;pre-trained representations reduce the need for many heavily-engineered task-specific architectures&rdquo;.  BERT Specifics There are two steps to the BERT framework: pre-training and fine-tuning   During pre training, the model is trained on unlabeled data over different pre-training tasks.">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://tangliyan.com/blog/posts/"},{"@type":"ListItem","position":2,"name":"BERT and RoBERTa ","item":"https://tangliyan.com/blog/posts/bert_roberta/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"BERT and RoBERTa ","name":"BERT and RoBERTa ","description":"BERT Recap Overview  Bert (Bidirectional Encoder Representations from Transformers) uses a \u0026ldquo;masked language model\u0026rdquo; to randomly mask some tokens from the input and predict the original vocabulary id of the masked token. Bert shows that \u0026ldquo;pre-trained representations reduce the need for many heavily-engineered task-specific architectures\u0026rdquo;.  BERT Specifics There are two steps to the BERT framework: pre-training and fine-tuning   During pre training, the model is trained on unlabeled data over different pre-training tasks.","keywords":["NLP"],"articleBody":"BERT Recap Overview  Bert (Bidirectional Encoder Representations from Transformers) uses a “masked language model” to randomly mask some tokens from the input and predict the original vocabulary id of the masked token. Bert shows that “pre-trained representations reduce the need for many heavily-engineered task-specific architectures”.  BERT Specifics There are two steps to the BERT framework: pre-training and fine-tuning   During pre training, the model is trained on unlabeled data over different pre-training tasks.\n  Each down stream task has separate fine-tuned models after each is first initialized with pre-trained parameters.\n  Input Output Representations   In order to handle a variety of down-stream tasks, the input must be able to represent a single sentence and sentence pair in one sequence.\n  The first token of every sequence is always a classification token [CLS].\n  Sentence pairs are separated by a special token [SEP].\n  Learned embeddings are added to every token indication whether it belongs to sentence A or sentence B.\n  Tasks Masked Language Modeling (MLM)\n In order to train a deep bidirectional representation, they simply mask some percentage of the input tokens at random, and then predict those tokens. Specifically, 15% of all Word Piece tokens in each sequence are masked at random  Next sentence prediction (NSP)\n In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction. Specifically, 50% of the time, sentence B is the actual sentence that follows sentence.  results Ablation studies Effect of Pre-training Tasks   No NSP: A bidirectional model which is trained using the MLM but no next sentence prediction. This model shows to significantly hurt performance on QNLI, NMLI, and SQuAD 1.1\n  LTR and No NSP: A left-context-only model which is trained using a standard Left-to-Right LM rather than MLM. This model performs worst than the MLM model on all tasks, with large drops on MRPC and SQuAD .\n  Effect of Model Sizes   It has long been known that increasing the model size will lead to continual improvements on large-scale tasks.\n  We believe this is the first work to demonstrate convincingly that scaling to extreme model sizes also leads to large improvements on very small scale tasks.\n  Replication study of BERT pre training that includes the specific Modifications   Training the model for longer\n  Removing the Next sentence prediction objective\n  Training on longer sequences\n  Dynamically changing the masking patterns applied to the data\n  Training Procedure Analysis   Static vs Dynamic Masking:\nThe original BERT performed masking once during data preprocessing, resulting in a single static mask.\nWe compare this strategy with dynamic masking where we generate the masking pattern every time we feed a sequence\n   Model input format and Next Sentence Prediction:\nThe NSP loss was hypothesized to be and important factor in training the original BERT model, but recent work has questioned the necessity of the NSP loss.\nThey found that using individual sentences hurts performance on downstream tasks and that removing the NSP loss matches or slightly improves downstream task performance.\n  Training with larger batches: They observe that training with large batches improves perplexity for the masked language modeling objective, as well as end-task accuracy.  RoBERTA tests and results  The aggregate of the BERT improvements are combined to form RoBERTa for Robustly optimized BERT approach. Specifically, RoBERTA is trained with dynamic masking, FULL-SENTENCES without NSP loss, large mini-batches and a larger byte-level BPE.  Results  RoBERTa achieves state of the art results on all 9 of the GLUE task development sets Crucially, RoBERTa uses the same masked language modeling pretraining-objective and architecture as BERT large, yet outperforms it.   Reference: This post is mostly written by my friend Emilio Rogalla in the NLP reading group.\n BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: https://arxiv.org/pdf/1810.04805.pdf RoBERTa: A Robustly Optimized BERT Pretraining Approach: https://arxiv.org/pdf/1907.11692.pdf  ","wordCount":"618","inLanguage":"en","datePublished":"2020-09-18T00:00:00Z","dateModified":"2020-09-18T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://tangliyan.com/blog/posts/bert_roberta/"},"publisher":{"@type":"Organization","name":"Liyan Tang","logo":{"@type":"ImageObject","url":"https://tangliyan.com/blog/favicon.ico"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://tangliyan.com/blog/ accesskey=h title="Liyan Tang (Alt + H)">Liyan Tang</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
<li>
<a href=https://tangliyan.com/blog/archives title=Archive>
<span>Archive</span>
</a>
</li>
<li>
<a href=https://tangliyan.com/blog/tags/ title=Tags>
<span>Tags</span>
</a>
</li>
<li>
<a href=https://tangliyan.com/blog/search/ title="Search (Alt + /)" accesskey=/>
<span>Search</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<div class=breadcrumbs><a href=https://tangliyan.com/blog/>Home</a>&nbsp;»&nbsp;<a href=https://tangliyan.com/blog/posts/>Posts</a></div>
<h1 class=post-title>
BERT and RoBERTa
</h1>
<div class=post-meta><span title="2020-09-18 00:00:00 +0000 UTC">September 18, 2020</span>&nbsp;·&nbsp;3 min
</div>
</header> <div class=toc>
<details>
<summary accesskey=c title="(Alt + C)">
<span class=details>Table of Contents</span>
</summary>
<div class=inner><ul>
<li>
<a href=#bert-recap aria-label="BERT Recap">BERT Recap</a><ul>
<li>
<a href=#overview aria-label=Overview>Overview</a></li>
<li>
<a href=#bert-specifics aria-label="BERT Specifics">BERT Specifics</a><ul>
<li>
<a href=#there-are-two-steps-to-the-bert-framework-pre-training-and-fine-tuning aria-label="There are two steps to the BERT framework: pre-training and fine-tuning">There are two steps to the BERT framework: pre-training and fine-tuning</a></li></ul>
</li>
<li>
<a href=#input-output-representations aria-label="Input Output Representations">Input Output Representations</a></li>
<li>
<a href=#tasks aria-label=Tasks>Tasks</a></li>
<li>
<a href=#results aria-label=results>results</a></li>
<li>
<a href=#ablation-studies aria-label="Ablation studies">Ablation studies</a><ul>
<li>
<a href=#effect-of-pre-training-tasks aria-label="Effect of Pre-training Tasks">Effect of Pre-training Tasks</a></li>
<li>
<a href=#effect-of-model-sizes aria-label="Effect of Model Sizes">Effect of Model Sizes</a></li></ul>
</li>
<li>
<a href=#replication-study-of-bert-pre-training-that-includes-the-specific-modifications aria-label="Replication study of BERT pre training that includes the specific Modifications">Replication study of BERT pre training that includes the specific Modifications</a></li>
<li>
<a href=#training-procedure-analysis aria-label="Training Procedure Analysis">Training Procedure Analysis</a></li></ul>
</li>
<li>
<a href=#roberta-tests-and-results aria-label="RoBERTA tests and results">RoBERTA tests and results</a><ul>
<li>
<a href=#results-1 aria-label=Results>Results</a>
</li>
</ul>
</li>
</ul>
</div>
</details>
</div>
<div class=post-content><h2 id=bert-recap>BERT Recap<a hidden class=anchor aria-hidden=true href=#bert-recap>#</a></h2>
<h3 id=overview>Overview<a hidden class=anchor aria-hidden=true href=#overview>#</a></h3>
<ul>
<li>Bert (Bidirectional Encoder Representations from Transformers) uses a &ldquo;masked language model&rdquo; to randomly mask some tokens from the input and predict the original vocabulary id of the masked token.</li>
<li>Bert shows that &ldquo;pre-trained representations reduce the need for many heavily-engineered task-specific architectures&rdquo;.</li>
</ul>
<h3 id=bert-specifics>BERT Specifics<a hidden class=anchor aria-hidden=true href=#bert-specifics>#</a></h3>
<h4 id=there-are-two-steps-to-the-bert-framework-pre-training-and-fine-tuning>There are two steps to the BERT framework: pre-training and fine-tuning<a hidden class=anchor aria-hidden=true href=#there-are-two-steps-to-the-bert-framework-pre-training-and-fine-tuning>#</a></h4>
<ul>
<li>
<p>During pre training, the model is trained on unlabeled data over different pre-training tasks.</p>
</li>
<li>
<p>Each down stream task has separate fine-tuned models after each is first initialized with pre-trained parameters.</p>
</li>
</ul>
<img src="https://img-blog.csdnimg.cn/20200918115610836.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_1,color_FFFFFF,t_70#pic_center" width=700>
<h3 id=input-output-representations>Input Output Representations<a hidden class=anchor aria-hidden=true href=#input-output-representations>#</a></h3>
<ul>
<li>
<p>In order to handle a variety of down-stream tasks, the input must be able to represent a single sentence and sentence pair in one sequence.</p>
</li>
<li>
<p>The first token of every sequence is always a classification token <code>[CLS]</code>.</p>
</li>
<li>
<p>Sentence pairs are separated by a special token <code>[SEP]</code>.</p>
</li>
<li>
<p>Learned embeddings are added to every token indication whether it belongs to sentence A or sentence B.</p>
</li>
</ul>
<h3 id=tasks>Tasks<a hidden class=anchor aria-hidden=true href=#tasks>#</a></h3>
<p><strong>Masked Language Modeling (MLM)</strong></p>
<ul>
<li>In order to train a deep bidirectional representation, they simply mask some percentage of the input tokens at random, and then predict those tokens.</li>
<li>Specifically, <strong>15% of all Word Piece tokens</strong> in each sequence are masked at random</li>
</ul>
<p><strong>Next sentence prediction (NSP)</strong></p>
<ul>
<li>In order to train a model that understands sentence relationships, we pre-train for a binarized <em>next sentence prediction</em>.</li>
<li>Specifically, <strong>50% of the time, sentence B is the actual sentence that follows sentence</strong>.</li>
</ul>
<h3 id=results>results<a hidden class=anchor aria-hidden=true href=#results>#</a></h3>
<img src=https://img-blog.csdnimg.cn/20200918115657773.png#pic_center width=700>
<h3 id=ablation-studies>Ablation studies<a hidden class=anchor aria-hidden=true href=#ablation-studies>#</a></h3>
<h4 id=effect-of-pre-training-tasks>Effect of Pre-training Tasks<a hidden class=anchor aria-hidden=true href=#effect-of-pre-training-tasks>#</a></h4>
<ul>
<li>
<p>No NSP: A bidirectional model which is trained using the MLM but no next sentence prediction. This model shows to <strong>significantly hurt performance on QNLI, NMLI, and SQuAD 1.1</strong></p>
</li>
<li>
<p>LTR and No NSP: A left-context-only model which is trained using a standard Left-to-Right LM rather than MLM. This model <strong>performs worst than the MLM model on all tasks, with large drops on MRPC and SQuAD</strong> .</p>
</li>
</ul>
<img src="https://img-blog.csdnimg.cn/20200918115736941.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_1,color_FFFFFF,t_70#pic_center" width=400>
<h4 id=effect-of-model-sizes>Effect of Model Sizes<a hidden class=anchor aria-hidden=true href=#effect-of-model-sizes>#</a></h4>
<ul>
<li>
<p>It has long been known that increasing the model size will lead to continual improvements on large-scale tasks.</p>
</li>
<li>
<p>We believe this is the first work to demonstrate convincingly that scaling to extreme model sizes also leads to <strong>large improvements on very small scale tasks</strong>.</p>
</li>
</ul>
<img src=https://img-blog.csdnimg.cn/20200918115807192.png#pic_center width=400>
<h3 id=replication-study-of-bert-pre-training-that-includes-the-specific-modifications>Replication study of BERT pre training that includes the specific Modifications<a hidden class=anchor aria-hidden=true href=#replication-study-of-bert-pre-training-that-includes-the-specific-modifications>#</a></h3>
<ol>
<li>
<p>Training the model for longer</p>
</li>
<li>
<p>Removing the Next sentence prediction objective</p>
</li>
<li>
<p>Training on longer sequences</p>
</li>
<li>
<p>Dynamically changing the masking patterns applied to the data</p>
</li>
</ol>
<h3 id=training-procedure-analysis>Training Procedure Analysis<a hidden class=anchor aria-hidden=true href=#training-procedure-analysis>#</a></h3>
<ol>
<li>
<p>Static vs Dynamic Masking:</p>
<p>The original BERT performed masking once during data preprocessing, resulting in a single <em>static</em> mask.</p>
<p>We compare this strategy with <em>dynamic masking</em> where we generate the masking pattern every time we feed a sequence</p>
</li>
</ol>
<img src=https://img-blog.csdnimg.cn/20200918115850487.png#pic_center width=400>
<ol start=2>
<li>
<p>Model input format and Next Sentence Prediction:</p>
<p>The NSP loss was hypothesized to be and important factor in training the original BERT model, but recent work has questioned the necessity of the NSP loss.</p>
<p>They found that <strong>using individual sentences hurts performance on downstream tasks</strong> and that <strong>removing the NSP loss matches or slightly improves downstream task performance</strong>.</p>
</li>
</ol>
<img src="https://img-blog.csdnimg.cn/20200918115946155.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_16,color_FFFFFF,t_70#pic_center">
<ol start=3>
<li>Training with larger batches:
They observe that training with large batches improves perplexity for the masked language modeling objective, as well as end-task accuracy.</li>
</ol>
<img src=https://img-blog.csdnimg.cn/20200918120013611.png#pic_center>
<h2 id=roberta-tests-and-results>RoBERTA tests and results<a hidden class=anchor aria-hidden=true href=#roberta-tests-and-results>#</a></h2>
<ul>
<li>The aggregate of the BERT improvements are combined to form <strong>RoBERTa</strong> for Robustly optimized BERT approach.</li>
<li>Specifically, RoBERTA is trained with dynamic masking, FULL-SENTENCES without NSP loss, large mini-batches and a larger byte-level BPE.</li>
</ul>
<h3 id=results-1>Results<a hidden class=anchor aria-hidden=true href=#results-1>#</a></h3>
<ul>
<li>RoBERTa achieves state of the art results on all 9 of the GLUE task development sets</li>
<li>Crucially, RoBERTa uses the same masked language modeling pretraining-objective and architecture as BERT large, yet outperforms it.</li>
</ul>
<img src="https://img-blog.csdnimg.cn/20200918120045725.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_16,color_FFFFFF,t_70#pic_center" width=700>
<hr>
<p>Reference:
<strong>This post is mostly written by my friend Emilio Rogalla in the NLP reading group.</strong></p>
<ul>
<li>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: <a href=https://arxiv.org/pdf/1810.04805.pdf>https://arxiv.org/pdf/1810.04805.pdf</a></li>
<li>RoBERTa: A Robustly Optimized BERT Pretraining Approach: <a href=https://arxiv.org/pdf/1907.11692.pdf>https://arxiv.org/pdf/1907.11692.pdf</a></li>
</ul>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=https://tangliyan.com/blog/tags/nlp/>NLP</a></li>
</ul>
<nav class=paginav>
<a class=prev href=https://tangliyan.com/blog/posts/corss_lingual/>
<span class=title>« Prev Page</span>
<br>
<span>Cross-Lingual Learning</span>
</a>
<a class=next href=https://tangliyan.com/blog/posts/bert_attn/>
<span class=title>Next Page »</span>
<br>
<span>Paper Review - What Does BERT Look At? An Analysis of BERT’s Attention</span>
</a>
</nav>
<div class=share-buttons>
<a target=_blank rel="noopener noreferrer" aria-label="share BERT and RoBERTa  on twitter" href="https://twitter.com/intent/tweet/?text=BERT%20and%20RoBERTa%20&url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fbert_roberta%2f&hashtags=NLP"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share BERT and RoBERTa  on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fbert_roberta%2f&title=BERT%20and%20RoBERTa%20&summary=BERT%20and%20RoBERTa%20&source=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fbert_roberta%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share BERT and RoBERTa  on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fbert_roberta%2f&title=BERT%20and%20RoBERTa%20"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share BERT and RoBERTa  on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fbert_roberta%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share BERT and RoBERTa  on whatsapp" href="https://api.whatsapp.com/send?text=BERT%20and%20RoBERTa%20%20-%20https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fbert_roberta%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share BERT and RoBERTa  on telegram" href="https://telegram.me/share/url?text=BERT%20and%20RoBERTa%20&url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fbert_roberta%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a>
</div>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2022 <a href=https://tangliyan.com/blog/>Liyan Tang</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
<script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerText='copy';function d(){a.innerText='copied!',setTimeout(()=>{a.innerText='copy'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script>
</body>
</html>