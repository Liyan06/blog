<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Paper Review - Decoupling Representation and Classifier for Long-Tailed Recognition | Liyan Tang</title>
<meta name=keywords content="CV,PAPER">
<meta name=description content="Authors: Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, Yannis Kalantidis">
<meta name=author content>
<link rel=canonical href=https://tangliyan.com/blog/posts/decoupling_representation/>
<link crossorigin=anonymous href=/blog/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css integrity="sha256-yIlj/i15RiAA/Q+xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/blog/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://tangliyan.com/blog/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://tangliyan.com/blog/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://tangliyan.com/blog/favicon-32x32.png>
<link rel=apple-touch-icon href=https://tangliyan.com/blog/apple-touch-icon.png>
<link rel=mask-icon href=https://tangliyan.com/blog/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.92.0">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-202974782-1','auto'),ga('send','pageview'))</script><meta property="og:title" content="Paper Review - Decoupling Representation and Classifier for Long-Tailed Recognition">
<meta property="og:description" content="Authors: Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, Yannis Kalantidis">
<meta property="og:type" content="article">
<meta property="og:url" content="https://tangliyan.com/blog/posts/decoupling_representation/"><meta property="og:image" content="https://tangliyan.com/blog/papermod-cover.png"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2021-01-08T00:00:00+00:00">
<meta property="article:modified_time" content="2021-01-08T00:00:00+00:00">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://tangliyan.com/blog/papermod-cover.png">
<meta name=twitter:title content="Paper Review - Decoupling Representation and Classifier for Long-Tailed Recognition">
<meta name=twitter:description content="Authors: Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, Yannis Kalantidis">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://tangliyan.com/blog/posts/"},{"@type":"ListItem","position":2,"name":"Paper Review - Decoupling Representation and Classifier for Long-Tailed Recognition","item":"https://tangliyan.com/blog/posts/decoupling_representation/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Paper Review - Decoupling Representation and Classifier for Long-Tailed Recognition","name":"Paper Review - Decoupling Representation and Classifier for Long-Tailed Recognition","description":"Authors: Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, Yannis Kalantidis","keywords":["CV","PAPER"],"articleBody":"Authors: Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, Yannis Kalantidis Paper reference: https://arxiv.org/pdf/1910.09217.pdf\nIntroduction When learning with long-tailed data, a common challenge is that instance-rich (or head) classes dominate the training procedure. The learned classification model tends to perform better on these classes, while performance is significantly worse for instance-scarce (or tail) classes (under-fitting).\nThe general scheme for long-tailed recognition is: classifiers are either learned jointly with the representations end-to-end, or via a two-stage approach where the classifier and the representation are jointly fine-tuned with variants of class-balanced sampling as a second stage.\nIn our work, we argue for decoupling representation and classification. We demonstrate that in a long-tailed scenario, this separation allows straightforward approaches to achieve high recognition performance, without the need for designing sampling strategies, balance-aware losses or adding memory modules.\nRecent Directions Recent studies' directions on solving long-tailed recognition problem:\n Data distribution re-balancing. Re-sample the dataset to achieve a more balanced data distribution. These methods include over-sampling, down-sampling and class-balanced sampling. Class-balanced Losses. Assign different losses to different training samples for each class. Transfer learning from head- to tail classes. Transfer features learned from head classes with abundant training instances to under-represented tail classes. However it is usually a non-trivial task to design specific modules for feature transfer.  Sampling Strategies For most sampling strategies presented below, the probability $p_j$ of sampling a data point from class $j$ is given by: $$ p_{j}=\\frac{n_{j}^{q}}{\\sum_{i=1}^{C} n_{i}^{q}} $$ where $q \\in[0,1]$, $n_j$ denote the number of training sample for class $j$ and $C$ is the number of training classes. Different sampling strategies arise for different values of $q$ and below we present strategies that correspond to $q=1, q=0,$ and $q=1 / 2$.\n Instance-balanced Sampling. This is the most common way of sampling data, where each training example has equal probability of being selected. Here $q=1$. For imbalanced datasets, instance-balanced sampling has been shown to be sub-optimal as the model under-fits for few-shot classes leading to lower accuracy, especially for balanced test sets. Class-balanced Sampling. Class-balanced sampling has been used to alleviate the drawback of Instance-balanced Sampling, as each class has an equal probability of being selected. Square-root sampling. Set $q = 1/2$. Progressively-balanced sampling. Combinations of the sampling strategies presented above. In practice this involves first using instance-balanced sampling for a number of epochs, and then class-balanced sampling for the last epochs. Here, we experiment with a softer version, that progressively “interpolates” between instance-balanced and class-balanced sampling as learning progresses class $j$ is now a function of the epoch $t,$ $$p_{j}^{\\mathrm{PB}}(t)=\\left(1-\\frac{t}{T}\\right) p_{j}^{\\mathrm{IB}}+\\frac{t}{T} p_{j}^{\\mathrm{CB}} $$ where $T$ is the total number of epochs.  Methods of Learning Classifiers Classifier Re-training (cRT) Re-train the classifier with class-balanced sampling. That is, keeping the representations fixed, we randomly re-initialize and optimize the classifier weights $W$ and $b$ for a small number of epochs using class-balanced sampling.\nNearest Class Mean classifier (NCM) First compute the mean feature representation for each class on the training set and then perform nearest neighbor search either using cosine similarity or the Euclidean distance computed on L2 normalized mean features. (the cosine similarity alleviates the weight imbalance problem via its inherent normalization)\n$\\tau$-normalized classifier $(\\tau$-normalized) We investigate an efficient approach to re-balance the decision boundaries of classifiers, inspired by an empirical observation:\n Empirical Observation: after joint training with instance-balanced sampling, the norms of the weights $\\left|w_{j}\\right|$ are correlated with the cardinality of the classes $n_j$, while, after fine-tuning the classifiers using class-balanced sampling, the norms of the classifier weights tend to be more similar.\n Inspired by the above observations, we consider rectifying imbalance of decision boundaries by adjusting the classifier weight norms directly through the following $\\tau$-normalization procedure. Formally, let $\\boldsymbol{W} = {w_{j}} \\in \\mathbb{R}^{d \\times C},$ where $w_{j} \\in \\mathbb{R}^{d}$ are the classifier weights corresponding to class $j .$ We scale the weights of $\\boldsymbol{W}$ to get $\\widetilde{\\boldsymbol{W}}= {\\widetilde{w_{j}} }$ by: $$ \\widetilde{w_{i}}=\\frac{w_{i}}{\\left|w_{i}\\right|^{\\tau}} $$ where $\\tau$ is a hyper-parameter controlling the “temperature” of the normalization, and $|\\cdot|$ denotes the $L_{2}$ norm. When $\\tau=1$, it reduces to standard $L_{2}$ -normalization. When $\\tau=0$, no scaling is imposed. We empirically choose $\\tau \\in(0,1)$ such that the weights can be rectified smoothly.\nLearnable weight scaling (LWS). Another way of interpreting $\\tau$-normalization would be to think of it as a re-scaling of the magnitude for each classifier $w_{i}$ keeping the direction unchanged. This could be written as $$ \\widetilde{w_{i}}=f_{i} * w_{i}, \\text { where } f_{i}=\\frac{1}{\\left|w_{i}\\right|^{\\tau}} $$ Although for $\\tau$-normalized in general $\\tau$ is chosen through cross-validation.\nNote: NCM and $\\tau$-normalized cases give competitive performance even though they are free of additional training and involve no additional sampling procedure.\nExperiments We perform extensive experiments on three large-scale long-tailed datasets.\nDatasets  Places-LT. Artificially truncated from the balanced version. Places-LT contains images from 365 categories and the number of images per class ranges from 4980 to 5. ImageNet-LT. Artificially truncated from the balanced version. ImageNet-LT has 1000 classes and the number of images per class ranges from 1280 to 5 images. iNaturalist 2018. iNaturalist 2018 is a real-world, naturally long-tailed dataset, consisting of samples from 8,142 species.  Evaluation Protocol To better examine performance variations across classes with different number of examples seen during training, we report accuracy on three splits of the set of classes: Many-shot (more than 100 images), Medium-shot (20∼100 images) and Few-shot (less than 20 images).\nThis figure compares different sampling strategies for the conventional joint training scheme to a number of variations of the decoupled learning scheme on the ImageNet-LT dataset.\nResults Sampling matters when training jointly  For Joint Learning, we see consistent gains in performance when using better sampling strategies. The trends are consistent for the overall performance as well as the medium- and few- shot classes, with progressively-balanced sampling giving the best results. Instance-balanced sampling gives the highest performance for the many-shot classes. This is well expected since the resulted model is highly skewed to the many-shot classes.  Instance-balanced sampling generalize well Among all decoupled methods, we see that Instance-balanced sampling gives the best results. This is particularly interesting, as it implies that data imbalance might not be an issue learning high-quality representations.\nDecoupled Learning strategy helps  For most cases, performance using decoupled methods is significantly better in terms of overall performance, as well as all splits apart from the many-shot case.  To further justify our claim that it is beneficial to decouple representation and classifier, we experiment with fine-tuning the backbone network (ResNeXt-50) jointly with the linear classifier. Here is the result: Fine-tuning the whole network yields the worst performance, while keeping the representation frozen performs best. This result suggests that decoupling representation and classifier is desirable for long-tailed recognition.\nWeight Norm Visualization This figure shows L2 norms of the weight vectors for all classifiers, as well as the training data distribution sorted in a descending manner with respect to the number of instances in the training set.\nCompare to SOTA We compare the performance of the decoupled schemes to other recent works that report state-of-the-art results on on three common long-tailed benchmarks. This is the result for ImageNet-LT.\nContributions  Instance-balanced sampling gives more generalizable representations that can achieve state-of-the-art performance after properly re-balancing the classifiers and without need of carefully designed losses or memory units. It is advantageous in long-tailed recognition to re-adjust the decision boundaries specified by the jointly learned classifier during representation learning (NCM, cRT, $\\tau$-normalized). By applying the decoupled learning scheme to standard networks, we achieve significantly higher accuracy than well established state-of-the-art methods on multiple long- tailed recognition benchmark datasets.   Reference:\n Decoupling Representation and Classifier for Long-Tailed Recognition. https://arxiv.org/abs/1910.09217.  ","wordCount":"1258","inLanguage":"en","datePublished":"2021-01-08T00:00:00Z","dateModified":"2021-01-08T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://tangliyan.com/blog/posts/decoupling_representation/"},"publisher":{"@type":"Organization","name":"Liyan Tang","logo":{"@type":"ImageObject","url":"https://tangliyan.com/blog/favicon.ico"}}}</script><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']]},svg:{fontCache:'global'}}</script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://tangliyan.com/blog/ accesskey=h title="Liyan Tang (Alt + H)">Liyan Tang</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
<li>
<a href=https://tangliyan.com/blog/archives title=Archive>
<span>Archive</span>
</a>
</li>
<li>
<a href=https://tangliyan.com/blog/tags/ title=Tags>
<span>Tags</span>
</a>
</li>
<li>
<a href=https://tangliyan.com/blog/search/ title="Search (Alt + /)" accesskey=/>
<span>Search</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<div class=breadcrumbs><a href=https://tangliyan.com/blog/>Home</a>&nbsp;»&nbsp;<a href=https://tangliyan.com/blog/posts/>Posts</a></div>
<h1 class=post-title>
Paper Review - Decoupling Representation and Classifier for Long-Tailed Recognition
</h1>
<div class=post-meta><span title="2021-01-08 00:00:00 +0000 UTC">January 8, 2021</span>&nbsp;·&nbsp;6 min
</div>
</header> <div class=toc>
<details>
<summary accesskey=c title="(Alt + C)">
<span class=details>Table of Contents</span>
</summary>
<div class=inner><ul>
<li>
<a href=#introduction aria-label=Introduction>Introduction</a></li>
<li>
<a href=#recent-directions aria-label="Recent Directions">Recent Directions</a></li>
<li>
<a href=#sampling-strategies aria-label="Sampling Strategies">Sampling Strategies</a></li>
<li>
<a href=#methods-of-learning-classifiers aria-label="Methods of Learning Classifiers">Methods of Learning Classifiers</a><ul>
<li>
<a href=#classifier-re-training-crt aria-label="Classifier Re-training (cRT)">Classifier Re-training (cRT)</a></li>
<li>
<a href=#nearest-class-mean-classifier-ncm aria-label="Nearest Class Mean classifier (NCM)">Nearest Class Mean classifier (NCM)</a></li>
<li>
<a href=#tau-normalized-classifier-tau-normalized aria-label="$\tau$-normalized classifier $(\tau$-normalized)">$\tau$-normalized classifier $(\tau$-normalized)</a></li></ul>
</li>
<li>
<a href=#experiments aria-label=Experiments>Experiments</a><ul>
<li>
<a href=#datasets aria-label=Datasets>Datasets</a></li>
<li>
<a href=#evaluation-protocol aria-label="Evaluation Protocol">Evaluation Protocol</a></li>
<li>
<a href=#results aria-label=Results>Results</a><ul>
<li>
<a href=#sampling-matters-when-training-jointly aria-label="Sampling matters when training jointly">Sampling matters when training jointly</a></li>
<li>
<a href=#instance-balanced-sampling-generalize-well aria-label="Instance-balanced sampling generalize well">Instance-balanced sampling generalize well</a></li>
<li>
<a href=#decoupled-learning-strategy-helps aria-label="Decoupled Learning strategy helps">Decoupled Learning strategy helps</a></li>
<li>
<a href=#weight-norm-visualization aria-label="Weight Norm Visualization">Weight Norm Visualization</a></li></ul>
</li>
<li>
<a href=#compare-to-sota aria-label="Compare to SOTA">Compare to SOTA</a></li></ul>
</li>
<li>
<a href=#contributions aria-label=Contributions>Contributions</a>
</li>
</ul>
</div>
</details>
</div>
<div class=post-content><p>Authors: Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, Yannis Kalantidis <br>
Paper reference: <a href=https://arxiv.org/pdf/1910.09217.pdf>https://arxiv.org/pdf/1910.09217.pdf</a></p>
<h3 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h3>
<img src="https://img-blog.csdnimg.cn/20210108111505689.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_16,color_FFFFFF,t_70#pic_center" width=550>
<p>When learning with long-tailed data, a common challenge is that instance-rich (or head) classes dominate the training procedure. The learned classification model tends to perform better on these classes, while performance is significantly worse for instance-scarce (or tail) classes (under-fitting).</p>
<p>The general scheme for long-tailed recognition is: classifiers are either learned jointly with the representations end-to-end, or via a two-stage approach where the classifier and the representation are jointly fine-tuned with variants of class-balanced sampling as a second stage.</p>
<p>In our work, we argue for <strong>decoupling representation</strong> and classification. We demonstrate that in a long-tailed scenario, this separation allows straightforward approaches to achieve high recognition performance, without the need for designing sampling strategies, balance-aware losses or adding memory modules.</p>
<h3 id=recent-directions>Recent Directions<a hidden class=anchor aria-hidden=true href=#recent-directions>#</a></h3>
<p>Recent studies' directions on solving long-tailed recognition problem:</p>
<ul>
<li><strong>Data distribution re-balancing</strong>. Re-sample the dataset to achieve a more balanced data distribution. These methods include <strong>over-sampling, down-sampling and class-balanced sampling</strong>.</li>
<li><strong>Class-balanced Losses.</strong> Assign different losses to different training samples for each class.</li>
<li><strong>Transfer learning from head- to tail classes.</strong> Transfer features learned from head classes with abundant training instances to under-represented tail classes. However it is usually a non-trivial task to design specific modules for feature transfer.</li>
</ul>
<h3 id=sampling-strategies>Sampling Strategies<a hidden class=anchor aria-hidden=true href=#sampling-strategies>#</a></h3>
<p>For most sampling strategies presented below, the probability $p_j$ of sampling a data point from class $j$ is given by: $$ p_{j}=\frac{n_{j}^{q}}{\sum_{i=1}^{C} n_{i}^{q}} $$ where $q \in[0,1]$, $n_j$ denote the number of training sample for class $j$ and $C$ is the number of training classes. Different sampling strategies arise for different values of $q$ and below we present strategies that correspond to $q=1, q=0,$ and $q=1 / 2$.</p>
<ul>
<li><strong>Instance-balanced Sampling.</strong> This is the most common way of sampling data, where each training example has equal probability of being selected. Here $q=1$. For imbalanced datasets, instance-balanced sampling has been shown to be sub-optimal as the model under-fits for few-shot classes leading to lower accuracy, especially for balanced test sets.</li>
<li><strong>Class-balanced Sampling.</strong> Class-balanced sampling has been used to alleviate the drawback of Instance-balanced Sampling, as each class has an equal probability of being selected.</li>
<li><strong>Square-root sampling</strong>. Set $q = 1/2$.</li>
<li><strong>Progressively-balanced sampling.</strong> Combinations of the sampling strategies presented above. In practice this involves first using instance-balanced sampling for a number of epochs, and then class-balanced sampling for the last epochs. Here, we experiment with a softer version, that progressively &ldquo;interpolates&rdquo; between instance-balanced and class-balanced sampling as learning progresses class $j$ is now a function of the epoch $t,$
$$p_{j}^{\mathrm{PB}}(t)=\left(1-\frac{t}{T}\right) p_{j}^{\mathrm{IB}}+\frac{t}{T} p_{j}^{\mathrm{CB}} $$ where $T$ is the total number of epochs.</li>
</ul>
<h3 id=methods-of-learning-classifiers>Methods of Learning Classifiers<a hidden class=anchor aria-hidden=true href=#methods-of-learning-classifiers>#</a></h3>
<h4 id=classifier-re-training-crt>Classifier Re-training (cRT)<a hidden class=anchor aria-hidden=true href=#classifier-re-training-crt>#</a></h4>
<p>Re-train the classifier with class-balanced sampling. That is, keeping the representations fixed, we randomly re-initialize and optimize the classifier weights $W$ and $b$ for a small number of epochs using class-balanced sampling.</p>
<h4 id=nearest-class-mean-classifier-ncm>Nearest Class Mean classifier (NCM)<a hidden class=anchor aria-hidden=true href=#nearest-class-mean-classifier-ncm>#</a></h4>
<p>First compute the mean feature representation for each class on the training set and then perform nearest neighbor search either using cosine similarity or the Euclidean distance computed on L2 normalized mean features. (the cosine similarity alleviates the weight imbalance problem via its inherent normalization)</p>
<h4 id=tau-normalized-classifier-tau-normalized>$\tau$-normalized classifier $(\tau$-normalized)<a hidden class=anchor aria-hidden=true href=#tau-normalized-classifier-tau-normalized>#</a></h4>
<p>We investigate an efficient approach to re-balance the decision boundaries of classifiers, inspired by an empirical observation:</p>
<blockquote>
<p>Empirical Observation: after joint training with instance-balanced sampling, the norms of the weights $\left|w_{j}\right|$ are correlated with the cardinality of the classes $n_j$, while, after fine-tuning the classifiers using class-balanced sampling, the norms of the classifier weights tend to be more similar.</p>
</blockquote>
<p>Inspired by the above observations, we consider rectifying imbalance of decision boundaries by adjusting the classifier weight norms directly through the following $\tau$-normalization procedure. Formally, let $\boldsymbol{W} = {w_{j}} \in \mathbb{R}^{d \times C},$ where $w_{j} \in \mathbb{R}^{d}$ are the classifier weights corresponding to class $j .$ We scale the weights of $\boldsymbol{W}$ to get $\widetilde{\boldsymbol{W}}= {\widetilde{w_{j}} }$ by: $$ \widetilde{w_{i}}=\frac{w_{i}}{\left|w_{i}\right|^{\tau}} $$ where $\tau$ is a hyper-parameter controlling the &ldquo;temperature&rdquo; of the normalization, and $|\cdot|$ denotes the $L_{2}$ norm. When $\tau=1$, it reduces to standard $L_{2}$ -normalization. When $\tau=0$, no scaling is imposed. We empirically choose $\tau \in(0,1)$ such that the weights can be rectified smoothly.</p>
<p><strong>Learnable weight scaling (LWS).</strong> Another way of interpreting $\tau$-normalization would be to think of it as a re-scaling of the magnitude for each classifier $w_{i}$ keeping the direction unchanged. This could be written as $$ \widetilde{w_{i}}=f_{i} * w_{i}, \text { where } f_{i}=\frac{1}{\left|w_{i}\right|^{\tau}} $$ Although for $\tau$-normalized in general $\tau$ is chosen through cross-validation.</p>
<p>Note: NCM and $\tau$-normalized cases give competitive performance even though they are free of additional training and involve no additional sampling procedure.</p>
<h3 id=experiments>Experiments<a hidden class=anchor aria-hidden=true href=#experiments>#</a></h3>
<p>We perform extensive experiments on three large-scale long-tailed datasets.</p>
<h4 id=datasets>Datasets<a hidden class=anchor aria-hidden=true href=#datasets>#</a></h4>
<ul>
<li><strong>Places-LT.</strong> Artificially truncated from the balanced version. Places-LT contains images from 365 categories and the number of images per class ranges from 4980 to 5.</li>
<li><strong>ImageNet-LT.</strong> Artificially truncated from the balanced version. ImageNet-LT has 1000 classes and the number of images per class ranges from 1280 to 5 images.</li>
<li><strong>iNaturalist 2018.</strong> iNaturalist 2018 is a real-world, naturally long-tailed dataset, consisting of samples from 8,142 species.</li>
</ul>
<h4 id=evaluation-protocol>Evaluation Protocol<a hidden class=anchor aria-hidden=true href=#evaluation-protocol>#</a></h4>
<p>To better examine performance variations across classes with different number of examples seen during training, we report accuracy on three splits of the set of classes: Many-shot (more than 100 images), Medium-shot (20∼100 images) and Few-shot (less than 20 images).</p>
<p>This figure compares different sampling strategies for the conventional joint training scheme to a number of variations of the decoupled learning scheme on the ImageNet-LT dataset.</p>
<p><img loading=lazy src="https://img-blog.csdnimg.cn/20210108063431814.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_16,color_FFFFFF,t_70#pic_center" alt=在这里插入图片描述>
</p>
<h4 id=results>Results<a hidden class=anchor aria-hidden=true href=#results>#</a></h4>
<h5 id=sampling-matters-when-training-jointly>Sampling matters when training jointly<a hidden class=anchor aria-hidden=true href=#sampling-matters-when-training-jointly>#</a></h5>
<ul>
<li>For Joint Learning, we see consistent gains in performance when using better sampling strategies.</li>
<li>The trends are consistent for the overall performance as well as the medium- and few- shot classes, with progressively-balanced sampling giving the best results.</li>
<li>Instance-balanced sampling gives the highest performance for the many-shot classes. This is well expected since the resulted model is highly skewed to the many-shot classes.</li>
</ul>
<h5 id=instance-balanced-sampling-generalize-well>Instance-balanced sampling generalize well<a hidden class=anchor aria-hidden=true href=#instance-balanced-sampling-generalize-well>#</a></h5>
<p>Among all decoupled methods, we see that Instance-balanced sampling gives the best results. This is particularly interesting, as it implies that <strong>data imbalance might not be an issue learning high-quality representations.</strong></p>
<h5 id=decoupled-learning-strategy-helps>Decoupled Learning strategy helps<a hidden class=anchor aria-hidden=true href=#decoupled-learning-strategy-helps>#</a></h5>
<ul>
<li>For most cases, performance using decoupled methods is significantly better in terms of overall performance, as well as all splits apart from the many-shot case.</li>
</ul>
<p>To further justify our claim that it is beneficial to decouple representation and classifier, we experiment with fine-tuning the backbone network (ResNeXt-50) jointly with the linear classifier. Here is the result:
<img src="https://img-blog.csdnimg.cn/20210108065947635.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_16,color_FFFFFF,t_70#pic_center" width=550></p>
<p>Fine-tuning the whole network yields the worst performance, while keeping the representation frozen performs best. This result suggests that decoupling representation and classifier is desirable for long-tailed recognition.</p>
<h5 id=weight-norm-visualization>Weight Norm Visualization<a hidden class=anchor aria-hidden=true href=#weight-norm-visualization>#</a></h5>
<img src="https://img-blog.csdnimg.cn/20210108072637317.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_16,color_FFFFFF,t_70#pic_center" width=500>
<p>This figure shows L2 norms of the weight vectors for all classifiers, as well as the training data distribution sorted in a descending manner with respect to the number of instances in the training set.</p>
<h4 id=compare-to-sota>Compare to SOTA<a hidden class=anchor aria-hidden=true href=#compare-to-sota>#</a></h4>
<p>We compare the performance of the decoupled schemes to other recent works that report state-of-the-art results on on three common long-tailed benchmarks. This is the result for ImageNet-LT.</p>
<img src="https://img-blog.csdnimg.cn/20210108073239960.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_16,color_FFFFFF,t_70#pic_center" width=650>
<h3 id=contributions>Contributions<a hidden class=anchor aria-hidden=true href=#contributions>#</a></h3>
<ul>
<li>Instance-balanced sampling gives more generalizable representations that can achieve state-of-the-art performance after properly re-balancing the classifiers and without need of carefully designed losses or memory units.</li>
<li>It is advantageous in long-tailed recognition to re-adjust the decision boundaries specified by the jointly learned classifier during representation learning (NCM, cRT, $\tau$-normalized).</li>
<li>By applying the decoupled learning scheme to standard networks, we achieve significantly higher accuracy than well established state-of-the-art methods on multiple long- tailed recognition benchmark datasets.</li>
</ul>
<hr>
<p>Reference:</p>
<ul>
<li>Decoupling Representation and Classifier for Long-Tailed Recognition. <a href=https://arxiv.org/abs/1910.09217>https://arxiv.org/abs/1910.09217</a>.</li>
</ul>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=https://tangliyan.com/blog/tags/cv/>CV</a></li>
<li><a href=https://tangliyan.com/blog/tags/paper/>PAPER</a></li>
</ul>
<nav class=paginav>
<a class=prev href=https://tangliyan.com/blog/posts/improving_zero_and/>
<span class=title>« Prev Page</span>
<br>
<span>Paper Review - Improving Zero and Few-Shot Abstractive Summarization with Intermediate Fine-tuning and Data Augmentation</span>
</a>
<a class=next href=https://tangliyan.com/blog/posts/relation_extraction/>
<span class=title>Next Page »</span>
<br>
<span>Overlook of Relation Extraction</span>
</a>
</nav>
<div class=share-buttons>
<a target=_blank rel="noopener noreferrer" aria-label="share Paper Review - Decoupling Representation and Classifier for Long-Tailed Recognition on twitter" href="https://twitter.com/intent/tweet/?text=Paper%20Review%20-%20Decoupling%20Representation%20and%20Classifier%20for%20Long-Tailed%20Recognition&url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fdecoupling_representation%2f&hashtags=CV%2cPAPER"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Paper Review - Decoupling Representation and Classifier for Long-Tailed Recognition on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fdecoupling_representation%2f&title=Paper%20Review%20-%20Decoupling%20Representation%20and%20Classifier%20for%20Long-Tailed%20Recognition&summary=Paper%20Review%20-%20Decoupling%20Representation%20and%20Classifier%20for%20Long-Tailed%20Recognition&source=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fdecoupling_representation%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Paper Review - Decoupling Representation and Classifier for Long-Tailed Recognition on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fdecoupling_representation%2f&title=Paper%20Review%20-%20Decoupling%20Representation%20and%20Classifier%20for%20Long-Tailed%20Recognition"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Paper Review - Decoupling Representation and Classifier for Long-Tailed Recognition on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fdecoupling_representation%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Paper Review - Decoupling Representation and Classifier for Long-Tailed Recognition on whatsapp" href="https://api.whatsapp.com/send?text=Paper%20Review%20-%20Decoupling%20Representation%20and%20Classifier%20for%20Long-Tailed%20Recognition%20-%20https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fdecoupling_representation%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Paper Review - Decoupling Representation and Classifier for Long-Tailed Recognition on telegram" href="https://telegram.me/share/url?text=Paper%20Review%20-%20Decoupling%20Representation%20and%20Classifier%20for%20Long-Tailed%20Recognition&url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fdecoupling_representation%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a>
</div>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2022 <a href=https://tangliyan.com/blog/>Liyan Tang</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
<script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerText='copy';function d(){a.innerText='copied!',setTimeout(()=>{a.innerText='copy'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script>
</body>
</html>