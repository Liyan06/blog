<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Graph Convolutional Neural Network - Spatial Convolution | Liyan Tang</title>
<meta name=keywords content="GRAPH,MATH">
<meta name=description content="Note This is the second post of the Graph Neural Networks (GNNs) series.
Convolutional graph neural networks (ConvGNNs) Convolutional graph neural networks (ConvGNNs) generalize the operation of convolution from grid data to graph data. The main idea is to generate a node $v$’s representation by aggregating its own features $\mathbf{x}_{v}$ and neighbors’ features $\mathbf{x}_{u}$, where $u \in N(v)$. Different from RecGNNs, ConvGNNs stack fixed number of multiple graph convolutional layers with different weights to extract high-level node representations.">
<meta name=author content>
<link rel=canonical href=https://tangliyan.com/blog/posts/spatial_conv/>
<link crossorigin=anonymous href=/blog/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css integrity="sha256-yIlj/i15RiAA/Q+xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/blog/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://tangliyan.com/blog/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://tangliyan.com/blog/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://tangliyan.com/blog/favicon-32x32.png>
<link rel=apple-touch-icon href=https://tangliyan.com/blog/apple-touch-icon.png>
<link rel=mask-icon href=https://tangliyan.com/blog/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.92.0">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-202974782-1','auto'),ga('send','pageview'))</script><meta property="og:title" content="Graph Convolutional Neural Network - Spatial Convolution">
<meta property="og:description" content="Note This is the second post of the Graph Neural Networks (GNNs) series.
Convolutional graph neural networks (ConvGNNs) Convolutional graph neural networks (ConvGNNs) generalize the operation of convolution from grid data to graph data. The main idea is to generate a node $v$’s representation by aggregating its own features $\mathbf{x}_{v}$ and neighbors’ features $\mathbf{x}_{u}$, where $u \in N(v)$. Different from RecGNNs, ConvGNNs stack fixed number of multiple graph convolutional layers with different weights to extract high-level node representations.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://tangliyan.com/blog/posts/spatial_conv/"><meta property="og:image" content="https://tangliyan.com/blog/papermod-cover.png"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2020-08-20T00:00:00+00:00">
<meta property="article:modified_time" content="2020-08-20T00:00:00+00:00">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://tangliyan.com/blog/papermod-cover.png">
<meta name=twitter:title content="Graph Convolutional Neural Network - Spatial Convolution">
<meta name=twitter:description content="Note This is the second post of the Graph Neural Networks (GNNs) series.
Convolutional graph neural networks (ConvGNNs) Convolutional graph neural networks (ConvGNNs) generalize the operation of convolution from grid data to graph data. The main idea is to generate a node $v$’s representation by aggregating its own features $\mathbf{x}_{v}$ and neighbors’ features $\mathbf{x}_{u}$, where $u \in N(v)$. Different from RecGNNs, ConvGNNs stack fixed number of multiple graph convolutional layers with different weights to extract high-level node representations.">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://tangliyan.com/blog/posts/"},{"@type":"ListItem","position":2,"name":"Graph Convolutional Neural Network - Spatial Convolution","item":"https://tangliyan.com/blog/posts/spatial_conv/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Graph Convolutional Neural Network - Spatial Convolution","name":"Graph Convolutional Neural Network - Spatial Convolution","description":"Note This is the second post of the Graph Neural Networks (GNNs) series.\nConvolutional graph neural networks (ConvGNNs) Convolutional graph neural networks (ConvGNNs) generalize the operation of convolution from grid data to graph data. The main idea is to generate a node $v$’s representation by aggregating its own features $\\mathbf{x}_{v}$ and neighbors’ features $\\mathbf{x}_{u}$, where $u \\in N(v)$. Different from RecGNNs, ConvGNNs stack fixed number of multiple graph convolutional layers with different weights to extract high-level node representations.","keywords":["GRAPH","MATH"],"articleBody":"Note This is the second post of the Graph Neural Networks (GNNs) series.\nConvolutional graph neural networks (ConvGNNs) Convolutional graph neural networks (ConvGNNs) generalize the operation of convolution from grid data to graph data. The main idea is to generate a node $v$’s representation by aggregating its own features $\\mathbf{x}_{v}$ and neighbors’ features $\\mathbf{x}_{u}$, where $u \\in N(v)$. Different from RecGNNs, ConvGNNs stack fixed number of multiple graph convolutional layers with different weights to extract high-level node representations.\nConvGNNs fall into two categories:\n  spatial-based GCN: Spatial-based approaches inherit ideas from RecGNNs to define graph convolutions by information propagation.\n  spectral-based GCN: Spectral based approaches define graph convolutions by introducing filters from the perspective of graph signal processing where the graph convolutional operation is interpreted as removing noises from graph signals.\n  Spatial-based methods have developed rapidly recently due to its attractive efficiency, flexibility, and generality. In this post, we mainly focus on spatial-based GCN and leave spectral-based GCN to the next post. Let’s get started.\nGCN Framework As shown in the figure above, the input of GCN is the entire graph. In each convolution layer, a convolution operation is performed on the neighbors of each node, and the center node representation is updated with the result of the convolution. Then an activation function such as ReLU is used before going through the next layer of convolution layer. The above process continues until the number of layers reaches the expected depth (a hyper-parameter).\nGCN v.s. RecGNN The main difference between GCN and RecGNN is that each convolutional layer of GCN has unique weights, and, on the other hand, in RecGNN the weights of each layer are shared.\nWhat is Convolution In mathematics, convolution is a mathematical operation on two functions $f$ and $g$ that produces a third function $(f∗g)$ expressing how the shape of one is modified by the other.\nThe term convolution is defined as the integral of the product of the two functions after one is reversed and shifted. The mathematical definition is the following:\n$$ \\begin{array}{c} (f * g)(t)=\\int_{-\\infty}^{\\infty} f(\\tau) g(t-\\tau) \\quad (\\text {continuous}) \\\\ (f * g)(t)=\\sum_{\\tau=-\\infty}^{\\infty} f(\\tau) g(t-\\tau) \\quad(\\text {discrete}) \\end{array} $$\nThe convolution formula can be described as a weighted average of the function $f(\\tau)$ at the moment $t$ where the weighting is given by $g(-\\tau)$ simply shifted by amount $t$. As $t$ changes, the weighting function emphasizes different parts of the input function.\nAs the figure shown above, the filter is moved over by one pixel and this process is repeated until all of the possible locations in the image are filtered. At each step, the convolution takes the weighted average of pixel values of the center pixel along with its neighbors. Since the center pixel is changing at each time step, the convolution is emphasizing on different parts of the image.\nSpatial-based ConvGNNs Analogous to the convolutional operation of a conventional CNN on an image, spatial-based methods define graph convolutions based on a node’s spatial relations.\nImages can be considered as a special form of graph with each pixel representing a node. Each pixel is directly connected to its nearby pixels, as illustrated in the figure above (left). A filter is applied to a $3 \\times 3$ patch by taking the weighted average of pixel values of the central node and its neighbors across each channel.\nSimilarly, the spatial-based graph convolutions convolve the central node’s representation with its neighbors’ representations to derive the updated representation for the central node. From another perspective, spatial-based ConvGNNs share the same idea of information propagation/message passing with RecGNNs. The spatial graph convolutional operation essentially propagates node information along edges.\nMessage Passing Neural Network (MPNN) Introduction to MPNN Message Passing Neural Network (MPNN) outlines a general framework of spatial-based ConvGNNs (It is not actually a model). It treats graph convolutions as a message passing process in which information can be passed from one node to another along edges directly. MPNN runs K-step message passing iterations to let information propagate further. The message passing function (namely the spatial graph convolution) is defined as\n$$ \\mathbf{h}_{v}^{(k)}=U_{k}\\left(\\mathbf{h}_{v}^{(k-1)}, \\sum_{u \\in N(v)} M_{k}\\left(\\mathbf{h}_{v}^{(k-1)}, \\mathbf{h}_{u}^{(k-1)}, \\mathbf{x}_{(v,u)}\\right)\\right) \\tag 1 $$\nor we can write it separately as:\n$$ \\begin{array} \\mathbf{m}_v^{k} = \\sum_{u \\in N(v)} M_{k}\\left(\\mathbf{h}_{v}^{(k-1)}, \\mathbf{h}_{u}^{(k-1)}, \\mathbf{x}_{(v,u)}\\right) \\ \\mathbf{h}_{v}^{(k)}=U_{k}\\left(\\mathbf{h}_{v}^{(k-1)}, \\mathbf{m}_v^{k}\\right) \\tag 2\\ \\end{array} $$\nNote:\n $\\mathbf{m}_v^{k}$ represents messages at node $v$ at time step $k$; $\\mathbf{h}_{v}^{(0)}=\\mathbf{x}_{v}$; $U_{k}(\\cdot)$ and $M_{k}(\\cdot)$ are functions with learnable parameters. From (2), we see that MPNN has two main steps: information passing and state update. $M_{k}(\\cdot)$ is the function for information passing and $U_{k}(\\cdot)$ is the function for state update. The formula can be interpreted as each node first collects information from neighbors and then updates its own hidden state.  For example, here is an illustration of updating node $A$:\nAfter deriving the hidden representations of each node, $\\mathbf{h}_{v}^{(K)}$ can be passed to an output layer to perform node-level prediction tasks or to a readout function to perform graph-level prediction tasks. The readout function generates a representation of the entire graph based on node hidden representations (I will talk about readout function in a later post). It is generally defined as $$ \\mathbf{h}_{G}=R\\left(\\mathbf{h}_{v}^{(K)} \\mid v \\in G\\right) $$ where $R(\\cdot)$ represents the readout function with learnable parameters, and $\\mathbf{h}_{G}$ is a representation of the entire graph.\nNote that MPNN can cover many existing GNNs by assuming different forms of $U_{k}(\\cdot), M_{k}(\\cdot),$ and $R(\\cdot)$.\nShortage of the MPNN framework The shortage of MPNN is that it convolves on the entire graph, which means that all nodes must be put into the memory to perform the convolution operation. But for large-scale graph, the convolution operation on the entire graph is not realistic. In the next section, I’m going to introduce a model (GraphSAGE) to overcome this problem.\nGraphSAGE (SAmple and aggreGatE) Overview of GraphSAGE As the number of neighbors of a node can vary from one to a thousand or even more, it is inefficient to take the full size of a node’s neighborhood. GraphSage adopts sampling to obtain a fixed number of neighbors for each node. It performs graph convolutions by\n$$ \\mathbf{h}_{v}^{(k)}=\\sigma(\\mathbf{W}^{(k)} \\cdot f_{k}\\left(\\mathbf{h}_{v}^{(k-1)},\\left{\\mathbf{h}_{u}^{(k-1)}, \\forall u \\in S_{\\mathcal{N}(v)}\\right}\\right) $$\nwhere $\\mathbf{h}_{v}^{(0)}=\\mathbf{x}_{v}, f_{k}(\\cdot)$ is an aggregation function, $S_{\\mathcal{N}(v)}$ is a random sample of the node $v$ ’s neighbors. The aggregation function should be invariant to the permutations of node orderings such as a mean, sum or max function.\nSpecifically, the sampling process in GraphSage is divided into three steps (corespond to the figure above):\n  First, several nodes are randomly sampled in the graph. Then for each node, they uniformly sample a fixed-size set of neighbors, instead of using full neighborhood sets. These fixed-size of neighbors are uniformly drawn from all possible neighbors, and they draw different uniform samples at each iteration. Here, neighbors are not necessarily first-order neighbors, but also second-order.\n  Update sampled node’s information by aggregating the information of neighbor nodes through some aggregate functions.\n  Calculate the loss at the sampled node. If it is an unsupervised task, we hope that the neighborhood nodes on the graph share similar node representations; if it is a supervised task, we can calculate the loss based on the task label of the specific node.\n  Aggregator Fuctions Ideally, an aggregator function would be invariant to permutations of its inputs. This property of the aggregation function ensures that a neural network model canbe trained and applied to arbitrarily ordered node neighborhood feature sets. In the paper, they examed three aggregation functions: Mean Aggregator, LSTM Aggretator, Pooling Aggretator.\nMean Aggregator: Their first examed aggregator function is the mean operator, where we simply take the elementwise mean of the vectors in $\\left{\\mathbf{h}{u}^{(k-1)}, \\forall u \\in S{\\mathcal{N}(v)}\\right}$. That is:\n$$ \\mathbf{h}_{v}^{(k)}=\\sigma(\\mathbf{W}^{(k)} \\cdot \\text{MEAN}\\left({\\mathbf{h}_{v}^{(k-1)}} \\cup \\left{\\mathbf{h}_{u}^{(k-1)}, \\forall u \\in S_{\\mathcal{N}(v)}\\right}\\right) $$\nLSTM Aggregator: Compared to the mean aggregator, LSTMs have the advantage of larger expressive capability. However, it is important to note that LSTMs are not permutation invariant, since they process their inputs in a sequential manner. They adapt LSTMs to operate on an unordered set by simply applying the LSTMs to a random permutation of the node’s neighbors.\nPooling Aggregator: each neighbor’s vector is independently fed through a fully-connected neural network; following this transformation, an elementwise max-pooling operation is applied to aggregate information across the neighbor set:\n$$ \\text {AGGREGATE}_{k}^{\\text {pool}}=\\max \\left({\\sigma\\left(\\mathbf{W}_{\\text {pool }} \\mathbf{h}_{u}^{(k)}+\\mathbf{b}\\right), \\forall u \\in S_{\\mathcal{N}(v)}}\\right) $$\nwhere $\\max$ denotes the element-wise max operator and $\\sigma$ is a nonlinear activation function.\nThe key point of the aggregstion function is that it can handle input with vaiable length.\nPATCHY-SAN Another distinct line of works achieve weight sharing across different locations by ranking a node’s neighbors based on certain criteria and associating each ranking with a learnable weight.\nOverview of PATCHY-SAN Definitions:\n  Node degree: The degree of a node is the number of edges connected to the node.\n  Graph labelings: Graph labelings are essentially node scores which can be derived by node degree, centrality, and Weisfeiler-Lehman algorithm (a procedure for partitioning the vertices of a graph. It is also known as color refinement).\n  PATCHY-SAN orders neighbors of each node according to their graph labelings and selects the top $\\omega$ neighbors. As each node now has a fixed number of ordered neighbors, graph-structured data can be converted into grid-structured data. PATCHY-SAN applies a standard 1D convolutional filter to aggregate neighborhood feature information where the order of the filter’s weights corresponds to the order of a node’s neighbors. The ranking criterion of PATCHY-SAN only considers graph structures.\nTwo problems considered in PATCHY-SAN   Given a collection of graphs, learn a function that can be used for classification and regression problems on unseen graphs.\n  Given a large graph, learn graph representations that can be used to infer unseen graph properties such as node types and missing edges.\n  Steps of PATCHY-SAN PATCHY-SAN solves the above two problems through the following three steps:\n  Node Squenece Selection: Specify an order in the graph for each node through some human-defined rules (e.x., a node with a large degree has a high score). Then take the first $\\omega$ nodes as the representative of the whole graph. If the number of nodes is smaller than $\\omega$, empty nodes should be used for padding.\n  Neighborhood graph construction: Take the nodes selected in the previous step as the center. Get their neighbors (can be first-order or second-order), which form $\\omega$ cliques. Sort the neighbors in each clique according to the node ranking obtained in the previous step, and then take the first $k$ neighbors and arrange them in order, that is, form $\\omega$ ordered cliques. If the number of nodes is smaller than $k$, empty nodes should be used for padding.\n  Graph Normalization: The normalization imposes an order on the nodes of the neighborhood graph so as to map from the unordered graph space to a vector space with a linear order. According to the graph labeling in each clique, all cliques can be converted into a fixed-length sequence $(k+1)$, including the center node. Then they can be concatenated according to the order of the center nodes to obtain a sequence with length of $\\omega \\cdot (k +1)$, which represents the entire graph. In this way, we can directly use the 1D convolution to model the sequence.\n   Reference:\n Convolution: https://en.wikipedia.org/wiki/Convolution From graph to graph convolution (2): https://www.cnblogs.com/SivilTaram/p/graph_neural_network_2.html http://www.matlog.net/icml2016_slides.pdf  Paper\n A Comprehensive Survey on Graph Neural Networks: https://arxiv.org/pdf/1901.00596.pdf Inductive Representation Learning on Large Graphs: https://arxiv.org/pdf/1706.02216.pdf Neural Message Passing for Quantum Chemistry: https://arxiv.org/pdf/1704.01212.pdf Learning Convolutional Neural Networks for Graphs: https://arxiv.org/pdf/1605.05273.pdf  ","wordCount":"1896","inLanguage":"en","datePublished":"2020-08-20T00:00:00Z","dateModified":"2020-08-20T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://tangliyan.com/blog/posts/spatial_conv/"},"publisher":{"@type":"Organization","name":"Liyan Tang","logo":{"@type":"ImageObject","url":"https://tangliyan.com/blog/favicon.ico"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://tangliyan.com/blog/ accesskey=h title="Liyan Tang (Alt + H)">Liyan Tang</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
<li>
<a href=https://tangliyan.com/blog/archives title=Archive>
<span>Archive</span>
</a>
</li>
<li>
<a href=https://tangliyan.com/blog/tags/ title=Tags>
<span>Tags</span>
</a>
</li>
<li>
<a href=https://tangliyan.com/blog/search/ title="Search (Alt + /)" accesskey=/>
<span>Search</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<div class=breadcrumbs><a href=https://tangliyan.com/blog/>Home</a>&nbsp;»&nbsp;<a href=https://tangliyan.com/blog/posts/>Posts</a></div>
<h1 class=post-title>
Graph Convolutional Neural Network - Spatial Convolution
</h1>
<div class=post-meta><span title="2020-08-20 00:00:00 +0000 UTC">August 20, 2020</span>&nbsp;·&nbsp;9 min
</div>
</header> <div class=toc>
<details>
<summary accesskey=c title="(Alt + C)">
<span class=details>Table of Contents</span>
</summary>
<div class=inner><ul>
<li>
<a href=#note aria-label=Note>Note</a></li>
<li>
<a href=#convolutional-graph-neural-networks-convgnns aria-label="Convolutional graph neural networks (ConvGNNs)">Convolutional graph neural networks (ConvGNNs)</a><ul>
<li>
<a href=#gcn-framework aria-label="GCN Framework">GCN Framework</a></li>
<li>
<a href=#gcn-vs-recgnn aria-label="GCN v.s. RecGNN">GCN v.s. RecGNN</a></li></ul>
</li>
<li>
<a href=#what-is-convolution aria-label="What is Convolution">What is Convolution</a></li>
<li>
<a href=#spatial-based-convgnns aria-label="Spatial-based ConvGNNs">Spatial-based ConvGNNs</a><ul>
<li>
<a href=#message-passing-neural-network-mpnn aria-label="Message Passing Neural Network (MPNN)">Message Passing Neural Network (MPNN)</a><ul>
<li>
<a href=#introduction-to-mpnn aria-label="Introduction to MPNN">Introduction to MPNN</a></li>
<li>
<a href=#shortage-of-the-mpnn-framework aria-label="Shortage of the MPNN framework">Shortage of the MPNN framework</a></li></ul>
</li>
<li>
<a href=#graphsage-sample-and-aggregate aria-label="GraphSAGE (SAmple and aggreGatE)">GraphSAGE (SAmple and aggreGatE)</a><ul>
<li>
<a href=#overview-of-graphsage aria-label="Overview of GraphSAGE">Overview of GraphSAGE</a></li>
<li>
<a href=#aggregator-fuctions aria-label="Aggregator Fuctions">Aggregator Fuctions</a></li></ul>
</li>
<li>
<a href=#patchy-san aria-label=PATCHY-SAN>PATCHY-SAN</a><ul>
<li>
<a href=#overview-of-patchy-san aria-label="Overview of PATCHY-SAN">Overview of PATCHY-SAN</a></li>
<li>
<a href=#two-problems-considered-in-patchy-san aria-label="Two problems considered in PATCHY-SAN">Two problems considered in PATCHY-SAN</a></li>
<li>
<a href=#steps-of-patchy-san aria-label="Steps of PATCHY-SAN">Steps of PATCHY-SAN</a>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</details>
</div>
<div class=post-content><h2 id=note>Note<a hidden class=anchor aria-hidden=true href=#note>#</a></h2>
<p>This is the second post of the Graph Neural Networks (GNNs) series.</p>
<h2 id=convolutional-graph-neural-networks-convgnns>Convolutional graph neural networks (ConvGNNs)<a hidden class=anchor aria-hidden=true href=#convolutional-graph-neural-networks-convgnns>#</a></h2>
<p>Convolutional graph neural networks (ConvGNNs) generalize the operation of convolution from grid data to graph data. <strong>The main idea is to generate a node $v$’s representation by
aggregating its own features $\mathbf{x}_{v}$ and neighbors’ features $\mathbf{x}_{u}$, where $u \in N(v)$</strong>. Different from RecGNNs, ConvGNNs stack fixed number of multiple graph convolutional layers with different weights to extract high-level node representations.</p>
<p>ConvGNNs fall into two categories:</p>
<ul>
<li>
<p><strong>spatial-based GCN</strong>: Spatial-based approaches inherit ideas from RecGNNs to define graph convolutions by information propagation.</p>
</li>
<li>
<p><strong>spectral-based GCN</strong>: Spectral based approaches define graph convolutions by introducing filters from the perspective of <strong>graph signal processing</strong> where the graph convolutional operation is interpreted as removing noises from graph signals.</p>
</li>
</ul>
<p>Spatial-based methods have developed rapidly recently due to its attractive efficiency, flexibility, and generality. In this post, we mainly focus on spatial-based GCN and leave spectral-based GCN to the next post. Let&rsquo;s get started.</p>
<h3 id=gcn-framework>GCN Framework<a hidden class=anchor aria-hidden=true href=#gcn-framework>#</a></h3>
<img src="https://img-blog.csdnimg.cn/20200820065543780.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_1,color_FFFFFF,t_70#pic_center" width=700>
<p>As shown in the figure above, the input of GCN is the entire graph. In each convolution layer, a convolution operation is performed on the neighbors of each node, and the center node representation is updated with the result of the convolution. Then an activation function such as ReLU is used before going through the next layer of convolution layer. The above process continues until the number of layers reaches the expected depth (a hyper-parameter).</p>
<h3 id=gcn-vs-recgnn>GCN v.s. RecGNN<a hidden class=anchor aria-hidden=true href=#gcn-vs-recgnn>#</a></h3>
<p><strong>The main difference between GCN and RecGNN is that each convolutional layer of GCN has unique weights, and, on the other hand, in RecGNN the weights of each layer are shared.</strong></p>
<h2 id=what-is-convolution>What is Convolution<a hidden class=anchor aria-hidden=true href=#what-is-convolution>#</a></h2>
<p>In mathematics, convolution is a mathematical operation on two functions $f$ and $g$ that produces a third function $(f∗g)$ expressing how the shape of one is modified by the other.</p>
<p><strong>The term convolution is defined as the integral of the product of the two functions after one is reversed and shifted</strong>. The mathematical definition is the following:</p>
<p>$$
\begin{array}{c}
(f * g)(t)=\int_{-\infty}^{\infty} f(\tau) g(t-\tau) \quad (\text {continuous}) \\ (f * g)(t)=\sum_{\tau=-\infty}^{\infty} f(\tau) g(t-\tau) \quad(\text {discrete})
\end{array}
$$</p>
<p>The convolution formula can be described as <strong>a weighted average of the function $f(\tau)$ at the moment $t$ where the weighting is given by $g(-\tau)$ simply shifted by amount $t$</strong>. <strong>As <em>$t$</em> changes, the weighting function emphasizes different parts of the input function.</strong></p>
<img src=https://img-blog.csdnimg.cn/20200820064606800.gif#pic_center width=350>
<p>As the figure shown above, the filter is moved over by one pixel and this process is repeated until all of the possible locations in the image are filtered. At each step, <em>the convolution takes the weighted average of pixel values of the center pixel along with its neighbors</em>. Since the center pixel is changing at each time step, the convolution is emphasizing on different parts of the image.</p>
<h2 id=spatial-based-convgnns>Spatial-based ConvGNNs<a hidden class=anchor aria-hidden=true href=#spatial-based-convgnns>#</a></h2>
<img src="https://img-blog.csdnimg.cn/20200820065025800.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_1,color_FFFFFF,t_70#pic_center" width=550>
<p>Analogous to the convolutional operation of a conventional CNN on an image, spatial-based methods define graph convolutions based on a node’s spatial relations.</p>
<p>Images can be considered as a special form of graph with each pixel representing a node. Each pixel is directly connected to its nearby pixels, as illustrated in the figure above (left). A filter is applied to a $3 \times 3$ patch by taking the weighted average of pixel values of the central node and its neighbors across each channel.</p>
<p>Similarly, the spatial-based graph convolutions convolve the central node’s representation with its neighbors’ representations to derive the updated representation for the central node. From another perspective, <strong>spatial-based ConvGNNs share the same idea of information propagation/message passing with RecGNNs</strong>. The spatial graph convolutional operation essentially propagates node information along edges.</p>
<h3 id=message-passing-neural-network-mpnn>Message Passing Neural Network (MPNN)<a hidden class=anchor aria-hidden=true href=#message-passing-neural-network-mpnn>#</a></h3>
<h4 id=introduction-to-mpnn>Introduction to MPNN<a hidden class=anchor aria-hidden=true href=#introduction-to-mpnn>#</a></h4>
<p>Message Passing Neural Network (MPNN) <strong>outlines a general framework of spatial-based ConvGNNs</strong> (<em>It is not actually a model</em>). It treats graph convolutions as a message passing process in which information can be passed from one node to another along edges directly. MPNN runs K-step message passing iterations to let information propagate further. The message passing function (namely the spatial graph convolution) is defined as</p>
<p>$$
\mathbf{h}_{v}^{(k)}=U_{k}\left(\mathbf{h}_{v}^{(k-1)}, \sum_{u \in N(v)} M_{k}\left(\mathbf{h}_{v}^{(k-1)}, \mathbf{h}_{u}^{(k-1)}, \mathbf{x}_{(v,u)}\right)\right) \tag 1
$$</p>
<p>or we can write it separately as:</p>
<p>$$
\begin{array}
\mathbf{m}_v^{k} = \sum_{u \in N(v)} M_{k}\left(\mathbf{h}_{v}^{(k-1)}, \mathbf{h}_{u}^{(k-1)}, \mathbf{x}_{(v,u)}\right) \
\mathbf{h}_{v}^{(k)}=U_{k}\left(\mathbf{h}_{v}^{(k-1)}, \mathbf{m}_v^{k}\right) \tag 2\
\end{array}
$$</p>
<p>Note:</p>
<ul>
<li>$\mathbf{m}_v^{k}$ represents messages at node $v$ at time step $k$;</li>
<li>$\mathbf{h}_{v}^{(0)}=\mathbf{x}_{v}$;</li>
<li>$U_{k}(\cdot)$ and $M_{k}(\cdot)$ are functions with learnable parameters.</li>
<li>From (2), we see that MPNN has two main steps: <em>information passing</em> and <em>state update</em>. $M_{k}(\cdot)$ is the function for information passing and $U_{k}(\cdot)$ is the function for state update.</li>
<li>The formula can be interpreted as each node first collects information from neighbors and then updates its own hidden state.</li>
</ul>
<p>For example, here is an illustration of updating node $A$:</p>
<img src="https://img-blog.csdnimg.cn/20200820064716133.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_11,color_FFFFFF,t_70#pic_center" width=600>
<p>After deriving the hidden representations of each node, $\mathbf{h}_{v}^{(K)}$ can be passed to an output layer to perform node-level prediction tasks or to a readout function to perform graph-level prediction tasks. The readout function generates a representation of the entire graph based on node hidden representations (I will talk about readout function in a later post). It is generally defined as
$$
\mathbf{h}_{G}=R\left(\mathbf{h}_{v}^{(K)} \mid v \in G\right)
$$
where $R(\cdot)$ represents the readout function with learnable parameters, and $\mathbf{h}_{G}$ is a representation of the entire graph.</p>
<p>Note that MPNN can cover many existing GNNs by assuming different forms of $U_{k}(\cdot), M_{k}(\cdot),$ and $R(\cdot)$.</p>
<h4 id=shortage-of-the-mpnn-framework>Shortage of the MPNN framework<a hidden class=anchor aria-hidden=true href=#shortage-of-the-mpnn-framework>#</a></h4>
<p>The shortage of MPNN is that it convolves on the entire graph, which means that all nodes must be put into the memory to perform the convolution operation. But for large-scale graph, the convolution operation on the entire graph is not realistic. In the next section, I&rsquo;m going to introduce a model (<em>GraphSAGE</em>) to overcome this problem.</p>
<h3 id=graphsage-sample-and-aggregate>GraphSAGE (SAmple and aggreGatE)<a hidden class=anchor aria-hidden=true href=#graphsage-sample-and-aggregate>#</a></h3>
<h4 id=overview-of-graphsage>Overview of GraphSAGE<a hidden class=anchor aria-hidden=true href=#overview-of-graphsage>#</a></h4>
<p>As the number of neighbors of a node can vary from one to a thousand or even more, it is inefficient to take the full size of a node&rsquo;s neighborhood. GraphSage adopts sampling to obtain a fixed number of neighbors for each node. It performs graph convolutions by</p>
<p>$$
\mathbf{h}_{v}^{(k)}=\sigma(\mathbf{W}^{(k)} \cdot f_{k}\left(\mathbf{h}_{v}^{(k-1)},\left{\mathbf{h}_{u}^{(k-1)}, \forall u \in S_{\mathcal{N}(v)}\right}\right)
$$</p>
<p>where $\mathbf{h}_{v}^{(0)}=\mathbf{x}_{v}, f_{k}(\cdot)$ is an aggregation function, $S_{\mathcal{N}(v)}$ is a random sample of the node $v$ &rsquo;s neighbors. The aggregation function should be invariant to the permutations of node orderings such as a mean, sum or max function.</p>
<img src="https://img-blog.csdnimg.cn/20200820064807726.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_1,color_FFFFFF,t_70#pic_center" width=700>
<p>Specifically, the sampling process in GraphSage is divided into three steps (corespond to the figure above):</p>
<ul>
<li>
<p>First, several nodes are randomly sampled in the graph. Then for each node, they uniformly sample a fixed-size set of neighbors, instead of using full neighborhood sets. These fixed-size of neighbors are uniformly drawn from all possible neighbors, and they draw different uniform samples at each iteration. Here, neighbors are not necessarily first-order neighbors, but also second-order.</p>
</li>
<li>
<p>Update sampled node&rsquo;s information by aggregating the information of neighbor nodes through some aggregate functions.</p>
</li>
<li>
<p>Calculate the loss at the sampled node. If it is an unsupervised task, we hope that the neighborhood nodes on the graph share similar node representations; if it is a supervised task, we can calculate the loss based on the task label of the specific node.</p>
</li>
</ul>
<h4 id=aggregator-fuctions>Aggregator Fuctions<a hidden class=anchor aria-hidden=true href=#aggregator-fuctions>#</a></h4>
<p>Ideally, an aggregator function would be invariant to permutations of its inputs. This property of the aggregation function ensures that a neural network model canbe trained and applied to arbitrarily ordered node neighborhood feature sets. In the paper, they examed three aggregation functions: <em>Mean Aggregator, LSTM Aggretator, Pooling Aggretator</em>.</p>
<p><strong>Mean Aggregator</strong>: Their first examed aggregator function is the mean operator, where we simply take the elementwise mean of the vectors in $\left{\mathbf{h}<em>{u}^{(k-1)}, \forall u \in S</em>{\mathcal{N}(v)}\right}$. That is:</p>
<p>$$
\mathbf{h}_{v}^{(k)}=\sigma(\mathbf{W}^{(k)} \cdot \text{MEAN}\left({\mathbf{h}_{v}^{(k-1)}} \cup \left{\mathbf{h}_{u}^{(k-1)}, \forall u \in S_{\mathcal{N}(v)}\right}\right)
$$</p>
<p><strong>LSTM Aggregator</strong>: Compared to the mean aggregator, LSTMs have the advantage of larger expressive capability. However, it is important to note that LSTMs are not permutation
invariant, since they process their inputs in a sequential manner. They adapt LSTMs to operate on an unordered set by simply applying the LSTMs to a random permutation of the node’s neighbors.</p>
<p><strong>Pooling Aggregator</strong>: each neighbor’s vector is independently fed through a fully-connected neural network; following this transformation, an elementwise max-pooling operation is applied to aggregate information across the neighbor set:</p>
<p>$$
\text {AGGREGATE}_{k}^{\text {pool}}=\max \left({\sigma\left(\mathbf{W}_{\text {pool }} \mathbf{h}_{u}^{(k)}+\mathbf{b}\right), \forall u \in S_{\mathcal{N}(v)}}\right)
$$</p>
<p>where $\max$ denotes the element-wise max operator and $\sigma$ is a nonlinear activation function.</p>
<p><em>The key point of the aggregstion function is that it can handle input with vaiable length.</em></p>
<h3 id=patchy-san>PATCHY-SAN<a hidden class=anchor aria-hidden=true href=#patchy-san>#</a></h3>
<p>Another distinct line of works achieve weight sharing across different locations by ranking a node’s neighbors based on certain criteria and associating each ranking with a learnable weight.</p>
<h4 id=overview-of-patchy-san>Overview of PATCHY-SAN<a hidden class=anchor aria-hidden=true href=#overview-of-patchy-san>#</a></h4>
<p>Definitions:</p>
<ul>
<li>
<p>Node degree: The degree of a node is the number of edges connected to the node.</p>
</li>
<li>
<p>Graph labelings: Graph labelings are essentially node scores which can be derived by node degree, centrality, and Weisfeiler-Lehman algorithm (a procedure for partitioning the vertices of a graph. It is also known as color refinement).</p>
</li>
</ul>
<img src="https://img-blog.csdnimg.cn/20200820064853999.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_16,color_FFFFFF,t_70#pic_center" width=600>
<p><strong>PATCHY-SAN</strong> orders neighbors of each node according to their graph labelings and selects the top $\omega$ neighbors. As each node now has a fixed number of ordered neighbors, graph-structured data can be converted into grid-structured data. PATCHY-SAN applies a standard 1D convolutional filter to aggregate neighborhood feature information where the order of the filter’s weights corresponds to the order of a node’s neighbors. The ranking criterion of PATCHY-SAN only considers graph structures.</p>
<h4 id=two-problems-considered-in-patchy-san>Two problems considered in PATCHY-SAN<a hidden class=anchor aria-hidden=true href=#two-problems-considered-in-patchy-san>#</a></h4>
<ol>
<li>
<p>Given a collection of graphs, learn a function that can be used for classification and regression problems on unseen graphs.</p>
</li>
<li>
<p>Given a large graph, learn graph representations that can be used to infer unseen graph properties such as node types and missing edges.</p>
</li>
</ol>
<h4 id=steps-of-patchy-san>Steps of PATCHY-SAN<a hidden class=anchor aria-hidden=true href=#steps-of-patchy-san>#</a></h4>
<img src="https://img-blog.csdnimg.cn/20200820064932206.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_1,color_FFFFFF,t_70#pic_center" width=700>
<p>PATCHY-SAN solves the above two problems through the following three steps:</p>
<ol>
<li>
<p><strong>Node Squenece Selection</strong>: Specify an order in the graph for each node through some human-defined rules (<em>e.x.</em>, a node with a large degree has a high score). Then take the first $\omega$ nodes as the representative of the whole graph. If the number of nodes is smaller than $\omega$, empty nodes should be used for padding.</p>
</li>
<li>
<p><strong>Neighborhood graph construction</strong>: Take the nodes selected in the previous step as the center. Get their neighbors (can be first-order or second-order), which form $\omega$ cliques. Sort the neighbors in each clique according to the node ranking obtained in the previous step, and then take the first $k$ neighbors and arrange them in order, that is, form $\omega$ ordered cliques. If the number of nodes is smaller than $k$, empty nodes should be used for padding.</p>
</li>
<li>
<p><strong>Graph Normalization</strong>: The normalization imposes an order on the nodes of the neighborhood graph so as to map from the unordered graph space to a vector space with a linear order. According to the graph labeling in each clique, all cliques can be converted into a fixed-length sequence $(k+1)$, including the center node. Then they can be concatenated according to the order of the center nodes to obtain a sequence with length of $\omega \cdot (k +1)$, which represents the entire graph. In this way, we can directly use the 1D convolution to model the sequence.</p>
</li>
</ol>
<hr>
<p>Reference:</p>
<ul>
<li>Convolution: <a href=https://en.wikipedia.org/wiki/Convolution>https://en.wikipedia.org/wiki/Convolution</a></li>
<li>From graph to graph convolution (2): <a href=https://www.cnblogs.com/SivilTaram/p/graph_neural_network_2.html>https://www.cnblogs.com/SivilTaram/p/graph_neural_network_2.html</a></li>
<li><a href=http://www.matlog.net/icml2016_slides.pdf>http://www.matlog.net/icml2016_slides.pdf</a></li>
</ul>
<p>Paper</p>
<ul>
<li>A Comprehensive Survey on Graph Neural Networks: <a href=https://arxiv.org/pdf/1901.00596.pdf>https://arxiv.org/pdf/1901.00596.pdf</a></li>
<li>Inductive Representation Learning on Large Graphs: <a href=https://arxiv.org/pdf/1706.02216.pdf>https://arxiv.org/pdf/1706.02216.pdf</a></li>
<li>Neural Message Passing for Quantum Chemistry: <a href=https://arxiv.org/pdf/1704.01212.pdf>https://arxiv.org/pdf/1704.01212.pdf</a></li>
<li>Learning Convolutional Neural Networks for Graphs: <a href=https://arxiv.org/pdf/1605.05273.pdf>https://arxiv.org/pdf/1605.05273.pdf</a></li>
</ul>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=https://tangliyan.com/blog/tags/graph/>GRAPH</a></li>
<li><a href=https://tangliyan.com/blog/tags/math/>MATH</a></li>
</ul>
<nav class=paginav>
<a class=prev href=https://tangliyan.com/blog/posts/spectral_conv/>
<span class=title>« Prev Page</span>
<br>
<span>Graph Convolutional Neural Network - Spectral Convolution</span>
</a>
<a class=next href=https://tangliyan.com/blog/posts/gnn/>
<span class=title>Next Page »</span>
<br>
<span>Introduction to Graph Neural Network (GNN)</span>
</a>
</nav>
<div class=share-buttons>
<a target=_blank rel="noopener noreferrer" aria-label="share Graph Convolutional Neural Network - Spatial Convolution on twitter" href="https://twitter.com/intent/tweet/?text=Graph%20Convolutional%20Neural%20Network%20-%20Spatial%20Convolution&url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fspatial_conv%2f&hashtags=GRAPH%2cMATH"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Graph Convolutional Neural Network - Spatial Convolution on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fspatial_conv%2f&title=Graph%20Convolutional%20Neural%20Network%20-%20Spatial%20Convolution&summary=Graph%20Convolutional%20Neural%20Network%20-%20Spatial%20Convolution&source=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fspatial_conv%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Graph Convolutional Neural Network - Spatial Convolution on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fspatial_conv%2f&title=Graph%20Convolutional%20Neural%20Network%20-%20Spatial%20Convolution"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Graph Convolutional Neural Network - Spatial Convolution on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fspatial_conv%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Graph Convolutional Neural Network - Spatial Convolution on whatsapp" href="https://api.whatsapp.com/send?text=Graph%20Convolutional%20Neural%20Network%20-%20Spatial%20Convolution%20-%20https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fspatial_conv%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Graph Convolutional Neural Network - Spatial Convolution on telegram" href="https://telegram.me/share/url?text=Graph%20Convolutional%20Neural%20Network%20-%20Spatial%20Convolution&url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fspatial_conv%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a>
</div>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2022 <a href=https://tangliyan.com/blog/>Liyan Tang</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
<script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerText='copy';function d(){a.innerText='copied!',setTimeout(()=>{a.innerText='copy'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script>
</body>
</html>