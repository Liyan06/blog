<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Introduction to Graph Neural Network (GNN) | Liyan Tang</title>
<meta name=keywords content="GRAPH,MATH">
<meta name=description content="Note This is the first post of the Graph Neural Networks (GNNs) series.
Background and Intuition There is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. For examples, in e-commence, a graph-based learning system can exploit the interactions between users and products to make highly accurate recommendations. In chemistry, molecules are modeled as graphs.">
<meta name=author content>
<link rel=canonical href=https://tangliyan.com/blog/posts/gnn/>
<link crossorigin=anonymous href=/blog/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css integrity="sha256-yIlj/i15RiAA/Q+xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/blog/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://tangliyan.com/blog/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://tangliyan.com/blog/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://tangliyan.com/blog/favicon-32x32.png>
<link rel=apple-touch-icon href=https://tangliyan.com/blog/apple-touch-icon.png>
<link rel=mask-icon href=https://tangliyan.com/blog/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.92.0">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-202974782-1','auto'),ga('send','pageview'))</script><meta property="og:title" content="Introduction to Graph Neural Network (GNN)">
<meta property="og:description" content="Note This is the first post of the Graph Neural Networks (GNNs) series.
Background and Intuition There is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. For examples, in e-commence, a graph-based learning system can exploit the interactions between users and products to make highly accurate recommendations. In chemistry, molecules are modeled as graphs.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://tangliyan.com/blog/posts/gnn/"><meta property="og:image" content="https://tangliyan.com/blog/papermod-cover.png"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2020-08-17T00:00:00+00:00">
<meta property="article:modified_time" content="2020-08-17T00:00:00+00:00">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://tangliyan.com/blog/papermod-cover.png">
<meta name=twitter:title content="Introduction to Graph Neural Network (GNN)">
<meta name=twitter:description content="Note This is the first post of the Graph Neural Networks (GNNs) series.
Background and Intuition There is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. For examples, in e-commence, a graph-based learning system can exploit the interactions between users and products to make highly accurate recommendations. In chemistry, molecules are modeled as graphs.">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://tangliyan.com/blog/posts/"},{"@type":"ListItem","position":2,"name":"Introduction to Graph Neural Network (GNN)","item":"https://tangliyan.com/blog/posts/gnn/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Introduction to Graph Neural Network (GNN)","name":"Introduction to Graph Neural Network (GNN)","description":"Note This is the first post of the Graph Neural Networks (GNNs) series.\nBackground and Intuition There is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. For examples, in e-commence, a graph-based learning system can exploit the interactions between users and products to make highly accurate recommendations. In chemistry, molecules are modeled as graphs.","keywords":["GRAPH","MATH"],"articleBody":"Note This is the first post of the Graph Neural Networks (GNNs) series.\nBackground and Intuition There is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. For examples, in e-commence, a graph-based learning system can exploit the interactions between users and products to make highly accurate recommendations. In chemistry, molecules are modeled as graphs.\nAnother example is the image data. We can represent an image as a regular grid in the Euclidean space. A convolutional neural network (CNN) is able to exploit the shift-invariance, local connectivity, and compositionality of image data. As a result, CNNs can extract local meaningful features that are shared with the entire data sets for various image analysis.\nAs graphs can be irregular, a graph may have a variable size of unordered nodes, and nodes from a graph may have a different number of neighbors, resulting in some important operations (e.g., convolutions) being easy to compute in the image domain, but difficult to apply to the graph domain.\nFurthermore, a core assumption of existing machine learning algorithms is that instances are independent of each other. This assumption no longer holds for graph data because each node is related to others by links of various types, such as citations, friendships, and interactions.\nThe complexity of graph data has imposed significant challenges on existing machine learning algorithms, and nowadays many studies on extending deep learning approaches for graph data have emerged.\nIntro to Graph Neural Networks Graph neural networks (GNNs) are categorized into four groups:\n Recurrent graph neural networks (RecGNNs) Convolutional graph neural networks (ConvGNNs) Graph autoencoders (GAEs) Spatial-temporal graph neural networks (STGNNs).  These early studies fall into the category of recurrent graph neural networks (RecGNNs). They learn a target node’s representation by propagating neighbor information in an iterative manner until a stable fixed point is reached. This process is computationally expensive, and recently there have been increasing efforts to overcome these challenges.\nEncouraged by the success of CNNs in the computer vision domain, a large number of methods that re-define the notion of convolution for graph data are developed in parallel. These approaches are under the umbrella of convolutional graph neural networks (ConvGNNs). ConvGNNs are divided into two main streams, the spectral-based approaches and the spatial-based approaches.\nGNNs Framework With the graph structure and node content information as inputs, the outputs of GNNs can focus on different graph analytics tasks with one of the following mechanisms:\n  Node-level outputs relate to node regression and node classification tasks. RecGNNs and ConvGNNs can extract high-level node representations by information propagation/graph convolution. With a multi-perceptron or a softmax layer as the output layer, GNNs are able to perform node-level tasks in an end-to-end manner.\n  Edge-level outputs relate to the edge classification and link prediction tasks. With two nodes’ hidden representations from GNNs as inputs, a similarity function or a neural network can be utilized to predict the label/connection strength of an edge.\n  Graph-level outputs relate to the graph classification task. To obtain a compact representation on the graph level, GNNs are often combined with pooling and readout operations.\n  Definition A graph is represented as $G=(V, E)$ where\n $V$ is the set of nodes/ vertices, $v_{i} \\in$ $V$ denotes a node in $V$; $E$ is the set of edges; The neighborhood of a node $v$ is defined as $N(v)={u \\in V \\mid(v, u) \\in E}$.  We are going to use the following notations in the post:\n $X \\in \\mathbf{R}^{n \\times d}$: The feature matrix of the graph, which is a concatenation of all node representations of the graph. $n$ is the number of nodes and $d$ is the dimension of a node feature vector. $\\mathbf{x}_{v}$: feature at node $v$. $\\mathbf{x}_{(v, u)}$: feature at the edge between node the $v$ and the node $u$. $\\mathbf{H} \\in \\mathbf{R}^{n \\times b}$: The node hidden feature matrix, where $b$ is the dimension of a hidden node feature vector. $\\mathbf{h}_{v} \\in \\mathbf{R}^b$: hidden state embedding for the node $v$.  Recurrent graph neural networks (RecGNNs) Introduction to RecGNNs Recurrent graph neural networks (RecGNNs) mostly are pioneer works of graph neural networks which are based on the fixed point theorem. The primary goal of RecGNNs is to learn an embedding for each node (node representation). More specifically, given a graph G, each node $x_v$ and edge $x_{(v, u)}$ connecting two nodes $x_v, x_u$ has its own feature, and the goal is to learn these features.\nRecGNNs aim to learn these node representations with recurrent neural architectures. They apply the same set of parameters recurrently over nodes in a graph to extract high-level node representations. They assume a node in a graph constantly exchanges information/ message with its neighbors until a stable equilibrium is reached. RecGNNs are conceptually important and inspired later research on convolutional graph neural networks.\nBased on an information diffusion mechanism, RecGNNs updates nodes' states by exchanging neighborhood information recurrently until a stable equilibrium is reached. A node’s hidden state is recurrently updated by\n$$ \\mathbf{h}_{v}^{(t)}=\\sum_{u \\in N(v)} f\\left(\\mathbf{x}_{v}, \\mathbf{x}_{(v, u)}, \\mathbf{x}_{u}, \\mathbf{h}_{u}^{(t-1)}\\right) \\tag 1 $$\nwhere\n $f(\\cdot)$ is a parametric function (also called local transaction function) which could be approximated by a neural network $\\mathbf{h}_{v}^{(0)}$ is initialized randomly The sum operation enables RecGNNs to be applicable to all nodes, even if the number of neighbors differs.  To ensure convergence, the recurrent function $f(\\cdot)$ must be a contraction mapping, which shrinks the distance between two points after projecting them into a latent space. Notice that when a convergence criterion is satisfied, the last step node hidden states are the final node embeddings/ representations. Then a local output function $g$ is added for the following downstream tasks. For example, we use\n$$ \\mathbf{o}_{v} = g\\left(\\mathbf{h}_{v}, \\mathbf{x}_{v}\\right) \\tag 2$$\nto describe how the output is produced. The pipeline could be visualized as the following figure:\nNote how the connections (green lines) match the sample graph on the left. From (1), the input of the first cell is $\\mathbf{x_1}, \\mathbf{x_{(1,3)}}, \\mathbf{x_3}, \\mathbf{h_3}$. After convergence, a local output function $g$ is added to produce output.\nBanach’s Fixed Point Theorem Let $F$ be a function obtained by stacking several $f$ together (aka global transition function). The fixed point theorem suggests that if $F$ is a contraction mapping, then $H$ will always converge to a fixed point regardless of what $H^0$ is.\nIn other words, after contraction mapping $F$, the image after mapping must be smaller than the original space. Imagine that this compression process continues, eventually all points in the original space will be mapped to one point.\n contraction mapping: a function $F$ from $M$ to itself, with the property that there is some nonnegative real number $0 \\leq k $$ d(F(x), F(y)) \\leq k d(x, y) \\tag 3 $$\nLet F be a neural netwrok, to make sure F is a contraction mapping, a penalty term has to be imposed on the Jacobian matrix of parameters. To illustrate the point, let x, y be two vectors in an one dimensional space, then from the definition of the contraction mapping, we have\n$$ \\begin{array}{c} |F(x)-F(y)| \\leq c|x-y|, \\ 0 \\leq cNotice that to be a contraction mapping, the gradient has to be less than $1$. We use Lagrange Multiplier to convert this constrained problem to an un-constrained problem and the training objective can be written as\n$$ J=\\operatorname{Loss}+\\lambda \\cdot \\max \\left(\\frac{|\\partial F|}{|\\partial \\mathbf{h}|}-c, 0\\right), c \\in(0,1) \\tag 4 $$\nwith hyper-parameter $\\lambda$. The penalty term is a hinge loss.\nAfter setting up labels for nodes' outputs, we could then update parameters by calculating the gradient of the back-propagation based on the loss from the forward-propagation.\nRecGNNs v.s. RNNs For simplicity, assuming that there are three nodes $\\mathbf{x_1}, \\mathbf{x_2}, \\mathbf{x_3}$ in GNN. Correspondingly, there is a sequence $(\\mathbf{x_1}, \\mathbf{x_2}, \\mathbf{x_3})$ in RNN. The followings are the differences:\n  RecGNNs are based on the fixed point theorem, which means that the length of RecGNNs unfolded along time is dynamic and determined according to the convergence conditions, while the length of RNNs unfolded along time is equal to the length of the sequence itself.\n  The input of each time step of RecGNNs is features of all nodes and edges, and the input of each time step of RNNs is the corresponding input at that moment. At the same time, the information flow between time steps is not the same, the former is determined by the edge, and the latter is determined by the order of the input sequence.\n  The goal of RecGNNs is to obtain a stable hidden state for each node, so it has outputs only after the hidden state converges; while RNN can output at each time step.\n  RecGNNs uses Almeida-Pineda (AP) algorithm to optimize back-propagation, while RNNs uses Back Propogation Through Time (BPTT). The former has requirements for convergence, while the latter does not.\n  Limitation of RecGNNs   There is limited influence of edges on the hidden state of nodes, and RecGNNs do not have learnable parameters for edges, which means that certain features of edges cannot be learned by the model.\n  Fixed point theorem will lead to too much information sharing between the hidden states of nodes, resulting in the over-smoothing problem, and node representations become less informative (all look similiar). Here is an illustration of over-smoothing problem:\n  Note: The purpose of using the fix point theorem is to make sure RecGNNs is stable. However, if the model is too deep, all hidden states of nodes will be similar (over-smoothing as stated above).\nGated Graph Neural Networks (GGNNs) Introduction to GGNNs Gated Graph Neural Network (GGNN) employs a gated recurrent unit (GRU) as a recurrent function, reducing the recurrence to a fixed number of steps. The advantage is that it no longer needs to constrain parameters to ensure convergence. A node hidden state is updated by its previous hidden states and its neighboring hidden states, defined as\n$$ \\mathbf{h}_{v}^{(t)}=G R U\\left(\\mathbf{h}_{v}^{(t-1)}, \\sum_{u \\in N(v)} \\mathbf{W}_{edge} \\mathbf{h}_{u}^{(t-1)}\\right) $$\nwhere $\\mathbf{h}^{(0)}_v =\\mathbf{x}_v$ and $\\mathbf{W}_{edge}$ consists of learnable parameters for edges. Notice that different from RecGNNs, node features $\\mathbf{x}_v$ do not appear in the formula of GGNN, but instead used as initialization of hidden states of nodes.\nGGNNs v.s. RecRNNs The major difference between GGNNs and RecRNNs is that GGNNs are longer based on the fixed point theorem, which means that the convergence is no longer required.\nOther than that, GGNNs use the BPTT algorithm to learn the model parameters. This can be problematic for large graphs, as GGNNs need to run the recurrent function multiple times over all nodes, requiring the intermediate states of all nodes to be stored in memory.\n Reference:\n https://en.wikipedia.org/wiki/Contraction_mapping Big thanks to this blog: From graph to graph convolution (1): https://www.cnblogs.com/SivilTaram/p/graph_neural_network_1.html https://en.wikipedia.org/wiki/File:Graph_Laplacian_Diffusion_Example.gif  Paper:\n A Comprehensive Survey on Graph Neural Networks: https://arxiv.org/pdf/1901.00596.pdf The graph neural network model: https://persagen.com/files/misc/scarselli2009graph.pdf Gated Graph Sequence Neural Networks: https://arxiv.org/pdf/1511.05493.pdf  ","wordCount":"1826","inLanguage":"en","datePublished":"2020-08-17T00:00:00Z","dateModified":"2020-08-17T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://tangliyan.com/blog/posts/gnn/"},"publisher":{"@type":"Organization","name":"Liyan Tang","logo":{"@type":"ImageObject","url":"https://tangliyan.com/blog/favicon.ico"}}}</script><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']]},svg:{fontCache:'global'}}</script>
<script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://tangliyan.com/blog/ accesskey=h title="Liyan Tang (Alt + H)">Liyan Tang</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
<li>
<a href=https://tangliyan.com/blog/archives title=Archive>
<span>Archive</span>
</a>
</li>
<li>
<a href=https://tangliyan.com/blog/tags/ title=Tags>
<span>Tags</span>
</a>
</li>
<li>
<a href=https://tangliyan.com/blog/search/ title="Search (Alt + /)" accesskey=/>
<span>Search</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<div class=breadcrumbs><a href=https://tangliyan.com/blog/>Home</a>&nbsp;»&nbsp;<a href=https://tangliyan.com/blog/posts/>Posts</a></div>
<h1 class=post-title>
Introduction to Graph Neural Network (GNN)
</h1>
<div class=post-meta><span title="2020-08-17 00:00:00 +0000 UTC">August 17, 2020</span>&nbsp;·&nbsp;9 min
</div>
</header> <div class=toc>
<details>
<summary accesskey=c title="(Alt + C)">
<span class=details>Table of Contents</span>
</summary>
<div class=inner><ul>
<li>
<a href=#note aria-label=Note>Note</a></li>
<li>
<a href=#background-and-intuition aria-label="Background and Intuition">Background and Intuition</a></li>
<li>
<a href=#intro-to-graph-neural-networks aria-label="Intro to Graph Neural Networks">Intro to Graph Neural Networks</a><ul>
<li>
<a href=#gnns-framework aria-label="GNNs Framework">GNNs Framework</a></li>
<li>
<a href=#definition aria-label=Definition>Definition</a></li></ul>
</li>
<li>
<a href=#recurrent-graph-neural-networks-recgnns aria-label="Recurrent graph neural networks (RecGNNs)">Recurrent graph neural networks (RecGNNs)</a><ul>
<li>
<a href=#introduction-to-recgnns aria-label="Introduction to RecGNNs">Introduction to RecGNNs</a></li>
<li>
<a href=#banachs-fixed-point-theorem aria-label="Banach&amp;rsquo;s Fixed Point Theorem">Banach&rsquo;s Fixed Point Theorem</a></li>
<li>
<a href=#recgnns-vs-rnns aria-label="RecGNNs v.s. RNNs">RecGNNs v.s. RNNs</a></li>
<li>
<a href=#limitation-of-recgnns aria-label="Limitation of RecGNNs">Limitation of RecGNNs</a></li></ul>
</li>
<li>
<a href=#gated-graph-neural-networks-ggnns aria-label="Gated Graph Neural Networks (GGNNs)">Gated Graph Neural Networks (GGNNs)</a><ul>
<li>
<a href=#introduction-to-ggnns aria-label="Introduction to GGNNs">Introduction to GGNNs</a></li>
<li>
<a href=#ggnns-vs-recrnns aria-label="GGNNs v.s. RecRNNs">GGNNs v.s. RecRNNs</a>
</li>
</ul>
</li>
</ul>
</div>
</details>
</div>
<div class=post-content><h2 id=note>Note<a hidden class=anchor aria-hidden=true href=#note>#</a></h2>
<p>This is the first post of the Graph Neural Networks (GNNs) series.</p>
<h2 id=background-and-intuition>Background and Intuition<a hidden class=anchor aria-hidden=true href=#background-and-intuition>#</a></h2>
<p>There is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. For examples, in e-commence, a graph-based learning system can exploit the interactions between users and products to make highly accurate recommendations. In chemistry, molecules are modeled as graphs.</p>
<p>Another example is the image data. We can represent an image as a regular grid in the Euclidean space. A convolutional neural network (CNN) is able to exploit the <em>shift-invariance, local connectivity, and compositionality</em> of image data. As a result, CNNs can extract local meaningful features that are shared with the entire data sets for various image analysis.</p>
<img src="https://img-blog.csdnimg.cn/20200817073916883.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_1,color_FFFFFF,t_70#pic_center" width=600>
<p>As graphs can be irregular, a graph may have a variable size of unordered nodes, and nodes from a graph may have a different number of neighbors, resulting in some important operations (<em>e.g.</em>, convolutions) being easy to compute in the image domain, but difficult to apply to the graph domain.</p>
<p>Furthermore, a core assumption of existing machine learning algorithms is that instances are independent of each other. This assumption no longer holds for graph data because each node is related to others by links of various types, such as citations, friendships, and interactions.</p>
<p>The complexity of graph data has imposed significant challenges on existing machine learning algorithms, and nowadays many studies on extending deep learning approaches for graph data have emerged.</p>
<h2 id=intro-to-graph-neural-networks>Intro to Graph Neural Networks<a hidden class=anchor aria-hidden=true href=#intro-to-graph-neural-networks>#</a></h2>
<p>Graph neural networks (GNNs) are categorized into four groups:</p>
<ul>
<li>Recurrent graph neural networks (RecGNNs)</li>
<li>Convolutional graph neural networks (ConvGNNs)</li>
<li>Graph autoencoders (GAEs)</li>
<li>Spatial-temporal graph neural networks (STGNNs).</li>
</ul>
<p>These early studies fall into the category of recurrent graph neural networks (RecGNNs). They learn a target node’s representation by propagating neighbor information in an iterative manner until a stable fixed point is reached. This process is computationally expensive, and recently there have been increasing efforts to overcome these challenges.</p>
<p>Encouraged by the success of CNNs in the computer vision domain, a large number of methods that re-define the notion of convolution for graph data are developed in parallel. These approaches are under the umbrella of convolutional graph neural networks (ConvGNNs). ConvGNNs are divided into two main streams, the spectral-based approaches and the spatial-based approaches.</p>
<h3 id=gnns-framework>GNNs Framework<a hidden class=anchor aria-hidden=true href=#gnns-framework>#</a></h3>
<p>With the graph structure and node content information as inputs, the outputs of GNNs can focus on different graph analytics tasks with one of the following mechanisms:</p>
<ul>
<li>
<p><strong>Node-level</strong> outputs relate to <strong>node regression</strong> and <strong>node classification</strong> tasks. RecGNNs and ConvGNNs can extract high-level node representations by information propagation/graph convolution. With a multi-perceptron or a softmax layer as the output layer, GNNs are able to perform node-level tasks in an end-to-end manner.</p>
</li>
<li>
<p><strong>Edge-level</strong> outputs relate to the <strong>edge classification</strong> and <strong>link prediction</strong> tasks. With two nodes’ hidden representations from GNNs as inputs, a similarity function or a neural network can be utilized to predict the label/connection strength of an edge.</p>
</li>
<li>
<p><strong>Graph-level</strong> outputs relate to the <strong>graph classification</strong> task. To obtain a compact representation on the graph level, GNNs are often combined with pooling and readout operations.</p>
</li>
</ul>
<h3 id=definition>Definition<a hidden class=anchor aria-hidden=true href=#definition>#</a></h3>
<p>A <em>graph</em> is represented as $G=(V, E)$ where</p>
<ul>
<li>$V$ is the set of nodes/ vertices, $v_{i} \in$ $V$ denotes a node in $V$;</li>
<li>$E$ is the set of edges;</li>
<li>The neighborhood of a node $v$ is defined as $N(v)={u \in V \mid(v, u) \in E}$.</li>
</ul>
<img src="https://img-blog.csdnimg.cn/20200817074018169.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_1,color_FFFFFF,t_70#pic_center" width=400>
<p>We are going to use the following notations in the post:</p>
<ul>
<li>$X \in \mathbf{R}^{n \times d}$: The feature matrix of the graph, which is a concatenation of all node representations of the graph. $n$ is the number of nodes and $d$ is the dimension of a node feature vector.</li>
<li>$\mathbf{x}_{v}$: feature at node $v$.</li>
<li>$\mathbf{x}_{(v, u)}$: feature at the edge between node the $v$ and the node $u$.</li>
<li>$\mathbf{H} \in \mathbf{R}^{n \times b}$: The node hidden feature matrix, where $b$ is the dimension of a hidden node feature vector.</li>
<li>$\mathbf{h}_{v} \in \mathbf{R}^b$: hidden state embedding for the node $v$.</li>
</ul>
<h2 id=recurrent-graph-neural-networks-recgnns>Recurrent graph neural networks (RecGNNs)<a hidden class=anchor aria-hidden=true href=#recurrent-graph-neural-networks-recgnns>#</a></h2>
<h3 id=introduction-to-recgnns>Introduction to RecGNNs<a hidden class=anchor aria-hidden=true href=#introduction-to-recgnns>#</a></h3>
<p>Recurrent graph neural networks (RecGNNs) mostly are pioneer works of graph neural networks which are based on <strong>the fixed point theorem</strong>. The primary goal of RecGNNs is to learn an embedding for each node (node representation). More specifically, given a graph G, each node $x_v$ and edge $x_{(v, u)}$ connecting two nodes $x_v, x_u$ has its own feature, and the goal is to learn these features.</p>
<p>RecGNNs aim to learn these node representations with recurrent neural architectures. They apply the same set of parameters recurrently over nodes in a graph to extract high-level node representations. They assume a node in a graph constantly exchanges information/ message with its neighbors until a stable equilibrium is reached. RecGNNs are conceptually important and inspired later research on convolutional graph neural networks.</p>
<p>Based on an information diffusion mechanism, RecGNNs updates nodes' states by exchanging neighborhood information recurrently until a stable equilibrium is reached. A node&rsquo;s hidden state is recurrently updated by</p>
<p>$$
\mathbf{h}_{v}^{(t)}=\sum_{u \in N(v)} f\left(\mathbf{x}_{v}, \mathbf{x}_{(v, u)}, \mathbf{x}_{u}, \mathbf{h}_{u}^{(t-1)}\right) \tag 1
$$</p>
<p>where</p>
<ul>
<li>$f(\cdot)$ is a parametric function (also called <strong>local transaction function</strong>) which could be approximated by a neural network</li>
<li>$\mathbf{h}_{v}^{(0)}$ is initialized randomly</li>
<li><em>The sum operation enables RecGNNs to be applicable to all nodes, even if the number of neighbors differs</em>.</li>
</ul>
<p>To ensure convergence, <em>the recurrent function $f(\cdot)$ must be a contraction mapping</em>, which shrinks the distance between two points after projecting them into a latent space. Notice that when a convergence criterion is satisfied, the last step node hidden states are the final node embeddings/ representations. Then a <strong>local output function</strong> $g$ is added for the following downstream tasks. For example, we use</p>
<p>$$ \mathbf{o}_{v} = g\left(\mathbf{h}_{v}, \mathbf{x}_{v}\right) \tag 2$$</p>
<p>to describe how the output is produced. The pipeline could be visualized as the following figure:</p>
<img src="https://img-blog.csdnimg.cn/20200817074110976.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_1,color_FFFFFF,t_70#pic_center" width=700>
<p>Note how the connections (green lines) match the sample graph on the left. From (1), the input of the first cell is $\mathbf{x_1}, \mathbf{x_{(1,3)}}, \mathbf{x_3}, \mathbf{h_3}$. After convergence, a local output function $g$ is added to produce output.</p>
<h3 id=banachs-fixed-point-theorem>Banach&rsquo;s Fixed Point Theorem<a hidden class=anchor aria-hidden=true href=#banachs-fixed-point-theorem>#</a></h3>
<p>Let $F$ be a function obtained by stacking several $f$ together (<em>aka</em> <strong>global transition function</strong>). The fixed point theorem suggests that if $F$ is a contraction mapping, then $H$ will always converge to a fixed point regardless of what $H^0$ is.</p>
<p>In other words, after contraction mapping $F$, the image after mapping must be smaller than the original space. Imagine that this compression process continues, eventually all points in the original space will be mapped to one point.</p>
<img src="https://img-blog.csdnimg.cn/20200817074154972.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_1,color_FFFFFF,t_70#pic_center" width=500>
<ul>
<li><strong>contraction mapping</strong>: a function $F$ from $M$ to itself, with the property that there is some nonnegative real number $0 \leq k&lt;1$ such that for all $x$ and $y$ in $M$,</li>
</ul>
<p>$$
d(F(x), F(y)) \leq k d(x, y) \tag 3
$$</p>
<p>Let F be a neural netwrok, to make sure F is a contraction mapping, a penalty term has to be imposed on the Jacobian matrix of parameters. To illustrate the point, let x, y be two vectors in an one dimensional space, then from the definition of the contraction mapping, we have</p>
<p>$$
\begin{array}{c}
|F(x)-F(y)| \leq c|x-y|, \ 0 \leq c&lt;1 \\ \frac{|F(x)-F(y)|}{|x-y|} \leq c \\ \frac{|F(x)-F(x-\Delta x)|}{|\Delta x|} \leq c \\ \left|F^{\prime}(x)\right|=\left|\frac{\partial F(x)}{\partial x}\right| \leq c
\end{array}
$$</p>
<p>Notice that to be a contraction mapping, the gradient has to be less than $1$. We use Lagrange Multiplier to convert this constrained problem to an un-constrained problem and the training objective can be written as</p>
<p>$$
J=\operatorname{Loss}+\lambda \cdot \max \left(\frac{|\partial F|}{|\partial \mathbf{h}|}-c, 0\right), c \in(0,1) \tag 4
$$</p>
<p>with hyper-parameter $\lambda$. The penalty term is a hinge loss.</p>
<p>After setting up labels for nodes' outputs, we could then update parameters by calculating the gradient of the back-propagation based on the loss from the forward-propagation.</p>
<h3 id=recgnns-vs-rnns>RecGNNs v.s. RNNs<a hidden class=anchor aria-hidden=true href=#recgnns-vs-rnns>#</a></h3>
<img src="https://img-blog.csdnimg.cn/20200817074242262.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_1,color_FFFFFF,t_70#pic_center" width=600>
<p>For simplicity, assuming that there are three nodes $\mathbf{x_1}, \mathbf{x_2}, \mathbf{x_3}$ in GNN. Correspondingly, there is a sequence $(\mathbf{x_1}, \mathbf{x_2}, \mathbf{x_3})$ in RNN. The followings are the differences:</p>
<ul>
<li>
<p>RecGNNs are based on the fixed point theorem, which means that the length of RecGNNs unfolded along time is dynamic and determined according to the convergence conditions, while the length of RNNs unfolded along time is equal to the length of the sequence itself.</p>
</li>
<li>
<p>The input of each time step of RecGNNs is features of all nodes and edges, and the input of each time step of RNNs is the corresponding input at that moment. At the same time, the information flow between time steps is not the same, the former is determined by the edge, and the latter is determined by the order of the input sequence.</p>
</li>
<li>
<p>The goal of RecGNNs is to obtain a stable hidden state for each node, so it has outputs only after the hidden state converges; while RNN can output at each time step.</p>
</li>
<li>
<p>RecGNNs uses Almeida-Pineda (AP) algorithm to optimize back-propagation, while RNNs uses Back Propogation Through Time (BPTT). The former has requirements for convergence, while the latter does not.</p>
</li>
</ul>
<h3 id=limitation-of-recgnns>Limitation of RecGNNs<a hidden class=anchor aria-hidden=true href=#limitation-of-recgnns>#</a></h3>
<ul>
<li>
<p>There is limited influence of edges on the hidden state of nodes, and RecGNNs do not have learnable parameters for edges, which means that certain features of edges cannot be learned by the model.</p>
</li>
<li>
<p>Fixed point theorem will lead to too much information sharing between the hidden states of nodes, resulting in the <strong>over-smoothing</strong> problem, and node representations become less informative (all look similiar). Here is an illustration of over-smoothing problem:</p>
</li>
</ul>
<img src=https://upload.wikimedia.org/wikipedia/commons/e/e1/Graph_Laplacian_Diffusion_Example.gif#pic_center width=450>
<p>Note: <em>The purpose of using the fix point theorem is to make sure RecGNNs is stable. However, if the model is too deep, all hidden states of nodes will be similar (over-smoothing as stated above).</em></p>
<h2 id=gated-graph-neural-networks-ggnns>Gated Graph Neural Networks (GGNNs)<a hidden class=anchor aria-hidden=true href=#gated-graph-neural-networks-ggnns>#</a></h2>
<h3 id=introduction-to-ggnns>Introduction to GGNNs<a hidden class=anchor aria-hidden=true href=#introduction-to-ggnns>#</a></h3>
<p>Gated Graph Neural Network (GGNN) employs a gated recurrent unit (GRU) as a recurrent function, reducing the recurrence to a fixed number of steps. The advantage is that it no longer needs to constrain parameters to ensure convergence. A node hidden state is updated by its previous hidden states and its neighboring hidden states, defined as</p>
<p>$$
\mathbf{h}_{v}^{(t)}=G R U\left(\mathbf{h}_{v}^{(t-1)}, \sum_{u \in N(v)} \mathbf{W}_{edge} \mathbf{h}_{u}^{(t-1)}\right)
$$</p>
<p>where $\mathbf{h}^{(0)}_v =\mathbf{x}_v$ and $\mathbf{W}_{edge}$ consists of learnable parameters for edges. Notice that different from RecGNNs, node features $\mathbf{x}_v$ do not appear in the formula of GGNN, but instead used as initialization of hidden states of nodes.</p>
<h3 id=ggnns-vs-recrnns>GGNNs v.s. RecRNNs<a hidden class=anchor aria-hidden=true href=#ggnns-vs-recrnns>#</a></h3>
<p>The major difference between GGNNs and RecRNNs is that GGNNs are longer based on the fixed point theorem, which means that the convergence is no longer required.</p>
<p>Other than that, GGNNs use the BPTT algorithm to learn the model parameters. <em>This can be problematic for large graphs, as GGNNs need to run the recurrent function multiple times over all nodes, requiring the intermediate states of all nodes to be stored in memory.</em></p>
<hr>
<p>Reference:</p>
<ul>
<li><a href=https://en.wikipedia.org/wiki/Contraction_mapping>https://en.wikipedia.org/wiki/Contraction_mapping</a></li>
<li>Big thanks to this blog: From graph to graph convolution (1): <a href=https://www.cnblogs.com/SivilTaram/p/graph_neural_network_1.html>https://www.cnblogs.com/SivilTaram/p/graph_neural_network_1.html</a></li>
<li><a href=https://en.wikipedia.org/wiki/File:Graph_Laplacian_Diffusion_Example.gif>https://en.wikipedia.org/wiki/File:Graph_Laplacian_Diffusion_Example.gif</a></li>
</ul>
<p>Paper:</p>
<ul>
<li>A Comprehensive Survey on Graph Neural Networks: <a href=https://arxiv.org/pdf/1901.00596.pdf>https://arxiv.org/pdf/1901.00596.pdf</a></li>
<li>The graph neural network model: <a href=https://persagen.com/files/misc/scarselli2009graph.pdf>https://persagen.com/files/misc/scarselli2009graph.pdf</a></li>
<li>Gated Graph Sequence Neural Networks: <a href=https://arxiv.org/pdf/1511.05493.pdf>https://arxiv.org/pdf/1511.05493.pdf</a></li>
</ul>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=https://tangliyan.com/blog/tags/graph/>GRAPH</a></li>
<li><a href=https://tangliyan.com/blog/tags/math/>MATH</a></li>
</ul>
<nav class=paginav>
<a class=prev href=https://tangliyan.com/blog/posts/spatial_conv/>
<span class=title>« Prev Page</span>
<br>
<span>Graph Convolutional Neural Network - Spatial Convolution</span>
</a>
<a class=next href=https://tangliyan.com/blog/posts/kaggle_jigsaw/>
<span class=title>Next Page »</span>
<br>
<span>Kaggle: Jigsaw Multilingual Toxic Comment Classification - top solutions</span>
</a>
</nav>
<div class=share-buttons>
<a target=_blank rel="noopener noreferrer" aria-label="share Introduction to Graph Neural Network (GNN) on twitter" href="https://twitter.com/intent/tweet/?text=Introduction%20to%20Graph%20Neural%20Network%20%28GNN%29&url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fgnn%2f&hashtags=GRAPH%2cMATH"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Introduction to Graph Neural Network (GNN) on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fgnn%2f&title=Introduction%20to%20Graph%20Neural%20Network%20%28GNN%29&summary=Introduction%20to%20Graph%20Neural%20Network%20%28GNN%29&source=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fgnn%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Introduction to Graph Neural Network (GNN) on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fgnn%2f&title=Introduction%20to%20Graph%20Neural%20Network%20%28GNN%29"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Introduction to Graph Neural Network (GNN) on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fgnn%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Introduction to Graph Neural Network (GNN) on whatsapp" href="https://api.whatsapp.com/send?text=Introduction%20to%20Graph%20Neural%20Network%20%28GNN%29%20-%20https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fgnn%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Introduction to Graph Neural Network (GNN) on telegram" href="https://telegram.me/share/url?text=Introduction%20to%20Graph%20Neural%20Network%20%28GNN%29&url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fgnn%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a>
</div>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2022 <a href=https://tangliyan.com/blog/>Liyan Tang</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
<script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerText='copy';function d(){a.innerText='copied!',setTimeout(()=>{a.innerText='copy'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script>
</body>
</html>