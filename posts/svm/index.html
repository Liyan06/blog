<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>SVM, Dual SVM, Non-linear SVM | Liyan Tang</title>
<meta name=keywords content="ML,MATH">
<meta name=description content="Linear SVM Idea We want to find a hyper-plane $w^\top x + b = 0$ that maximizes the margin.
Set up We first show that the vector $w$ is orthogonal to this hyper-plane. Let $x_1$, $x_2$ be any element on the hyper-plane. So we have $w^\top x_1 + b = 0$ and $w^\top x_2 + b = 0$. Then $w^\top (x_1 - x_2) = 0$, which implies $w$ is orthogonal to the hyper-plane.">
<meta name=author content>
<link rel=canonical href=https://tangliyan.com/blog/posts/svm/>
<link crossorigin=anonymous href=/blog/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css integrity="sha256-yIlj/i15RiAA/Q+xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/blog/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://tangliyan.com/blog/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://tangliyan.com/blog/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://tangliyan.com/blog/favicon-32x32.png>
<link rel=apple-touch-icon href=https://tangliyan.com/blog/apple-touch-icon.png>
<link rel=mask-icon href=https://tangliyan.com/blog/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.92.0">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-202974782-1','auto'),ga('send','pageview'))</script><meta property="og:title" content="SVM, Dual SVM, Non-linear SVM">
<meta property="og:description" content="Linear SVM Idea We want to find a hyper-plane $w^\top x + b = 0$ that maximizes the margin.
Set up We first show that the vector $w$ is orthogonal to this hyper-plane. Let $x_1$, $x_2$ be any element on the hyper-plane. So we have $w^\top x_1 + b = 0$ and $w^\top x_2 + b = 0$. Then $w^\top (x_1 - x_2) = 0$, which implies $w$ is orthogonal to the hyper-plane.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://tangliyan.com/blog/posts/svm/"><meta property="og:image" content="https://tangliyan.com/blog/papermod-cover.png"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2020-04-01T00:00:00+00:00">
<meta property="article:modified_time" content="2020-04-01T00:00:00+00:00">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://tangliyan.com/blog/papermod-cover.png">
<meta name=twitter:title content="SVM, Dual SVM, Non-linear SVM">
<meta name=twitter:description content="Linear SVM Idea We want to find a hyper-plane $w^\top x + b = 0$ that maximizes the margin.
Set up We first show that the vector $w$ is orthogonal to this hyper-plane. Let $x_1$, $x_2$ be any element on the hyper-plane. So we have $w^\top x_1 + b = 0$ and $w^\top x_2 + b = 0$. Then $w^\top (x_1 - x_2) = 0$, which implies $w$ is orthogonal to the hyper-plane.">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://tangliyan.com/blog/posts/"},{"@type":"ListItem","position":2,"name":"SVM, Dual SVM, Non-linear SVM","item":"https://tangliyan.com/blog/posts/svm/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"SVM, Dual SVM, Non-linear SVM","name":"SVM, Dual SVM, Non-linear SVM","description":"Linear SVM Idea We want to find a hyper-plane $w^\\top x + b = 0$ that maximizes the margin.\nSet up We first show that the vector $w$ is orthogonal to this hyper-plane. Let $x_1$, $x_2$ be any element on the hyper-plane. So we have $w^\\top x_1 + b = 0$ and $w^\\top x_2 + b = 0$. Then $w^\\top (x_1 - x_2) = 0$, which implies $w$ is orthogonal to the hyper-plane.","keywords":["ML","MATH"],"articleBody":"Linear SVM Idea We want to find a hyper-plane $w^\\top x + b = 0$ that maximizes the margin.\nSet up We first show that the vector $w$ is orthogonal to this hyper-plane. Let $x_1$, $x_2$ be any element on the hyper-plane. So we have $w^\\top x_1 + b = 0$ and $w^\\top x_2 + b = 0$. Then $w^\\top (x_1 - x_2) = 0$, which implies $w$ is orthogonal to the hyper-plane. Now, we set two dashed lines to $w^\\top x + b = 1$ and $w^\\top x + b = -1$. In fact, “$1$” doesn’t matter and we can pick any value here. “$1$” is just the convention.\nNow we pick any line parallel (orthogonal) to $w$ (hyper-plane), then the line intersect two dashed line with point $x^{(+)}$ and $x^{(-)}$. We want to maximize the margin\n$$margin = || x^{(+)} - x^{(-)}||$$\nRe-express margin Since $w$ is parallel to $x^{(+)} - x^{(-)}$, we have $x^{(+)} - x^{(-)}$ = $\\lambda w$ for some $\\lambda$. Then\n$$x^{(+)} = \\lambda w + x^{(-)}$$\nSince $w^\\top x^{(+)} + b = 1$, we have $w^\\top (\\lambda w + x^{(-)}) + b = 1$ and then $\\lambda w^\\top w + w^\\top x^{(-)} + b = 1$. Since $w^\\top x^{(-)} + b = -1$, we have $\\lambda = w^\\top w = 2$. So\n$$ \\lambda = \\frac{2}{w^\\top w}$$\nNow we can rewrite the margin as\n$$ margin = ||(\\lambda w + x^{(-)}) - x^{(-)}|| = ||\\lambda w|| = ||\\frac{2}{w^\\top w} w || = \\frac{2}{||w||}$$\nConstruct an optimization problem we can construct the following objective function for SVM:\nwe can re-write it as\nSoft version of linear SVM Note that the above constraints are hard constraints and it only works if the data are linearly separable.\nTherefore, if data is not linearly separable, we want to make a soft constraint (relaxation). That is, we allow the model to make some error, but we will add some penalty for them.\nNote that if $\\lambda \\to \\infty$, then we allow no error; if $\\lambda = 0$, then we add no penalty. We call $\\epsilon_i$ a slack variable. Ideally, we want $\\epsilon_i = 0$; if it makes an error, $\\epsilon_i  0$.\nConvert to Hinge Loss Since $(w^\\top x^{(i)} + b)y^{(i)} \\geq 1 - \\epsilon_i$, we have $\\epsilon_i \\geq 1-(w^\\top x^{(i)} + b)y^{(i)}$. If $\\epsilon_i \\leq 0$, we have no loss. Otherwise, we add $\\epsilon_i = 1-(w^\\top x^{(i)} + b)y^{(i)}$ as loss. So right now our new objective function is\nStochastic Gradient descent for Hinge Loss objective function:\nNon-linear SVM We are going to map all points through a non-linear function and then used SVM in this transformed space. The idea is that if the non-linear map we use maps the two sets of points such that the two sets of points can be separated by a line after the transformation, then SVM can be used in this transformed space instead of the original space.\nLet $\\phi: \\mathcal{X} \\to \\mathcal{F}$ be the non linear map described in the earlier paragraph, where $\\mathcal{X}$ is the space from which inputs points are coming from and $\\mathcal{F}$ is the transformed space. For SVM to work, we don’t need to know $\\phi$ explicitly, but only need to know the dot product of the transformed points $⟨\\phi(x_i), \\phi(x_j)⟩$. So, instead of working with $\\phi$, they can instead work with $K: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}$ where $K$ takes two points as input and returns a real value that represents $⟨\\phi(x_i), \\phi(x_j)⟩$. Note that $\\phi$ exists only when $K$ is positive definite. With this, we are able to run SVM on an infinite dimensional space.\nGram Matrix: Given a set of vectors in $\\mathcal{V}$, the Gram Matrix is the matrix of all possible inner products in $\\mathcal{V}$. That is, $G_{ij} = v_i \\cdot v_j$.\nCurse of Dimensionality As the dimensionality increases, the classifier’s performance increases until the optimal number of features is reached. Further increasing the dimensionality without increasing the number of training samples results in a decrease in classifier performance. The common theme of these problems is that when the dimensionality increases, the volume of the space increases so fast that the available data become sparse. In order to obtain a statistically sound and reliable result, the amount of data needed to support the result often grows exponentially with the dimensionality. Also, the time complexity of a classification algorithm is proportional to the dimension of the data point. So, higher dimension means larger time complexity (not to mention space complexity to store those large dimensional points).\nMercer’s Theorem (simplified idea): In a finite input space, if the Kernel matrix $\\mathbf{K}$ (also known as Gram matrix) is positive semi-definite ($\\mathbf{K}_{ij} = K(x_i, x_j) = \\phi(x_i) \\cdot \\phi(x_j)$), then the matrix element, i.e. the function K, can be a kernel function.\nExample of a kernel function Let $x_i = (x_{i1}, x_{i2}), x_j = (x_{j1}, x_{j2})$ and $\\phi(x_i)=(x_{i1}^2, \\sqrt{2}x_{i1}x_{i2}, x_{i2}^2), \\phi(x_j)=(x_{j1}^2, \\sqrt{2}x_{j1}x_{j2}, x_{j2}^2)$. Then\nFrom this example, we see that even if the dimension of the feature space is higher than the input space, we can still do the computation in the low dimension input space as long as we choose a good kernel! Therefore, it’s possible to run SVM on an infinite dimensional feature space but do the same amount of computation as in the low dimension input space if we choose a good kernel.\nNote:\n Let $n$ be the dimension of input space, $N (» n)$ be the dimension of the feature space, then if we choose a kernel $K$ properly, we are able to compute the dot product in higher dimensional space but in complexity $O(n)$ instead of $O(N)$. If the classification algorithm is only dependent on the dot product and has no dependency on the actual map $\\phi$, I can use the kernel trick to run the algorithm in high dimensional space with almost no additional cost.  Common Kernel Function Polynomial Kernel $$k(x_i, x_j) = (x_i \\cdot x_j + 1)^d$$\nwhere $d$ is the degree of the polynomial. This type of kernel represents the similarity of vectors in a feature space over polynomials of the original variables. It is popular in natural language processing.\nGaussian Kernel $$k(x, y) = \\text{exp}\\left(-\\frac{|x_i - x_j|^2}{2\\sigma^2}\\right)$$\nThis type of kernel is useful when there is no prior knowledge about the data; it has good performance when there is the assumption og general smoothness of the data. It is an example of the radial basis function kernel (below). $\\sigma$ is the regularization variable that can be tuned specifically for each problem.\nGaussian Radial Basis Function (RBF) $$k(x_i, x_j) = \\text{exp}(-\\gamma |x_i - x_j|^2)$$\nfor $\\gamma  0$. The difference between this kernel and the gaussian kernel is the amount of regularization applied.\nExponential Kernel $$k(x, y) = \\text{exp}\\left(-\\frac{|x_i - x_j|}{2\\sigma^2}\\right)$$\nDual problem of SVM Our primal problem is\nNote that the primal problem of SVM is a convex problem and the constraints are convex. We know that for any convex optimization problem with differentiable objective and constraint functions, any points that satisfy the KKT conditions are primal and dual optimal, and have zero duality gap.\nWe can re-write the primal problem as\nwhere $\\phi(x_i)$ is a map of $x_i$ from input space to feature space. Now\nThen we can use the $4$-th KKT condition (gradient w.r.t. $w, b, \\epsilon_i$ is $0$):\nTherefore we have\nOur dual problem of SVM is\nNote that the maximization only depends on the dot product of $\\phi(x_i), \\phi(x_j)$. We define a function\n$$K(x_i, x_j) = \\phi(x_i) \\cdot \\phi(x_j)$$\nAll we need is the function $K$, a kernel function, which provides with the dot product of two vectors in another space and we don’t need to know the transformation into the other space.\nWe can re-write the problem as\n Reference:\n https://www.quora.com/Why-should-a-kernel-function-satisfy-Mercers-condition https://scikit-learn.org/stable/modules/svm.html#svm-kernels https://kfrankc.com/posts/2019/06/21/kernel-functions-svm# https://towardsdatascience.com/understanding-support-vector-machine-part-2-kernel-trick-mercers-theorem-e1e6848c6c4d https://www.datasciencecentral.com/profiles/blogs/about-the-curse-of-dimensionality https://en.wikipedia.org/wiki/Curse_of_dimensionality https://stats.stackexchange.com/questions/44166/kernelised-k-nearest-neighbour https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec13.pdf  ","wordCount":"1294","inLanguage":"en","datePublished":"2020-04-01T00:00:00Z","dateModified":"2020-04-01T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://tangliyan.com/blog/posts/svm/"},"publisher":{"@type":"Organization","name":"Liyan Tang","logo":{"@type":"ImageObject","url":"https://tangliyan.com/blog/favicon.ico"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://tangliyan.com/blog/ accesskey=h title="Liyan Tang (Alt + H)">Liyan Tang</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
<li>
<a href=https://tangliyan.com/blog/archives title=Archive>
<span>Archive</span>
</a>
</li>
<li>
<a href=https://tangliyan.com/blog/tags/ title=Tags>
<span>Tags</span>
</a>
</li>
<li>
<a href=https://tangliyan.com/blog/search/ title="Search (Alt + /)" accesskey=/>
<span>Search</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<div class=breadcrumbs><a href=https://tangliyan.com/blog/>Home</a>&nbsp;»&nbsp;<a href=https://tangliyan.com/blog/posts/>Posts</a></div>
<h1 class=post-title>
SVM, Dual SVM, Non-linear SVM
</h1>
<div class=post-meta><span title="2020-04-01 00:00:00 +0000 UTC">April 1, 2020</span>&nbsp;·&nbsp;7 min
</div>
</header> <div class=toc>
<details>
<summary accesskey=c title="(Alt + C)">
<span class=details>Table of Contents</span>
</summary>
<div class=inner><ul>
<li>
<a href=#linear-svm aria-label="Linear SVM">Linear SVM</a><ul>
<li>
<a href=#idea aria-label=Idea>Idea</a></li>
<li>
<a href=#set-up aria-label="Set up">Set up</a></li>
<li>
<a href=#re-express-margin aria-label="Re-express margin">Re-express margin</a></li>
<li>
<a href=#construct-an-optimization-problem aria-label="Construct an optimization problem">Construct an optimization problem</a></li>
<li>
<a href=#soft-version-of-linear-svm aria-label="Soft version of linear SVM">Soft version of linear SVM</a></li>
<li>
<a href=#convert-to-hinge-loss aria-label="Convert to Hinge Loss">Convert to Hinge Loss</a></li></ul>
</li>
<li>
<a href=#non-linear-svm aria-label="Non-linear SVM">Non-linear SVM</a><ul>
<li>
<a href=#curse-of-dimensionality aria-label="Curse of Dimensionality">Curse of Dimensionality</a></li>
<li>
<a href=#example-of-a-kernel-function aria-label="Example of a kernel function">Example of a kernel function</a></li></ul>
</li>
<li>
<a href=#common-kernel-function aria-label="Common Kernel Function">Common Kernel Function</a><ul>
<li>
<a href=#polynomial-kernel aria-label="Polynomial Kernel">Polynomial Kernel</a></li>
<li>
<a href=#gaussian-kernel aria-label="Gaussian Kernel">Gaussian Kernel</a></li>
<li>
<a href=#gaussian-radial-basis-function-rbf aria-label="Gaussian Radial Basis Function (RBF)">Gaussian Radial Basis Function (RBF)</a></li>
<li>
<a href=#exponential-kernel aria-label="Exponential Kernel">Exponential Kernel</a></li></ul>
</li>
<li>
<a href=#dual-problem-of-svm aria-label="Dual problem of SVM">Dual problem of SVM</a>
</li>
</ul>
</div>
</details>
</div>
<div class=post-content><h2 id=linear-svm>Linear SVM<a hidden class=anchor aria-hidden=true href=#linear-svm>#</a></h2>
<img src=https://img-blog.csdnimg.cn/20200328002655944.jpg width=600>
<h3 id=idea>Idea<a hidden class=anchor aria-hidden=true href=#idea>#</a></h3>
<p>We want to find a hyper-plane $w^\top x + b = 0$ that maximizes the margin.</p>
<h3 id=set-up>Set up<a hidden class=anchor aria-hidden=true href=#set-up>#</a></h3>
<p>We first show that the vector $w$ is orthogonal to this hyper-plane. Let $x_1$, $x_2$ be any element on the hyper-plane. So we have $w^\top x_1 + b = 0$ and $w^\top x_2 + b = 0$. Then $w^\top (x_1 - x_2) = 0$, which implies $w$ is orthogonal to the hyper-plane. Now, we set two dashed lines to $w^\top x + b = 1$ and $w^\top x + b = -1$. In fact, &ldquo;$1$&rdquo; doesn&rsquo;t matter and we can pick any value here. &ldquo;$1$&rdquo; is just the convention.</p>
<p>Now we pick any line parallel (orthogonal) to $w$ (hyper-plane), then the line intersect two dashed line with point $x^{(+)}$ and $x^{(-)}$. We want to maximize the margin</p>
<p>$$margin = || x^{(+)} - x^{(-)}||$$</p>
<h3 id=re-express-margin>Re-express margin<a hidden class=anchor aria-hidden=true href=#re-express-margin>#</a></h3>
<p>Since $w$ is parallel to $x^{(+)} - x^{(-)}$, we have $x^{(+)} - x^{(-)}$ = $\lambda w$ for some $\lambda$. Then</p>
<p>$$x^{(+)} = \lambda w + x^{(-)}$$</p>
<p>Since $w^\top x^{(+)} + b = 1$, we have $w^\top (\lambda w + x^{(-)}) + b = 1$ and then $\lambda w^\top w + w^\top x^{(-)} + b = 1$. Since $w^\top x^{(-)} + b = -1$, we have $\lambda = w^\top w = 2$. So</p>
<p>$$ \lambda = \frac{2}{w^\top w}$$</p>
<p>Now we can rewrite the margin as</p>
<p>$$ margin = ||(\lambda w + x^{(-)}) - x^{(-)}|| = ||\lambda w|| = ||\frac{2}{w^\top w} w || = \frac{2}{||w||}$$</p>
<h3 id=construct-an-optimization-problem>Construct an optimization problem<a hidden class=anchor aria-hidden=true href=#construct-an-optimization-problem>#</a></h3>
<p>we can construct the following objective function for SVM:</p>
<img src=https://img-blog.csdnimg.cn/20200328002800822.png width=400>
<p>we can re-write it as</p>
<img src=https://img-blog.csdnimg.cn/20200328002820917.png width=400>
<h3 id=soft-version-of-linear-svm>Soft version of linear SVM<a hidden class=anchor aria-hidden=true href=#soft-version-of-linear-svm>#</a></h3>
<p>Note that the above constraints are <em>hard constraints</em> and it only works if the data are linearly separable.</p>
<p>Therefore, if data is not linearly separable, we want to make a soft constraint (relaxation). That is, we allow the model to make some error, but we will add some penalty for them.</p>
<img src=https://img-blog.csdnimg.cn/20200328002840841.png width=500>
<p>Note that if $\lambda \to \infty$, then we allow no error; if $\lambda = 0$, then we add no penalty. We call $\epsilon_i$ a slack variable. Ideally, we want $\epsilon_i = 0$; if it makes an error, $\epsilon_i > 0$.</p>
<h3 id=convert-to-hinge-loss>Convert to Hinge Loss<a hidden class=anchor aria-hidden=true href=#convert-to-hinge-loss>#</a></h3>
<p>Since $(w^\top x^{(i)} + b)y^{(i)} \geq 1 - \epsilon_i$, we have $\epsilon_i \geq 1-(w^\top x^{(i)} + b)y^{(i)}$. If $\epsilon_i \leq 0$, we have no loss. Otherwise, we add $\epsilon_i = 1-(w^\top x^{(i)} + b)y^{(i)}$ as loss. So right now our new objective function is</p>
<img src=https://img-blog.csdnimg.cn/20200328002913718.png width=500>
<p><strong>Stochastic Gradient descent for Hinge Loss objective function</strong>:</p>
<img src=https://img-blog.csdnimg.cn/20200328003339521.jpeg width=500>
<h2 id=non-linear-svm>Non-linear SVM<a hidden class=anchor aria-hidden=true href=#non-linear-svm>#</a></h2>
<p>We are going to map all points through a non-linear function and then used SVM in this transformed space. The idea is that if the non-linear map we use maps the two sets of points such that the two sets of points can be separated by a line after the transformation, then SVM can be used in this transformed space instead of the original space.</p>
<p>Let $\phi: \mathcal{X} \to \mathcal{F}$ be the non linear map described in the earlier paragraph, where $\mathcal{X}$ is the space from which inputs points are coming from and $\mathcal{F}$ is the transformed space. For SVM to work, we don&rsquo;t need to know $\phi$ explicitly, but only need to know the dot product of the transformed points $⟨\phi(x_i), \phi(x_j)⟩$. So, instead of working with $\phi$, they can instead work with $K: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ where $K$ takes two points as input and returns a real value that represents $⟨\phi(x_i), \phi(x_j)⟩$. Note that $\phi$ exists only when $K$ is positive definite. With this, we are able to run SVM on an infinite dimensional space.</p>
<p><strong>Gram Matrix:</strong> Given a set of vectors in $\mathcal{V}$, the Gram Matrix is the matrix of all possible inner products in $\mathcal{V}$. That is, $G_{ij} = v_i \cdot v_j$.</p>
<h3 id=curse-of-dimensionality>Curse of Dimensionality<a hidden class=anchor aria-hidden=true href=#curse-of-dimensionality>#</a></h3>
<p>As the dimensionality increases, the classifier&rsquo;s performance increases until the optimal number of features is reached. Further increasing the dimensionality without increasing the number of training samples results in a decrease in classifier performance. The common theme of these problems is that when the dimensionality increases, the volume of the space increases so fast that the available data become sparse. In order to obtain a statistically sound and reliable result, the amount of data needed to support the result often grows exponentially with the dimensionality. Also, the time complexity of a classification algorithm is proportional to the dimension of the data point. So, higher dimension means larger time complexity (not to mention space complexity to store those large dimensional points).</p>
<p><strong>Mercer&rsquo;s Theorem (simplified idea):</strong> In a finite input space, if the Kernel matrix $\mathbf{K}$ (also known as Gram matrix) is positive semi-definite ($\mathbf{K}_{ij} = K(x_i, x_j) = \phi(x_i) \cdot \phi(x_j)$), then the matrix element, <em>i.e.</em> the function K, can be a kernel function.</p>
<h3 id=example-of-a-kernel-function>Example of a kernel function<a hidden class=anchor aria-hidden=true href=#example-of-a-kernel-function>#</a></h3>
<p>Let $x_i = (x_{i1}, x_{i2}), x_j = (x_{j1}, x_{j2})$ and $\phi(x_i)=(x_{i1}^2, \sqrt{2}x_{i1}x_{i2}, x_{i2}^2), \phi(x_j)=(x_{j1}^2, \sqrt{2}x_{j1}x_{j2}, x_{j2}^2)$. Then</p>
<img src=https://img-blog.csdnimg.cn/20200328003446513.png width=500>
<p>From this example, we see that even if the dimension of the feature space is higher than the input space, we can still do the computation in the low dimension input space as long as we choose a good kernel! <strong>Therefore, it&rsquo;s possible to run SVM on an infinite dimensional feature space but do the same amount of computation as in the low dimension input space if we choose a good kernel.</strong></p>
<p><strong>Note:</strong></p>
<ul>
<li><strong>Let $n$ be the dimension of input space, $N (&#187; n)$ be the dimension of the feature space, then if we choose a kernel $K$ properly, we are able to compute the dot product in higher dimensional space but in complexity $O(n)$ instead of $O(N)$.</strong></li>
<li><strong>If the classification algorithm is only dependent on the dot product and has no dependency on the actual map $\phi$, I can use the kernel trick to run the algorithm in high dimensional space with almost no additional cost.</strong></li>
</ul>
<h2 id=common-kernel-function>Common Kernel Function<a hidden class=anchor aria-hidden=true href=#common-kernel-function>#</a></h2>
<h3 id=polynomial-kernel>Polynomial Kernel<a hidden class=anchor aria-hidden=true href=#polynomial-kernel>#</a></h3>
<p>$$k(x_i, x_j) = (x_i \cdot x_j + 1)^d$$</p>
<p>where $d$ is the degree of the polynomial. This type of kernel represents the similarity of vectors in a feature space over polynomials of the original variables. It is popular in natural language processing.</p>
<h3 id=gaussian-kernel>Gaussian Kernel<a hidden class=anchor aria-hidden=true href=#gaussian-kernel>#</a></h3>
<p>$$k(x, y) = \text{exp}\left(-\frac{|x_i - x_j|^2}{2\sigma^2}\right)$$</p>
<p>This type of kernel is useful when there is no prior knowledge about the data; it has good performance when there is the assumption og general smoothness of the data. It is an example of the radial basis function kernel (below). $\sigma$ is the regularization variable that can be tuned specifically for each problem.</p>
<h3 id=gaussian-radial-basis-function-rbf>Gaussian Radial Basis Function (RBF)<a hidden class=anchor aria-hidden=true href=#gaussian-radial-basis-function-rbf>#</a></h3>
<p>$$k(x_i, x_j) = \text{exp}(-\gamma |x_i - x_j|^2)$$</p>
<p>for $\gamma > 0$. The difference between this kernel and the gaussian kernel is the amount of regularization applied.</p>
<h3 id=exponential-kernel>Exponential Kernel<a hidden class=anchor aria-hidden=true href=#exponential-kernel>#</a></h3>
<p>$$k(x, y) = \text{exp}\left(-\frac{|x_i - x_j|}{2\sigma^2}\right)$$</p>
<h2 id=dual-problem-of-svm>Dual problem of SVM<a hidden class=anchor aria-hidden=true href=#dual-problem-of-svm>#</a></h2>
<p>Our primal problem is</p>
<img src=https://img-blog.csdnimg.cn/20200328003834999.png width=500>
<p>Note that the primal problem of SVM is a convex problem and the constraints are convex. We know that for any convex optimization problem with differentiable objective and constraint functions, any points that satisfy the KKT conditions are primal and dual optimal, and have zero duality gap.</p>
<p>We can re-write the primal problem as</p>
<img src=https://img-blog.csdnimg.cn/20200328003905975.png width=500>
<p>where $\phi(x_i)$ is a map of $x_i$ from input space to feature space. Now</p>
<img src=https://img-blog.csdnimg.cn/20200328003930682.png width=800>
<p>Then we can use the $4$-th KKT condition (gradient w.r.t. $w, b, \epsilon_i$ is $0$):</p>
<img src=https://img-blog.csdnimg.cn/20200328003947720.png width=600>
<p>Therefore we have</p>
<img src=https://img-blog.csdnimg.cn/20200328004005674.png width=800>
<p>Our dual problem of SVM is</p>
<img src=https://img-blog.csdnimg.cn/20200328004036594.png width=800>
<p>Note that the maximization only depends on the dot product of $\phi(x_i), \phi(x_j)$. We define a function</p>
<p>$$K(x_i, x_j) = \phi(x_i) \cdot \phi(x_j)$$</p>
<p>All we need is the function $K$, a kernel function, which provides with the dot product of two vectors in another space and we don&rsquo;t need to know the transformation into the other space.</p>
<p>We can re-write the problem as</p>
<img src=https://img-blog.csdnimg.cn/20200328004054885.png width=700>
<hr>
<p>Reference:</p>
<ul>
<li><a href=https://www.quora.com/Why-should-a-kernel-function-satisfy-Mercers-condition>https://www.quora.com/Why-should-a-kernel-function-satisfy-Mercers-condition</a></li>
<li><a href=https://scikit-learn.org/stable/modules/svm.html#svm-kernels>https://scikit-learn.org/stable/modules/svm.html#svm-kernels</a></li>
<li><a href=https://kfrankc.com/posts/2019/06/21/kernel-functions-svm#>https://kfrankc.com/posts/2019/06/21/kernel-functions-svm#</a></li>
<li><a href=https://towardsdatascience.com/understanding-support-vector-machine-part-2-kernel-trick-mercers-theorem-e1e6848c6c4d>https://towardsdatascience.com/understanding-support-vector-machine-part-2-kernel-trick-mercers-theorem-e1e6848c6c4d</a></li>
<li><a href=https://www.datasciencecentral.com/profiles/blogs/about-the-curse-of-dimensionality>https://www.datasciencecentral.com/profiles/blogs/about-the-curse-of-dimensionality</a></li>
<li><a href=https://en.wikipedia.org/wiki/Curse_of_dimensionality>https://en.wikipedia.org/wiki/Curse_of_dimensionality</a></li>
<li><a href=https://stats.stackexchange.com/questions/44166/kernelised-k-nearest-neighbour>https://stats.stackexchange.com/questions/44166/kernelised-k-nearest-neighbour</a></li>
<li><a href=https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec13.pdf>https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec13.pdf</a></li>
</ul>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=https://tangliyan.com/blog/tags/ml/>ML</a></li>
<li><a href=https://tangliyan.com/blog/tags/math/>MATH</a></li>
</ul>
<nav class=paginav>
<a class=prev href=https://tangliyan.com/blog/posts/kaggle_google_quest/>
<span class=title>« Prev Page</span>
<br>
<span>Kaggle: Google Quest Q&A Labeling - my solution</span>
</a>
<a class=next href=https://tangliyan.com/blog/posts/convex2/>
<span class=title>Next Page »</span>
<br>
<span>Introduction to Convex Optimization - Primal problem to Dual problem</span>
</a>
</nav>
<div class=share-buttons>
<a target=_blank rel="noopener noreferrer" aria-label="share SVM, Dual SVM, Non-linear SVM on twitter" href="https://twitter.com/intent/tweet/?text=SVM%2c%20Dual%20SVM%2c%20Non-linear%20SVM&url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fsvm%2f&hashtags=ML%2cMATH"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share SVM, Dual SVM, Non-linear SVM on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fsvm%2f&title=SVM%2c%20Dual%20SVM%2c%20Non-linear%20SVM&summary=SVM%2c%20Dual%20SVM%2c%20Non-linear%20SVM&source=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fsvm%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share SVM, Dual SVM, Non-linear SVM on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fsvm%2f&title=SVM%2c%20Dual%20SVM%2c%20Non-linear%20SVM"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share SVM, Dual SVM, Non-linear SVM on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fsvm%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share SVM, Dual SVM, Non-linear SVM on whatsapp" href="https://api.whatsapp.com/send?text=SVM%2c%20Dual%20SVM%2c%20Non-linear%20SVM%20-%20https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fsvm%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share SVM, Dual SVM, Non-linear SVM on telegram" href="https://telegram.me/share/url?text=SVM%2c%20Dual%20SVM%2c%20Non-linear%20SVM&url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fsvm%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a>
</div>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2022 <a href=https://tangliyan.com/blog/>Liyan Tang</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
<script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerText='copy';function d(){a.innerText='copied!',setTimeout(()=>{a.innerText='copy'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script>
</body>
</html>