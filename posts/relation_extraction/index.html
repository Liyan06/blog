<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Overlook of Relation Extraction | Liyan Tang</title>
<meta name=keywords content="NLP,RE">
<meta name=description content="Information Extraction v.s. Relation Extraction Information Extraction: Information extraction is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents and other electronically represented sources.
Relation extraction (RE) is an important task in IE. It focuses on extracting relations between entities. A complete relation RE system consists of
 a named entity recognizer to identify named entities from text. an entity linker to link entities to existing knowledge graphs.">
<meta name=author content>
<link rel=canonical href=https://tangliyan.com/blog/posts/relation_extraction/>
<link crossorigin=anonymous href=/blog/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css integrity="sha256-yIlj/i15RiAA/Q+xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/blog/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://tangliyan.com/blog/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://tangliyan.com/blog/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://tangliyan.com/blog/favicon-32x32.png>
<link rel=apple-touch-icon href=https://tangliyan.com/blog/apple-touch-icon.png>
<link rel=mask-icon href=https://tangliyan.com/blog/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.92.0">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-202974782-1','auto'),ga('send','pageview'))</script><meta property="og:title" content="Overlook of Relation Extraction">
<meta property="og:description" content="Information Extraction v.s. Relation Extraction Information Extraction: Information extraction is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents and other electronically represented sources.
Relation extraction (RE) is an important task in IE. It focuses on extracting relations between entities. A complete relation RE system consists of
 a named entity recognizer to identify named entities from text. an entity linker to link entities to existing knowledge graphs.">
<meta property="og:type" content="article">
<meta property="og:url" content="https://tangliyan.com/blog/posts/relation_extraction/"><meta property="og:image" content="https://tangliyan.com/blog/papermod-cover.png"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2021-01-03T00:00:00+00:00">
<meta property="article:modified_time" content="2021-01-03T00:00:00+00:00">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://tangliyan.com/blog/papermod-cover.png">
<meta name=twitter:title content="Overlook of Relation Extraction">
<meta name=twitter:description content="Information Extraction v.s. Relation Extraction Information Extraction: Information extraction is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents and other electronically represented sources.
Relation extraction (RE) is an important task in IE. It focuses on extracting relations between entities. A complete relation RE system consists of
 a named entity recognizer to identify named entities from text. an entity linker to link entities to existing knowledge graphs.">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://tangliyan.com/blog/posts/"},{"@type":"ListItem","position":2,"name":"Overlook of Relation Extraction","item":"https://tangliyan.com/blog/posts/relation_extraction/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Overlook of Relation Extraction","name":"Overlook of Relation Extraction","description":"Information Extraction v.s. Relation Extraction Information Extraction: Information extraction is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents and other electronically represented sources.\nRelation extraction (RE) is an important task in IE. It focuses on extracting relations between entities. A complete relation RE system consists of\n a named entity recognizer to identify named entities from text. an entity linker to link entities to existing knowledge graphs.","keywords":["NLP","RE"],"articleBody":"Information Extraction v.s. Relation Extraction Information Extraction: Information extraction is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents and other electronically represented sources.\nRelation extraction (RE) is an important task in IE. It focuses on extracting relations between entities. A complete relation RE system consists of\n a named entity recognizer to identify named entities from text. an entity linker to link entities to existing knowledge graphs. a relational classifier to determine relations between entities by given context (most difficult and important).  Existing Works of RE RE methods follows the typical supervised setting, from early pattern-based methods, statistical approaches, to recent neural models.\nPattern-based Methods The early methods use sentence analysis tools to identify syntactic elements in text, then automatically construct pattern rules from these elements. Later work involves larger corpora, more formats of patterns and more efficient ways of extraction.\nStatistical Relation Extraction Models Statistical Methods requires less human efforts so that statistical relation extraction (SRE) has been extensively studied. Approaches includes:\n feature-based methods which design lexical, syntactic and semantic features for entity pairs and their corresponding context (hard to design features). kernel-based methods measures the similarities between relation representations and textual instances (hard to design kernel functions). Graphical methods abstract the dependencies between entities, text and relations in the form of directed acyclic graphs, and then use inference models to identify the correct relations (still limited model capacities). embedding models. Encode text into low-dimensional semantic spaces and extract relations from textual embeddings. E.g. Knowledge Graph (KG) embeddings.  Neural Relation Extraction Methods The performance of SOTA RE models. The adoption of neural models began in 2013. Neural relation extraction (NRE) models can effectively capture textual information and generalize to wider range of data. NRE mainly utilizes both word embeddings and positional embeddings and focus on designing and utilizing various network architectures to capture the relational semantics within text. Methods includes:\n Recursive Neural Networks. Learn compositional representations for sentences recursively. Convolutional neural networks (CNNs). Model local textual patterns. Recurrent neural networks (RNNs). Handle long sequential data. Graph neural networks (GNNs). Build word/entity graphs for reasoning. Attention-based neural networks. Aggregate global relational information.  Currently, Transformers and Pre-trained LM models achieves SOTA on intra-sentence RE.\nFuture Directions Despite the success of existing RE methods, most of them still work in a simplified setting. These methods mainly focus on training models with large amounts of human annotations to classify two given entities within one sentence into pre-defined relations. However, the real world is much more complicated than this simple setting:\n collecting high-quality human annotations is expensive and time-consuming. many long-tail relations cannot provide large amounts of training examples. most facts are expressed by long context consisting of multiple sentences. using a pre-defined set to cover those relations with open-ended growth is difficult.  Utilizing More Data The researchers have formed a consensus that utilizing more data is a potential way towards more powerful RE models.\nDistant supervision (DS) assumption has been used to automatically label data by aligning existing KGs with plain text. For any entity pair in KGs, sentences mentioning both the entities will be labeled with their corresponding relations in KGs.\nHere is an illustration of DS relation extraction. With the fact (Apple Inc., product, iPhone), DS finds all sentences mentioning the two entities and annotates them with the relation product, which inevitably brings noise labels. Methods to Denoise DS Data  Adopt multi-instance learning by combining sentences with same entity pairs and then selecting informative instances from them. Incorporating extra context information such as KGs. Utilize sophisticated mechanisms and training strategies to enhance distantly supervised NRE models.  Open Problem for Utilizing More Data  Existing DS methods focus on denoising auto-labeled instances. Explore better DS schemes is valuable. Perform unsupervised or semi-supervised learning for utilizing large-scale unlabeled data as well as using knowledge from KGs and introducing human experts in the loop.  Performing More Efficient Learning Real-world relation distributions are long-tail and most relations have very limited relational facts and corresponding sentences. We can see the long tail distributions from two DS datasets in the following figure:\nFew-shot Learning Few-shot learning is a good fit for learning long-tail relations efficiently. It trains good representations of instances or learns ways of fast adaptation from existing large-scale data, and then transfer to new tasks.\nA typical few-shot learning setting is the N-way K-shot setting, where models are given N random-sampled new relations, along with K training examples for each relation. Here is an example. Give a few instances for new relation types, few-shot RE models classify query sentences into one of the given relations.\nHere are a few challenges for few-shot learning:\n Few-shot domain adaptation. Few-shot none-of-the-above detection. Conventional few-shot models have the difficulty to form a good representation for the none-of-the-above (NOTA) relation in the N-way K-shot setting. Therefore, it is crucial to study how to identify NOTA instances . Few-shot RE may result in a easy classification task if total amount of relations is small. As the following figure shows, as the number of relations increase, the performance drops. Current models cannot truly understand relations similar in semantics.   Handling More Complicated Context Most existing methods focus on intra-sentence RE and thus are inadequate for identifying relational facts expressed in a long document. Extracting relations from complicated context is a challenging task requiring reading, memorizing and reasoning for discovering relational facts across multiple sentences.\nHere is a post (Connecting the Dots: Document-level Neural Relation Extraction with Edge-oriented Graphs) I wrote for explaining using GNN on document-level relation extraction task.\nOrienting More Open Domains Our world undergoes growth of relations and it is not possible to pre-specify all relations by human experts. Thus, we need RE systems that do not rely on pre-defined relation schemas and can work in open scenarios. Here are current explorations in handling open relations:\n Open information extraction (Open IE), which extracts relation phrases and arguments (entities) from text.  Relation discovery, which discover unseen relation types from unsupervised data. Here is an example of casting relation discovery as a clustering task.   Note: There are many redundant extracted relations. Normalizing these phrases is crucial for downstream tasks. For example, relations on (Barack Obama, was born in , Honolulu) and (Obama, place of birth, Honolulu) are actually identical. So they should be normalized.\n Reference:\n More Data, More Relations, More Context and More Openness: A Review and Outlook for Relation Extraction. https://arxiv.org/pdf/2004.03186.pdf. Connecting the Dots: Document-level Neural Relation Extraction with Edge-oriented Graphs. https://arxiv.org/pdf/1909.00228.pdf. Distant supervision for relation extraction without labeled data. https://www.aclweb.org/anthology/P09-1113.pdf.  ","wordCount":"1082","inLanguage":"en","datePublished":"2021-01-03T00:00:00Z","dateModified":"2021-01-03T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://tangliyan.com/blog/posts/relation_extraction/"},"publisher":{"@type":"Organization","name":"Liyan Tang","logo":{"@type":"ImageObject","url":"https://tangliyan.com/blog/favicon.ico"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://tangliyan.com/blog/ accesskey=h title="Liyan Tang (Alt + H)">Liyan Tang</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
<li>
<a href=https://tangliyan.com/blog/archives title=Archive>
<span>Archive</span>
</a>
</li>
<li>
<a href=https://tangliyan.com/blog/tags/ title=Tags>
<span>Tags</span>
</a>
</li>
<li>
<a href=https://tangliyan.com/blog/search/ title="Search (Alt + /)" accesskey=/>
<span>Search</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<div class=breadcrumbs><a href=https://tangliyan.com/blog/>Home</a>&nbsp;»&nbsp;<a href=https://tangliyan.com/blog/posts/>Posts</a></div>
<h1 class=post-title>
Overlook of Relation Extraction
</h1>
<div class=post-meta><span title="2021-01-03 00:00:00 +0000 UTC">January 3, 2021</span>&nbsp;·&nbsp;6 min
</div>
</header> <div class=toc>
<details>
<summary accesskey=c title="(Alt + C)">
<span class=details>Table of Contents</span>
</summary>
<div class=inner><ul>
<li>
<a href=#information-extraction-vs-relation-extraction aria-label="Information Extraction v.s. Relation Extraction">Information Extraction v.s. Relation Extraction</a></li>
<li>
<a href=#existing-works-of-re aria-label="Existing Works of RE">Existing Works of RE</a><ul>
<li>
<a href=#pattern-based-methods aria-label="Pattern-based Methods">Pattern-based Methods</a></li>
<li>
<a href=#statistical-relation-extraction-models aria-label="Statistical Relation Extraction Models">Statistical Relation Extraction Models</a></li>
<li>
<a href=#neural-relation-extraction-methods aria-label="Neural Relation Extraction Methods">Neural Relation Extraction Methods</a></li></ul>
</li>
<li>
<a href=#future-directions aria-label="Future Directions">Future Directions</a><ul>
<li>
<a href=#utilizing-more-data aria-label="Utilizing More Data">Utilizing More Data</a><ul>
<li>
<a href=#methods-to-denoise-ds-data aria-label="Methods to Denoise DS Data">Methods to Denoise DS Data</a></li>
<li>
<a href=#open-problem-for-utilizing-more-data aria-label="Open Problem for Utilizing More Data">Open Problem for Utilizing More Data</a></li></ul>
</li>
<li>
<a href=#performing-more-efficient-learning aria-label="Performing More Efficient Learning">Performing More Efficient Learning</a><ul>
<li>
<a href=#few-shot-learning aria-label="Few-shot Learning">Few-shot Learning</a></li></ul>
</li>
<li>
<a href=#handling-more-complicated-context aria-label="Handling More Complicated Context">Handling More Complicated Context</a></li>
<li>
<a href=#orienting-more-open-domains aria-label="Orienting More Open Domains">Orienting More Open Domains</a>
</li>
</ul>
</li>
</ul>
</div>
</details>
</div>
<div class=post-content><h2 id=information-extraction-vs-relation-extraction>Information Extraction v.s. Relation Extraction<a hidden class=anchor aria-hidden=true href=#information-extraction-vs-relation-extraction>#</a></h2>
<p><strong>Information Extraction:</strong> Information extraction is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents and other electronically represented sources.</p>
<p>Relation extraction (RE) is an important task in IE. It focuses on extracting relations between entities. A complete relation RE system consists of</p>
<ol>
<li>a named entity recognizer to identify named entities from text.</li>
<li>an entity linker to link entities to existing knowledge graphs.</li>
<li>a relational classifier to determine relations between entities by given context (most difficult and important).</li>
</ol>
<img src="https://img-blog.csdnimg.cn/20210103011251539.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_10,color_FFFFFF,t_70#pic_center" width=550>
<h2 id=existing-works-of-re>Existing Works of RE<a hidden class=anchor aria-hidden=true href=#existing-works-of-re>#</a></h2>
<p>RE methods follows the typical supervised setting, from early <strong>pattern-based methods, statistical approaches, to recent neural models.</strong></p>
<h3 id=pattern-based-methods>Pattern-based Methods<a hidden class=anchor aria-hidden=true href=#pattern-based-methods>#</a></h3>
<p>The early methods use sentence analysis tools to identify syntactic elements in text, then automatically construct pattern rules from these elements. Later work involves larger corpora, more formats of patterns and more efficient ways of extraction.</p>
<h3 id=statistical-relation-extraction-models>Statistical Relation Extraction Models<a hidden class=anchor aria-hidden=true href=#statistical-relation-extraction-models>#</a></h3>
<p>Statistical Methods requires less human efforts so that statistical relation extraction (SRE) has been extensively studied. Approaches includes:</p>
<ul>
<li><strong>feature-based methods</strong> which design lexical, syntactic and semantic features for entity pairs and their corresponding context (hard to design features).</li>
<li><strong>kernel-based methods</strong> measures the similarities between relation representations and textual instances (hard to design kernel functions).</li>
<li><strong>Graphical methods</strong> abstract the dependencies between entities, text and relations in the form of directed acyclic graphs, and then use inference models to identify the correct relations (still limited model capacities).</li>
<li><strong>embedding models</strong>. Encode text into low-dimensional semantic spaces and extract relations from textual embeddings. E.g. Knowledge Graph (KG) embeddings.</li>
</ul>
<h3 id=neural-relation-extraction-methods>Neural Relation Extraction Methods<a hidden class=anchor aria-hidden=true href=#neural-relation-extraction-methods>#</a></h3>
<p>The performance of SOTA RE models. The adoption of neural models began in 2013.
<img src="https://img-blog.csdnimg.cn/20210103010811185.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_10,color_FFFFFF,t_70#pic_center" width=550></p>
<p>Neural relation extraction (NRE) models can effectively capture textual information and generalize to wider range of data. NRE mainly utilizes both word embeddings and positional embeddings and focus on designing and utilizing various network architectures to capture the relational semantics within text. Methods includes:</p>
<ul>
<li>Recursive Neural Networks. Learn compositional representations for sentences recursively.</li>
<li>Convolutional neural networks (CNNs). Model local textual patterns.</li>
<li>Recurrent neural networks (RNNs). Handle long sequential data.</li>
<li>Graph neural networks (GNNs). Build word/entity graphs for reasoning.</li>
<li>Attention-based neural networks. Aggregate global relational information.</li>
</ul>
<p>Currently, Transformers and Pre-trained LM models achieves SOTA on <em>intra-sentence</em> RE.</p>
<h2 id=future-directions>Future Directions<a hidden class=anchor aria-hidden=true href=#future-directions>#</a></h2>
<p>Despite the success of existing RE methods, most of them still work in a simplified setting. These methods mainly focus on training models with large amounts of human annotations to classify two given entities within one sentence into pre-defined relations. However, the real world is much more complicated than this simple setting:</p>
<ul>
<li>collecting high-quality human annotations is expensive and time-consuming.</li>
<li>many long-tail relations cannot provide large amounts of training examples.</li>
<li>most facts are expressed by long context consisting of multiple sentences.</li>
<li>using a pre-defined set to cover those relations with open-ended growth is difficult.</li>
</ul>
<h3 id=utilizing-more-data>Utilizing More Data<a hidden class=anchor aria-hidden=true href=#utilizing-more-data>#</a></h3>
<p>The researchers have formed a consensus that utilizing more data is a potential way towards more powerful RE models.</p>
<p>Distant supervision (DS) assumption has been used to automatically label data by aligning existing KGs with plain text. For any entity pair in KGs, sentences mentioning both the entities will be labeled with their corresponding relations in KGs.</p>
<p>Here is an illustration of DS relation extraction. With the fact (<em>Apple Inc.</em>, <code>product</code>, <em>iPhone</em>), DS finds all sentences mentioning the two entities and annotates them with the relation <code>product</code>, <strong>which inevitably brings noise labels</strong>.
<img src="https://img-blog.csdnimg.cn/20210103012938136.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_10,color_FFFFFF,t_70#pic_center" width=550></p>
<h4 id=methods-to-denoise-ds-data>Methods to Denoise DS Data<a hidden class=anchor aria-hidden=true href=#methods-to-denoise-ds-data>#</a></h4>
<ul>
<li>Adopt multi-instance learning by combining sentences with same entity pairs and then selecting informative instances from them.</li>
<li>Incorporating extra context information such as KGs.</li>
<li>Utilize sophisticated mechanisms and training strategies to enhance distantly supervised NRE models.</li>
</ul>
<h4 id=open-problem-for-utilizing-more-data>Open Problem for Utilizing More Data<a hidden class=anchor aria-hidden=true href=#open-problem-for-utilizing-more-data>#</a></h4>
<ul>
<li>Existing DS methods focus on denoising auto-labeled instances. Explore better DS schemes is valuable.</li>
<li>Perform unsupervised or semi-supervised learning for utilizing large-scale unlabeled data as well as using knowledge from KGs and introducing human experts in the loop.</li>
</ul>
<h3 id=performing-more-efficient-learning>Performing More Efficient Learning<a hidden class=anchor aria-hidden=true href=#performing-more-efficient-learning>#</a></h3>
<p>Real-world relation distributions are long-tail and most relations have very limited relational facts and corresponding sentences. We can see the long tail distributions from two DS datasets in the following figure:</p>
<img src="https://img-blog.csdnimg.cn/2021010311282422.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_16,color_FFFFFF,t_70#pic_center" width=550>
<h4 id=few-shot-learning>Few-shot Learning<a hidden class=anchor aria-hidden=true href=#few-shot-learning>#</a></h4>
<p><strong>Few-shot learning</strong> is a good fit for learning long-tail relations efficiently. It trains good representations of instances or learns ways of fast adaptation from existing large-scale data, and then transfer to new tasks.</p>
<p>A typical few-shot learning setting is the <strong>N-way K-shot</strong> setting, where models are given N random-sampled new relations, along with K training examples for each relation. Here is an example. Give a few instances for new relation types, few-shot RE models classify query sentences into one of the given relations.</p>
<img src="https://img-blog.csdnimg.cn/20210103113552226.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_10,color_FFFFFF,t_70#pic_center" width=550>
<p>Here are a few challenges for few-shot learning:</p>
<ul>
<li><strong>Few-shot domain adaptation</strong>.</li>
<li><strong>Few-shot none-of-the-above detection</strong>. Conventional few-shot models have the difficulty to form a good representation for the none-of-the-above (NOTA) relation in the N-way K-shot setting. Therefore, it is crucial to study how to identify NOTA instances .</li>
<li><strong>Few-shot RE may result in a easy classification task if total amount of relations is small</strong>. As the following figure shows, as the number of relations increase, the performance drops.</li>
<li><strong>Current models cannot truly understand relations similar in semantics</strong>.
<img src="https://img-blog.csdnimg.cn/20210103114721687.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_16,color_FFFFFF,t_70#pic_center" width=550></li>
</ul>
<h3 id=handling-more-complicated-context>Handling More Complicated Context<a hidden class=anchor aria-hidden=true href=#handling-more-complicated-context>#</a></h3>
<p>Most existing methods focus on intra-sentence RE and thus are inadequate for identifying relational facts expressed in a long document. Extracting relations from complicated context is a challenging task requiring reading, memorizing and reasoning for discovering relational facts across multiple sentences.</p>
<p>Here is a post (<a href=./posts/connecting_the_dots>Connecting the Dots: Document-level Neural Relation Extraction with Edge-oriented Graphs</a>) I wrote for explaining using GNN on document-level relation extraction task.</p>
<h3 id=orienting-more-open-domains>Orienting More Open Domains<a hidden class=anchor aria-hidden=true href=#orienting-more-open-domains>#</a></h3>
<p>Our world undergoes growth of relations and it is not possible to pre-specify all relations by human experts. Thus, we need RE systems that do not rely on pre-defined relation schemas and can work in open scenarios. Here are current explorations in handling open relations:</p>
<ul>
<li><strong>Open information extraction (Open IE)</strong>, which extracts relation phrases and arguments (entities) from text.
<img src=https://img-blog.csdnimg.cn/20210103120825967.png#pic_center width=550></li>
<li><strong>Relation discovery</strong>, which discover unseen relation types from unsupervised data. Here is an example of casting relation discovery as a clustering task.
<img src="https://img-blog.csdnimg.cn/20210103121202584.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_16,color_FFFFFF,t_70#pic_center" width=550></li>
</ul>
<p>Note: There are many redundant extracted relations. Normalizing these phrases is crucial for downstream tasks. For example, relations on (<em>Barack Obama</em>, <code>was born in</code> , <em>Honolulu</em>) and (<em>Obama</em>, <code>place of birth</code>, <em>Honolulu</em>) are actually identical. So they should be normalized.</p>
<hr>
<p>Reference:</p>
<ul>
<li>More Data, More Relations, More Context and More Openness: A Review and Outlook for Relation Extraction. <a href=https://arxiv.org/pdf/2004.03186.pdf>https://arxiv.org/pdf/2004.03186.pdf</a>.</li>
<li>Connecting the Dots: Document-level Neural Relation Extraction with Edge-oriented Graphs. <a href=https://arxiv.org/pdf/1909.00228.pdf>https://arxiv.org/pdf/1909.00228.pdf</a>.</li>
<li>Distant supervision for relation extraction without labeled data. <a href=https://www.aclweb.org/anthology/P09-1113.pdf>https://www.aclweb.org/anthology/P09-1113.pdf</a>.</li>
</ul>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=https://tangliyan.com/blog/tags/nlp/>NLP</a></li>
<li><a href=https://tangliyan.com/blog/tags/re/>RE</a></li>
</ul>
<nav class=paginav>
<a class=prev href=https://tangliyan.com/blog/posts/decoupling_representation/>
<span class=title>« Prev Page</span>
<br>
<span>Paper Review - Decoupling Representation and Classifier for Long-Tailed Recognition</span>
</a>
<a class=next href=https://tangliyan.com/blog/posts/connecting_the_dots/>
<span class=title>Next Page »</span>
<br>
<span>Paper Review - Connecting the Dots: Document-level Neural Relation Extraction with Edge-oriented Graphs</span>
</a>
</nav>
<div class=share-buttons>
<a target=_blank rel="noopener noreferrer" aria-label="share Overlook of Relation Extraction on twitter" href="https://twitter.com/intent/tweet/?text=Overlook%20of%20Relation%20Extraction&url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2frelation_extraction%2f&hashtags=NLP%2cRE"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Overlook of Relation Extraction on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2frelation_extraction%2f&title=Overlook%20of%20Relation%20Extraction&summary=Overlook%20of%20Relation%20Extraction&source=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2frelation_extraction%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Overlook of Relation Extraction on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2frelation_extraction%2f&title=Overlook%20of%20Relation%20Extraction"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Overlook of Relation Extraction on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2frelation_extraction%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Overlook of Relation Extraction on whatsapp" href="https://api.whatsapp.com/send?text=Overlook%20of%20Relation%20Extraction%20-%20https%3a%2f%2ftangliyan.com%2fblog%2fposts%2frelation_extraction%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Overlook of Relation Extraction on telegram" href="https://telegram.me/share/url?text=Overlook%20of%20Relation%20Extraction&url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2frelation_extraction%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a>
</div>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2022 <a href=https://tangliyan.com/blog/>Liyan Tang</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
<script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerText='copy';function d(){a.innerText='copied!',setTimeout(()=>{a.innerText='copy'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script>
</body>
</html>