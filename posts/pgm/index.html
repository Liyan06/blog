<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Probabilistic Graphical Model (PGM) | Liyan Tang</title>
<meta name=keywords content="ML,MATH">
<meta name=description content="Probabilistic Graphical Model (PGM) Definition: A probabilistic graphical model is a probabilistic model for which a graph expresses the conditional dependence structure between random variables.
In general, PGM obeys following rules: $$ \begin{aligned} &\text {Sum Rule : } p\left(x_{1}\right)=\int p\left(x_{1}, x_{2}\right) d x_{2}\\ &\text {Product Rule : } p\left(x_{1}, x_{2}\right)=p\left(x_{1} | x_{2}\right) p\left(x_{2}\right)\\ &\text {Chain Rule: } p\left(x_{1}, x_{2}, \cdots, x_{p}\right)=\prod_{i=1}^{p} p\left(x_{i} | x_{i+1, x_{i+2}} \ldots x_{p}\right)\\ &\text {Bayesian Rule: } p\left(x_{1} | x_{2}\right)=\frac{p\left(x_{2} | x_{1}\right) p\left(x_{1}\right)}{p\left(x_{2}\right)} \end{aligned} $$">
<meta name=author content>
<link rel=canonical href=https://tangliyan.com/blog/posts/pgm/>
<link crossorigin=anonymous href=/blog/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css integrity="sha256-yIlj/i15RiAA/Q+xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/blog/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://tangliyan.com/blog/favicon.ico>
<link rel=icon type=image/png sizes=16x16 href=https://tangliyan.com/blog/favicon-16x16.png>
<link rel=icon type=image/png sizes=32x32 href=https://tangliyan.com/blog/favicon-32x32.png>
<link rel=apple-touch-icon href=https://tangliyan.com/blog/apple-touch-icon.png>
<link rel=mask-icon href=https://tangliyan.com/blog/safari-pinned-tab.svg>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.92.0">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript>
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-202974782-1','auto'),ga('send','pageview'))</script><meta property="og:title" content="Probabilistic Graphical Model (PGM)">
<meta property="og:description" content="Probabilistic Graphical Model (PGM) Definition: A probabilistic graphical model is a probabilistic model for which a graph expresses the conditional dependence structure between random variables.
In general, PGM obeys following rules: $$ \begin{aligned} &\text {Sum Rule : } p\left(x_{1}\right)=\int p\left(x_{1}, x_{2}\right) d x_{2}\\ &\text {Product Rule : } p\left(x_{1}, x_{2}\right)=p\left(x_{1} | x_{2}\right) p\left(x_{2}\right)\\ &\text {Chain Rule: } p\left(x_{1}, x_{2}, \cdots, x_{p}\right)=\prod_{i=1}^{p} p\left(x_{i} | x_{i+1, x_{i+2}} \ldots x_{p}\right)\\ &\text {Bayesian Rule: } p\left(x_{1} | x_{2}\right)=\frac{p\left(x_{2} | x_{1}\right) p\left(x_{1}\right)}{p\left(x_{2}\right)} \end{aligned} $$">
<meta property="og:type" content="article">
<meta property="og:url" content="https://tangliyan.com/blog/posts/pgm/"><meta property="og:image" content="https://tangliyan.com/blog/papermod-cover.png"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2020-05-11T00:00:00+00:00">
<meta property="article:modified_time" content="2020-05-11T00:00:00+00:00">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://tangliyan.com/blog/papermod-cover.png">
<meta name=twitter:title content="Probabilistic Graphical Model (PGM)">
<meta name=twitter:description content="Probabilistic Graphical Model (PGM) Definition: A probabilistic graphical model is a probabilistic model for which a graph expresses the conditional dependence structure between random variables.
In general, PGM obeys following rules: $$ \begin{aligned} &\text {Sum Rule : } p\left(x_{1}\right)=\int p\left(x_{1}, x_{2}\right) d x_{2}\\ &\text {Product Rule : } p\left(x_{1}, x_{2}\right)=p\left(x_{1} | x_{2}\right) p\left(x_{2}\right)\\ &\text {Chain Rule: } p\left(x_{1}, x_{2}, \cdots, x_{p}\right)=\prod_{i=1}^{p} p\left(x_{i} | x_{i+1, x_{i+2}} \ldots x_{p}\right)\\ &\text {Bayesian Rule: } p\left(x_{1} | x_{2}\right)=\frac{p\left(x_{2} | x_{1}\right) p\left(x_{1}\right)}{p\left(x_{2}\right)} \end{aligned} $$">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://tangliyan.com/blog/posts/"},{"@type":"ListItem","position":2,"name":"Probabilistic Graphical Model (PGM)","item":"https://tangliyan.com/blog/posts/pgm/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Probabilistic Graphical Model (PGM)","name":"Probabilistic Graphical Model (PGM)","description":"Probabilistic Graphical Model (PGM) Definition: A probabilistic graphical model is a probabilistic model for which a graph expresses the conditional dependence structure between random variables.\nIn general, PGM obeys following rules: $$ \\begin{aligned} \u0026amp;\\text {Sum Rule : } p\\left(x_{1}\\right)=\\int p\\left(x_{1}, x_{2}\\right) d x_{2}\\\\ \u0026amp;\\text {Product Rule : } p\\left(x_{1}, x_{2}\\right)=p\\left(x_{1} | x_{2}\\right) p\\left(x_{2}\\right)\\\\ \u0026amp;\\text {Chain Rule: } p\\left(x_{1}, x_{2}, \\cdots, x_{p}\\right)=\\prod_{i=1}^{p} p\\left(x_{i} | x_{i+1, x_{i+2}} \\ldots x_{p}\\right)\\\\ \u0026amp;\\text {Bayesian Rule: } p\\left(x_{1} | x_{2}\\right)=\\frac{p\\left(x_{2} | x_{1}\\right) p\\left(x_{1}\\right)}{p\\left(x_{2}\\right)} \\end{aligned} $$","keywords":["ML","MATH"],"articleBody":"Probabilistic Graphical Model (PGM) Definition: A probabilistic graphical model is a probabilistic model for which a graph expresses the conditional dependence structure between random variables.\nIn general, PGM obeys following rules: $$ \\begin{aligned} \u0026\\text {Sum Rule : } p\\left(x_{1}\\right)=\\int p\\left(x_{1}, x_{2}\\right) d x_{2}\\\\ \u0026\\text {Product Rule : } p\\left(x_{1}, x_{2}\\right)=p\\left(x_{1} | x_{2}\\right) p\\left(x_{2}\\right)\\\\ \u0026\\text {Chain Rule: } p\\left(x_{1}, x_{2}, \\cdots, x_{p}\\right)=\\prod_{i=1}^{p} p\\left(x_{i} | x_{i+1, x_{i+2}} \\ldots x_{p}\\right)\\\\ \u0026\\text {Bayesian Rule: } p\\left(x_{1} | x_{2}\\right)=\\frac{p\\left(x_{2} | x_{1}\\right) p\\left(x_{1}\\right)}{p\\left(x_{2}\\right)} \\end{aligned} $$\nAs the dimension of the data increases, the chain rule is harder to compute. In fact, many models try to simplify it in some ways.\nWhy we need probabilistic graphical models Reasons:\n  They provide a simple way to visualize the structure of a probabilistic model and can be used to design and motivate new models.\n  Insights into the properties of the model, including conditional independence properties, can be obtained by inspection of the graph.\n  Complex computations, required to perform inference and learning in sophisticated models, can be expressed in terms of graphical manipulations, in which underlying mathematical expressions are carried along implicitly.\n  Three major parts of PGM   Representation: Express a probability distribution that models some real-world phenomenon.\n  Inference: Obtain answers to relevant questions from our models.\n  Learning: Fit a model to real-world data.\n  We are going to mainly focus on Representation in this post.\nRepresentation Representation: Express a probability distribution that models some real-world phenomenon.\nDirected graphical models (Bayesian networks) Directed graphical models is also known as Bayesian networks.\nIntuition:\nIn a directed graph, vertices correspond to variables $x_i$ and edges indicate dependency relationships. Once the graphical representation of a directed graph is given (directed acyclic graphs), we can easily calculate the joint probability. For example, from the figure above, we can calculate the joint probability $p(a,b,c,d,e)$ by\n$$p(a,b,c,d,e) = p(a) \\cdot p(b|a) \\cdot p(c|b,d) \\cdot p(d) \\cdot p(e|c)$$\nFormal Definition:\nA Bayesian network is a directed graph $G= (V,E)$ together with\n  A random variable $x_i$ for each node $i \\in V$.\n  One conditional probability distribution (CPD) $p(x_i \\mid x_{A_i})$ per node, specifying the probability of $x_i$ conditioned on its parents’ values.\n  Note:\n  Bayesian networks represent probability distributions that can be formed via products of smaller, local conditional probability distributions (one for each variable). Another way to say it is that each factor in the factorization of $p(a,b,c,d,e)$ is locally normalized (every factor can sum up to one).\n  Directed models are often used as generative models.\n  Undirected graphical models (Markov random fields) Undirected graphical models is also known as Markov random fields (MRFs).\nUnlike in the directed case, we cannot say anything about how one variable is generated from another set of variables (as a conditional probability distribution would do).\nIntuition:\nSuppose we have five students doing a project and we want to evaluate how well they would cooperate together. Since five people are too many to be evaluated as a whole, we devide it into small subgroups and evaluate these subgroups respectively. In fact, these small subgroups are called clique and we would introduce it later in this section.\nHere, we introduce the concept of potential function $\\phi$ to evaluete how well they would cooperate together. You can think of it as a score that measures how well a clique cooperate. Higher scores indicate better cooperation. In fact, we requie scores to be non-negative, and depending on how we define the potential functions, we would get different models. As the figure shown above, we could write $p(a,b,c,d,e)$ as\n$$p(a,b,c,d,e) = \\phi_1(a,b,c) \\cdot \\phi_2(b,d) \\cdot \\phi_3(d,e)$$\nNote that the left hand side of the queation is a probability but the right hand side is a product of potentials/ scores. To make the right hand side a valid probability, we need to introduce a normalization term $1/Z$. Hence it becomes\n$$p(a,b,c,d,e) = \\frac{1}{Z} \\cdot \\phi_1(a,b,c) \\cdot \\phi_2(b,d) \\cdot \\phi_3(d,e)$$\nHere we say $p(a,b,c,d,e)$ is globally normalized. Also, we call $Z$ a partition function, which is\n$$ Z = \\sum_{a,b,c,d,e} \\phi_1(a,b,c) \\cdot \\phi_2(b,d) \\cdot \\phi_3(d,e) \\tag 1$$\nNotice that the summation in $(1)$ is over the exponentially many possible assignments to $a,b,c,d$ and $e$. For this reason, computing $Z$ is intractable in general, but much work exists on how to approximate it.\nFormal Definition:\n  cliques: fully connected subgraphs.\n  maximal clique: A clique is a maximal clique if it is not contained in any larger clique.\n  A Markov Random Field (MRF) is a probability distribution $p$ over variables $x_{1}, \\ldots, x_{n}$ defined by an undirected graph $G$ in which nodes correspond to variables $x_{i} .$ The probability $p$ has the form\n$$p\\left(x_{1}, \\ldots, x_{n}\\right)=\\frac{1}{Z} \\prod_{c \\in C} \\phi_{c}\\left(x_{c}\\right) \\tag 2$$\nwhere $C$ denotes the set of cliques of $G,$ and each factor $\\phi_{c}$ is a non-negative function over the variables in a clique. The partition function\n$$ Z=\\sum_{x_{1}, \\ldots, x_{n}} \\prod_{c \\in C} \\phi_{c}\\left(x_{c}\\right) $$\nis a normalizing constant that ensures that the distribution sums to one.\nMarkov Properties of undirected graph   Global Markov Property: $p$ satisfies the global Markov property with respect to a graph $G$ if for any disjoint vertex subsets $A$, $B$, and $C$, such that $C$ separates $A$ and $B$, the random variables $X_A$ are conditionally independent of $X_B$ given $X_C$. Here,we say $C$ separates $A$ and $B$ if every path from a node in $A$ to a node in B passes through a node in $C$ (d-seperation).\n  Local Markov Property: $p$ satisfies the local Markov property with respect to $G$ if the conditional distribution of a variable given its neighbors is independent of the remaining nodes.\n  Pairwise Markov Property: $p$ satisfies the pairwise markov property with respect to $G$ if for any pair of non-adjacent nodes, $s,t \\in V$, we have $X_{s} \\perp X_{t} | X_{V \\backslash{s, t}}$.\n  Note:\n  A distribution $p$ that satisfies the global Markov property is said to be a Markov random field or Markov network with respect to the graph.\n  Global Markov Property $\\Rightarrow$ Local Markov Property $\\Rightarrow$Pairwise Markov Property.\n  A Markov random field reflects conditional independency since it satisfies the Local Markov Property.\n  To see whether a distribution is a Markov random field or Markov network, we have the following theorem:\nHammersley-Clifford Theorem: Suppose $p$ is a strictly positive distribution, and $G$ is an undirected graph that indexes the domain of $p$. Then $p$ is Markov with respect to G if and only if $p$ factorizes over the cliques of the graph $G$.\n  Comparison between Bayesian networks and Markov random fields   Bayesian networks effectively show causality, whereas MRFs cannot. Thus, MRFs are preferable for problems where there is no clear causality between random variables.\n  It is much easier to generate data from a Bayesian network, which is important in some applications.\n  In Markov random fields, computing the normalization constant $Z$ requires a summation over the exponentially many possible assignments. For this reason, computing $Z$ is intractable in general, but much work exists on how to approximate it.\n  Moral graph A moral graph is used to find the equivalent undirected form of a directed acyclic graph.\nThe moralized counterpart of a directed acyclic graph is formed by\n  Add edges between all pairs of non-adjacent nodes that have a common child.\n  Make all edges in the graph undirected.\n  Here is an example:\nNote that a Bayesian network can always be converted into an undirected network.\nTherefore, MRFs have more power than Bayesian networks, but are more difficult to deal with computationally. A general rule of thumb is to use Bayesian networks whenever possible, and only switch to MRFs if there is no natural way to model the problem with a directed graph\nFactor Graph A Markov network has an undesirable ambiguity from the factorization perspective. Consider the three-node Markov network in the figure (left). Any distribution that factorizes as\n$$p(x_1, x_2, x_3) \\propto \\phi(x_1,x_2,x_3) \\tag 3$$\nfor some positive function $\\phi$ is Markov with respect to this graph (check Hammersley-Clifford Theorem mentioned earlier). However, we may wish to use a more restricted parameterization, where\n$$p(x1, x2, x3) \\propto \\phi_1(x_1, x_2)\\phi_1(x_2, x_3)\\phi_1(x_1, x_3) \\tag 4$$\nThe model family in $(4)$ is smaller, and therefore may be more amenable to parameter estimation. But the Markov network formalism cannot distinguish between these two parameterizations. In order to state models more precisely, the factorization in $(2)$ can be represented directly by means of a factor graph.\nDefinition (factor graph): A factor graph is a bipartite graph $G = (V, F, E)$ in which a variable node $x_i \\in V$ is connected to a factor node $\\phi_a \\in F$ if $x_i$ is an argument to $\\phi_a$.\nAn example of a factor graph is shown on the right side of the figure above. In the figure, the circles are variable nodes, and the shaded boxes are factor nodes. Notice that, unlike the undirected graph, the factor graph depicts the factorization of the model unambiguously.\nRemark: Directed models can be thought of as a kind of factor graph, in which the individual factors are locally normalized in a special fashion so that globally $Z = 1$.\nInference Inference: Obtain answers to relevant questions from our models.\n Marginal inference: what is the probability of a given variable in our model after we sum everything else out?  $$ p(y=1) = \\sum_{x_1} \\sum_{x_2} \\cdots \\sum_{x_n} p(y=1, x_1, x_2, \\dotsc, x_n)$$\n Maximum a posteriori (MAP) inference: what is the most likely assignment to the variables in the model?  $$ \\max_{x_1, \\dotsc, x_n} p(y=1, x_1, \\dotsc, x_n)$$\nLearning Learning: Fit a model to real-world data.\n  Parameter learning: the graph structure is known and we want to estimate the parameters.\n  complete case:\n We use Maximum Likelihood Estimation to estimate parameters.    incomplete case:\n  We use EM Algorithm to approximate parameters.\n  Example: Guassian Mixture Model (GMM), Hidden Markov Model (HMM).\n      Structure learning: we want to estimate the graph, i.e., determine from data how the variables depend on each other.\n   Reference:\n Bishop, Christopher M., “Pattern Recognition and Machine Learning,” Springer, 2006. https://ermongroup.github.io/cs228-notes/ https://en.wikipedia.org/wiki/Moral_graph https://space.bilibili.com/97068901 https://zhenkewu.com/assets/pdfs/slides/teaching/2016/biostat830/lecture_notes/Lecture4.pdf https://skggm.github.io/skggm/tour https://homepages.inf.ed.ac.uk/csutton/publications/crftutv2.pdf  ","wordCount":"1645","inLanguage":"en","datePublished":"2020-05-11T00:00:00Z","dateModified":"2020-05-11T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://tangliyan.com/blog/posts/pgm/"},"publisher":{"@type":"Organization","name":"Liyan Tang","logo":{"@type":"ImageObject","url":"https://tangliyan.com/blog/favicon.ico"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://tangliyan.com/blog/ accesskey=h title="Liyan Tang (Alt + H)">Liyan Tang</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
<li>
<a href=https://tangliyan.com/blog/archives title=Archive>
<span>Archive</span>
</a>
</li>
<li>
<a href=https://tangliyan.com/blog/tags/ title=Tags>
<span>Tags</span>
</a>
</li>
<li>
<a href=https://tangliyan.com/blog/search/ title="Search (Alt + /)" accesskey=/>
<span>Search</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<div class=breadcrumbs><a href=https://tangliyan.com/blog/>Home</a>&nbsp;»&nbsp;<a href=https://tangliyan.com/blog/posts/>Posts</a></div>
<h1 class=post-title>
Probabilistic Graphical Model (PGM)
</h1>
<div class=post-meta><span title="2020-05-11 00:00:00 +0000 UTC">May 11, 2020</span>&nbsp;·&nbsp;8 min
</div>
</header> <div class=toc>
<details>
<summary accesskey=c title="(Alt + C)">
<span class=details>Table of Contents</span>
</summary>
<div class=inner><ul>
<li>
<a href=#probabilistic-graphical-model-pgm aria-label="Probabilistic Graphical Model (PGM)">Probabilistic Graphical Model (PGM)</a></li>
<li>
<a href=#why-we-need-probabilistic-graphical-models aria-label="Why we need probabilistic graphical models">Why we need probabilistic graphical models</a></li>
<li>
<a href=#three-major-parts-of-pgm aria-label="Three major parts of PGM">Three major parts of PGM</a></li>
<li>
<a href=#representation aria-label=Representation>Representation</a><ul>
<li>
<a href=#directed-graphical-models-bayesian-networks aria-label="Directed graphical models (Bayesian networks)">Directed graphical models (Bayesian networks)</a></li>
<li>
<a href=#undirected-graphical-models-markov-random-fields aria-label="Undirected graphical models (Markov random fields)">Undirected graphical models (Markov random fields)</a></li>
<li>
<a href=#markov-properties-of-undirected-graph aria-label="Markov Properties of undirected graph">Markov Properties of undirected graph</a></li>
<li>
<a href=#comparison-between-bayesian-networks-and-markov-random-fields aria-label="Comparison between Bayesian networks and Markov random fields">Comparison between Bayesian networks and Markov random fields</a></li>
<li>
<a href=#moral-graph aria-label="Moral graph">Moral graph</a></li>
<li>
<a href=#factor-graph aria-label="Factor Graph">Factor Graph</a></li></ul>
</li>
<li>
<a href=#inference aria-label=Inference>Inference</a></li>
<li>
<a href=#learning aria-label=Learning>Learning</a>
</li>
</ul>
</div>
</details>
</div>
<div class=post-content><h2 id=probabilistic-graphical-model-pgm>Probabilistic Graphical Model (PGM)<a hidden class=anchor aria-hidden=true href=#probabilistic-graphical-model-pgm>#</a></h2>
<p><strong>Definition</strong>: A <strong>probabilistic graphical model</strong> is a probabilistic model for which a graph expresses the conditional dependence structure between random variables.</p>
<p>In general, PGM obeys following rules:
$$
\begin{aligned}
&\text {Sum Rule : } p\left(x_{1}\right)=\int p\left(x_{1}, x_{2}\right) d x_{2}\\ &\text {Product Rule : } p\left(x_{1}, x_{2}\right)=p\left(x_{1} | x_{2}\right) p\left(x_{2}\right)\\ &\text {Chain Rule: } p\left(x_{1}, x_{2}, \cdots, x_{p}\right)=\prod_{i=1}^{p} p\left(x_{i} | x_{i+1, x_{i+2}} \ldots x_{p}\right)\\ &\text {Bayesian Rule: } p\left(x_{1} | x_{2}\right)=\frac{p\left(x_{2} | x_{1}\right) p\left(x_{1}\right)}{p\left(x_{2}\right)}
\end{aligned}
$$</p>
<p>As the dimension of the data increases, the chain rule is harder to compute. In fact, many models try to simplify it in some ways.</p>
<h2 id=why-we-need-probabilistic-graphical-models>Why we need probabilistic graphical models<a hidden class=anchor aria-hidden=true href=#why-we-need-probabilistic-graphical-models>#</a></h2>
<p>Reasons:</p>
<ul>
<li>
<p>They provide a simple way to visualize the structure of a probabilistic model and can be used to design and motivate new models.</p>
</li>
<li>
<p>Insights into the properties of the model, including conditional independence properties, can be obtained by inspection of the graph.</p>
</li>
<li>
<p>Complex computations, required to perform inference and learning in sophisticated models, can be expressed in terms of graphical manipulations, in which underlying mathematical expressions are carried along implicitly.</p>
</li>
</ul>
<h2 id=three-major-parts-of-pgm>Three major parts of PGM<a hidden class=anchor aria-hidden=true href=#three-major-parts-of-pgm>#</a></h2>
<ul>
<li>
<p><strong>Representation</strong>: Express a probability distribution that models some real-world phenomenon.</p>
</li>
<li>
<p><strong>Inference</strong>: Obtain answers to relevant questions from our models.</p>
</li>
<li>
<p><strong>Learning</strong>: Fit a model to real-world data.</p>
</li>
</ul>
<p>We are going to mainly focus on <em>Representation</em> in this post.</p>
<h2 id=representation>Representation<a hidden class=anchor aria-hidden=true href=#representation>#</a></h2>
<p><strong>Representation</strong>: Express a probability distribution that models some real-world phenomenon.</p>
<img src="https://img-blog.csdnimg.cn/20200511022026708.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_25,color_FFFFFF,t_70" width=600>
<h3 id=directed-graphical-models-bayesian-networks>Directed graphical models (Bayesian networks)<a hidden class=anchor aria-hidden=true href=#directed-graphical-models-bayesian-networks>#</a></h3>
<p><strong>Directed graphical models</strong> is also known as <strong>Bayesian networks</strong>.</p>
<img src="https://img-blog.csdnimg.cn/20200511022108791.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_30,color_FFFFFF,t_70" width=400>
<p><em>Intuition:</em></p>
<p>In a directed graph, vertices correspond to variables $x_i$ and edges indicate dependency relationships. Once the graphical representation of a directed graph is given (<em>directed acyclic graphs</em>), we can easily calculate the joint probability. For example, from the figure above, we can calculate the joint probability $p(a,b,c,d,e)$ by</p>
<p>$$p(a,b,c,d,e) = p(a) \cdot p(b|a) \cdot p(c|b,d) \cdot p(d) \cdot p(e|c)$$</p>
<p><em>Formal Definition:</em></p>
<p>A Bayesian network is a directed graph $G= (V,E)$ together with</p>
<ul>
<li>
<p>A random variable $x_i$ for each node $i \in V$.</p>
</li>
<li>
<p>One conditional probability distribution (CPD) $p(x_i \mid x_{A_i})$ per node, specifying the probability of $x_i$ conditioned on its parents’ values.</p>
</li>
</ul>
<p>Note:</p>
<ul>
<li>
<p>Bayesian networks represent probability distributions that can be formed via products of smaller, <strong>local</strong> conditional probability distributions (one for each variable). Another way to say it is that each <em>factor</em> in the factorization of $p(a,b,c,d,e)$ is <strong>locally normalized</strong> (every factor can sum up to one).</p>
</li>
<li>
<p>Directed models are often used as generative models.</p>
</li>
</ul>
<h3 id=undirected-graphical-models-markov-random-fields>Undirected graphical models (Markov random fields)<a hidden class=anchor aria-hidden=true href=#undirected-graphical-models-markov-random-fields>#</a></h3>
<p><strong>Undirected graphical models</strong> is also known as <strong>Markov random fields</strong> (<em>MRFs</em>).</p>
<img src="https://img-blog.csdnimg.cn/20200511023823753.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_25,color_FFFFFF,t_70" width=400>
<p>Unlike in the directed case, we cannot say anything about how one variable is generated from another set of variables (as a conditional probability distribution would do).</p>
<p><em>Intuition:</em></p>
<p>Suppose we have five students doing a project and we want to evaluate how well they would cooperate together. Since five people are too many to be evaluated as a whole, we devide it into small subgroups and evaluate these subgroups respectively. In fact, these small subgroups are called <strong>clique</strong> and we would introduce it later in this section.</p>
<img src="https://img-blog.csdnimg.cn/20200511023912396.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_25,color_FFFFFF,t_70" width=400>
<p>Here, we introduce the concept of <strong>potential function $\phi$</strong> to evaluete how well they would cooperate together. You can think of it as a score that measures how well a clique cooperate. Higher scores indicate better cooperation. In fact, we requie scores to be non-negative, and depending on how we define the potential functions, we would get different models. As the figure shown above, we could write $p(a,b,c,d,e)$ as</p>
<p>$$p(a,b,c,d,e) = \phi_1(a,b,c) \cdot \phi_2(b,d) \cdot \phi_3(d,e)$$</p>
<p>Note that the left hand side of the queation is a probability but the right hand side is a product of potentials/ scores. To make the right hand side a valid probability, we need to introduce a normalization term $1/Z$. Hence it becomes</p>
<p>$$p(a,b,c,d,e) = \frac{1}{Z} \cdot \phi_1(a,b,c) \cdot \phi_2(b,d) \cdot \phi_3(d,e)$$</p>
<p>Here we say $p(a,b,c,d,e)$ is <strong>globally normalized</strong>. Also, we call $Z$ a partition function, which is</p>
<p>$$ Z = \sum_{a,b,c,d,e} \phi_1(a,b,c) \cdot \phi_2(b,d) \cdot \phi_3(d,e) \tag 1$$</p>
<p>Notice that the summation in $(1)$ is over the exponentially many possible assignments to $a,b,c,d$ and $e$. For this reason, computing $Z$ is intractable in general, but much work exists on how to approximate it.</p>
<p><em>Formal Definition:</em></p>
<ul>
<li>
<p><strong>cliques:</strong> fully connected subgraphs.</p>
</li>
<li>
<p><strong>maximal clique:</strong> A clique is a maximal clique if it is not contained in any larger clique.</p>
</li>
</ul>
<p>A Markov Random Field (MRF) is a probability distribution $p$ over variables $x_{1}, \ldots, x_{n}$ defined by an <em>undirected</em> graph $G$ in which nodes correspond to variables $x_{i} .$ The probability $p$ has the form</p>
<p>$$p\left(x_{1}, \ldots, x_{n}\right)=\frac{1}{Z} \prod_{c \in C} \phi_{c}\left(x_{c}\right)
\tag 2$$</p>
<p>where $C$ denotes the set of cliques of $G,$ and each <em>factor</em> $\phi_{c}$ is a non-negative function over the variables in a clique. The <em>partition function</em></p>
<p>$$ Z=\sum_{x_{1}, \ldots, x_{n}} \prod_{c \in C} \phi_{c}\left(x_{c}\right) $$</p>
<p>is a normalizing constant that ensures that the distribution sums to one.</p>
<h3 id=markov-properties-of-undirected-graph>Markov Properties of undirected graph<a hidden class=anchor aria-hidden=true href=#markov-properties-of-undirected-graph>#</a></h3>
<img src="https://img-blog.csdnimg.cn/20200511014032141.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_30,color_FFFFFF,t_70" width=650>
<ul>
<li>
<p><strong>Global Markov Property:</strong> $p$ satisfies the global Markov property with respect to a graph $G$ if for any disjoint vertex subsets $A$, $B$, and $C$, such that $C$ separates $A$ and $B$, the random variables $X_A$ are conditionally independent of $X_B$ given $X_C$.
Here,we say $C$ separates $A$ and $B$ if every path from a node in $A$ to a node in B passes through a node in $C$ (<strong>d-seperation</strong>).</p>
</li>
<li>
<p><strong>Local Markov Property:</strong> $p$ satisfies the local Markov property with respect to $G$ if the conditional distribution of a variable given its neighbors is independent of the remaining nodes.</p>
</li>
<li>
<p><strong>Pairwise Markov Property:</strong> $p$ satisfies the pairwise markov property with respect to $G$ if for any pair of non-adjacent nodes, $s,t \in V$, we have $X_{s} \perp X_{t} | X_{V \backslash{s, t}}$.</p>
</li>
</ul>
<p>Note:</p>
<ul>
<li>
<p>A distribution $p$ that satisfies the global Markov property is said to be a Markov random field or Markov network with respect to the graph.</p>
</li>
<li>
<p>Global Markov Property $\Rightarrow$ Local Markov Property $\Rightarrow$Pairwise Markov Property.</p>
</li>
<li>
<p>A Markov random field reflects conditional independency since it satisfies the Local Markov Property.</p>
</li>
<li>
<p>To see whether a distribution is a Markov random field or Markov network, we have the following theorem:</p>
<p><strong>Hammersley-Clifford Theorem:</strong> Suppose $p$ is a strictly positive distribution, and $G$ is an undirected graph that indexes the domain of $p$. Then $p$ is Markov with respect to G if and only if $p$ factorizes over the cliques of the graph $G$.</p>
</li>
</ul>
<h3 id=comparison-between-bayesian-networks-and-markov-random-fields>Comparison between Bayesian networks and Markov random fields<a hidden class=anchor aria-hidden=true href=#comparison-between-bayesian-networks-and-markov-random-fields>#</a></h3>
<ul>
<li>
<p>Bayesian networks effectively show causality, whereas MRFs cannot. Thus, <em>MRFs are preferable for problems where there is no clear causality between random variables.</em></p>
</li>
<li>
<p>It is much easier to generate data from a Bayesian network, which is important in some applications.</p>
</li>
<li>
<p>In Markov random fields, computing the normalization constant $Z$ requires a summation over the exponentially many possible assignments. For this reason, computing $Z$ is intractable in general, but much work exists on how to approximate it.</p>
</li>
</ul>
<h3 id=moral-graph>Moral graph<a hidden class=anchor aria-hidden=true href=#moral-graph>#</a></h3>
<p>A moral graph is used to find the equivalent undirected form of a directed acyclic graph.</p>
<p>The moralized counterpart of a directed acyclic graph is formed by</p>
<ol>
<li>
<p>Add edges between all pairs of non-adjacent nodes that have a common child.</p>
</li>
<li>
<p>Make all edges in the graph undirected.</p>
</li>
</ol>
<p>Here is an example:</p>
<img src="https://img-blog.csdnimg.cn/20200511021826734.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_30,color_FFFFFF,t_70" width=500>
<p>Note that a Bayesian network can always be converted into an undirected network.</p>
<p>Therefore, MRFs have more power than Bayesian networks, but are more difficult to deal with computationally. A general rule of thumb is to use Bayesian networks whenever possible, and only switch to MRFs if there is no natural way to model the problem with a directed graph</p>
<h3 id=factor-graph>Factor Graph<a hidden class=anchor aria-hidden=true href=#factor-graph>#</a></h3>
<img src="https://img-blog.csdnimg.cn/20200511021859206.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_25,color_FFFFFF,t_70" width=600>
<p>A Markov network has an undesirable ambiguity from the factorization perspective. Consider the three-node Markov network in the figure (left). Any distribution that factorizes as</p>
<p>$$p(x_1, x_2, x_3) \propto \phi(x_1,x_2,x_3) \tag 3$$</p>
<p>for some positive function $\phi$ is Markov with respect to this graph (check <em>Hammersley-Clifford Theorem</em> mentioned earlier). However, we may wish to use a more restricted parameterization, where</p>
<p>$$p(x1, x2, x3) \propto \phi_1(x_1, x_2)\phi_1(x_2, x_3)\phi_1(x_1, x_3) \tag 4$$</p>
<p>The model family in $(4)$ is smaller, and therefore may be more amenable to parameter estimation. But the Markov network formalism cannot distinguish between these two parameterizations. In order to state models more precisely, the factorization in $(2)$ can be represented directly by means of a <strong>factor graph</strong>.</p>
<p><em>Definition (factor graph): A factor graph is a bipartite graph $G = (V, F, E)$ in which a variable node $x_i \in V$ is connected to a factor node $\phi_a \in F$ if $x_i$ is an argument to $\phi_a$.</em></p>
<p>An example of a factor graph is shown on the right side of the figure above. In the figure, <em>the circles are variable nodes, and the shaded boxes are factor nodes</em>. Notice that, unlike the undirected graph, the factor graph depicts the factorization of the model unambiguously.</p>
<p><strong>Remark: Directed models can be thought of as a kind of factor graph, in which the individual factors are locally normalized in a special fashion so that globally $Z = 1$.</strong></p>
<h2 id=inference>Inference<a hidden class=anchor aria-hidden=true href=#inference>#</a></h2>
<p><strong>Inference</strong>: Obtain answers to relevant questions from our models.</p>
<ul>
<li><strong>Marginal inference</strong>: what is the probability of a given variable in our model after we sum everything else out?</li>
</ul>
<p>$$ p(y=1) = \sum_{x_1} \sum_{x_2} \cdots \sum_{x_n} p(y=1, x_1, x_2, \dotsc, x_n)$$</p>
<ul>
<li><strong>Maximum a posteriori (MAP) inference</strong>: what is the most likely assignment to the variables in the model?</li>
</ul>
<p>$$ \max_{x_1, \dotsc, x_n} p(y=1, x_1, \dotsc, x_n)$$</p>
<h2 id=learning>Learning<a hidden class=anchor aria-hidden=true href=#learning>#</a></h2>
<p><strong>Learning</strong>: Fit a model to real-world data.</p>
<ul>
<li>
<p><strong>Parameter learning</strong>: the graph structure is known and we want to estimate the parameters.</p>
<ul>
<li>
<p>complete case:</p>
<ul>
<li>We use Maximum Likelihood Estimation to estimate parameters.</li>
</ul>
</li>
<li>
<p>incomplete case:</p>
<ul>
<li>
<p>We use <a href=https://liyantang.blog.csdn.net/article/details/105722481>EM Algorithm</a> to approximate parameters.</p>
</li>
<li>
<p>Example: Guassian Mixture Model (GMM), <a href=https://liyantang.blog.csdn.net/article/details/105898644>Hidden Markov Model (HMM)</a>.</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Structure learning</strong>: we want to estimate the graph, <em>i.e.</em>, determine from data how the variables depend on each other.</p>
</li>
</ul>
<hr>
<p>Reference:</p>
<ul>
<li>Bishop, Christopher M., &ldquo;Pattern Recognition and Machine Learning,&rdquo; Springer, 2006.</li>
<li><a href=https://ermongroup.github.io/cs228-notes/>https://ermongroup.github.io/cs228-notes/</a></li>
<li><a href=https://en.wikipedia.org/wiki/Moral_graph>https://en.wikipedia.org/wiki/Moral_graph</a></li>
<li><a href=https://space.bilibili.com/97068901>https://space.bilibili.com/97068901</a></li>
<li><a href=https://zhenkewu.com/assets/pdfs/slides/teaching/2016/biostat830/lecture_notes/Lecture4.pdf>https://zhenkewu.com/assets/pdfs/slides/teaching/2016/biostat830/lecture_notes/Lecture4.pdf</a></li>
<li><a href=https://skggm.github.io/skggm/tour>https://skggm.github.io/skggm/tour</a></li>
<li><a href=https://homepages.inf.ed.ac.uk/csutton/publications/crftutv2.pdf>https://homepages.inf.ed.ac.uk/csutton/publications/crftutv2.pdf</a></li>
</ul>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=https://tangliyan.com/blog/tags/ml/>ML</a></li>
<li><a href=https://tangliyan.com/blog/tags/math/>MATH</a></li>
</ul>
<nav class=paginav>
<a class=prev href=https://tangliyan.com/blog/posts/gmm/>
<span class=title>« Prev Page</span>
<br>
<span>Gaussian mixture model (GMM), k-means</span>
</a>
<a class=next href=https://tangliyan.com/blog/posts/hmm/>
<span class=title>Next Page »</span>
<br>
<span>Hidden Markov Model (HMM)</span>
</a>
</nav>
<div class=share-buttons>
<a target=_blank rel="noopener noreferrer" aria-label="share Probabilistic Graphical Model (PGM) on twitter" href="https://twitter.com/intent/tweet/?text=Probabilistic%20Graphical%20Model%20%28PGM%29&url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fpgm%2f&hashtags=ML%2cMATH"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Probabilistic Graphical Model (PGM) on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fpgm%2f&title=Probabilistic%20Graphical%20Model%20%28PGM%29&summary=Probabilistic%20Graphical%20Model%20%28PGM%29&source=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fpgm%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Probabilistic Graphical Model (PGM) on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fpgm%2f&title=Probabilistic%20Graphical%20Model%20%28PGM%29"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Probabilistic Graphical Model (PGM) on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fpgm%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Probabilistic Graphical Model (PGM) on whatsapp" href="https://api.whatsapp.com/send?text=Probabilistic%20Graphical%20Model%20%28PGM%29%20-%20https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fpgm%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share Probabilistic Graphical Model (PGM) on telegram" href="https://telegram.me/share/url?text=Probabilistic%20Graphical%20Model%20%28PGM%29&url=https%3a%2f%2ftangliyan.com%2fblog%2fposts%2fpgm%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a>
</div>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2022 <a href=https://tangliyan.com/blog/>Liyan Tang</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
<script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerText='copy';function d(){a.innerText='copied!',setTimeout(()=>{a.innerText='copy'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script>
</body>
</html>